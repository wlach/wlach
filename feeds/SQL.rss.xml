<?xml version="1.0" encoding="utf-8"?> 
<rss version="2.0">
 <channel>
  <title>Untitled Site: Posts tagged 'SQL'</title>
  <description>Untitled Site: Posts tagged 'SQL'</description>
  <link>http://www.example.com/tags/SQL.html</link>
  <lastBuildDate>Fri, 23 Oct 2015 04:00:00 UT</lastBuildDate>
  <pubDate>Fri, 23 Oct 2015 04:00:00 UT</pubDate>
  <ttl>1800</ttl>
  <item>
   <title>The new old Perfherder data model</title>
   <link>http://www.example.com/2015/10/the-new-old-perfherder-data-model.html?utm_source=SQL&amp;utm_medium=RSS</link>
   <guid>urn:http-www-example-com:-2015-10-the-new-old-perfherder-data-model-html</guid>
   <pubDate>Fri, 23 Oct 2015 04:00:00 UT</pubDate>
   <description>&lt;html&gt;
&lt;p&gt;I spent a good chunk of time last quarter redesigning how &lt;a href="https://wiki.mozilla.org/Auto-tools/Projects/Perfherder"&gt;Perfherder&lt;/a&gt; stores its data internally. Here are some notes on this change, for posterity.&lt;/p&gt;

&lt;p&gt;Perfherder&amp;#8217;s data model is based around two concepts:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;Series signatures: A unique set of properties (platform, test name, suite name, options) that identifies a performance test.&lt;/li&gt;
 &lt;li&gt;Series data: A set of measurements for a series signature, indexed by treeherder push and job information.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;When it was first written, Perfherder stored the second type of data as a JSON-encoded series in a relational (MySQL) database. That is, instead of storing each datum as a row in the database, we would store sequences of them. The assumption was that for the common case (getting a bunch of data to plot on a graph), this would be faster than fetching a bunch of rows and then encoding them as JSON. Unfortunately this wasn&amp;#8217;t really true, and it had some serious drawbacks besides.&lt;/p&gt;

&lt;p&gt;First, the approach&amp;#8217;s performance was awful when it came time to add new data. To avoid needing to decode or download the full stored series when you wanted to render only a small subset of it, we stored the same series multiple times over various time intervals. For example, we stored the series data for one day, one week&amp;#8230; all the way up to one year. You can probably see the problem already: you have to decode and re-encode the same data structure many times for each time interval for every new performance datum you were inserting into the database. The pseudo code looked something like this for each push:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for each platform we're testing talos on:
  for each talos job for the platform:
    for each test suite in the talos job:
      for each subtest in the test suite:
        for each time interval in one year, 90 days, 60 days, ...:
           fetch and decode json series for that time interval from db
           add datapoint to end of series
           re-encode series as json and store in db&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Consider that we have some 6 platforms (android, linux64, osx, winxp, win7, win8), 20ish test suites with potentially dozens of subtests&amp;#8230; and you can see where the problems begin.&lt;/p&gt;

&lt;p&gt;In addition to being slow to write, this was also a pig in terms of disk space consumption. The overhead of JSON (&amp;#8220;{, }&amp;#8221; characters, object properties) really starts to add up when you&amp;#8217;re storing millions of performance measurements. We got around this (sort of) by gzipping the contents of these series, but that still left us with gigantic mysql replay logs as we stored the complete &amp;#8220;transaction&amp;#8221; of replacing each of these series rows thousands of times per day. At one point, we completely ran out of disk space on the treeherder staging instance due to this issue.&lt;/p&gt;

&lt;p&gt;Read performance was also often terrible for many common use cases. The original assumption I mentioned above was wrong: rendering points on a graph is only one use case a system like Perfherder has to handle. We also want to be able to get the set of series values associated with two result sets (to render comparison views) or to look up the data associated with a particular job. We were essentially indexing the performance data only on one single dimension (time) which made these other types of operations unnecessarily complex and slow &amp;#8212; especially as the data you want to look up ages. For example, to look up a two week old comparison between two pushes, you&amp;#8217;d also have to fetch the data for &lt;em&gt;every&lt;/em&gt; subsequent push. That&amp;#8217;s a lot of unnecessary overhead when you&amp;#8217;re rendering a comparison view with 100 or so different performance tests:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2015/08/Screen-Shot-2015-08-07-at-1.57.39-PM.png"&gt;&lt;img src="/files/2015/08/Screen-Shot-2015-08-07-at-1.57.39-PM-300x178.png" alt="Screen Shot 2015-08-07 at 1.57.39 PM" width="300" height="178" class="alignnone size-medium wp-image-1229" srcset="/files/2015/08/Screen-Shot-2015-08-07-at-1.57.39-PM-300x178.png 300w, /files/2015/08/Screen-Shot-2015-08-07-at-1.57.39-PM.png 1003w" sizes="(max-width: 300px) 100vw, 300px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So what&amp;#8217;s the alternative? It&amp;#8217;s actually the most obvious thing: just encode one database row per performance series value and create indexes on each of the properties that we might want to search on (repository, timestamp, job id, push id). Yes, this is a lot of rows (the new database stands at 48 million rows of performance data, and counting) but you know what? MySQL is &lt;em&gt;designed&lt;/em&gt; to handle that sort of load. The current performance data table looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;+----------------+------------------+
| Field          | Type             |
+----------------+------------------+
| id             | int(11)          |
| job_id         | int(10) unsigned |
| result_set_id  | int(10) unsigned |
| value          | double           |
| push_timestamp | datetime(6)      |
| repository_id  | int(11)          | 
| signature_id   | int(11)          | 
+----------------+------------------+&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;MySQL can store each of these structures very efficiently, I haven&amp;#8217;t done the exact calculations, but this is well under 50 bytes per row. Including indexes, the complete set of performance data going back to last year clocks in at 15 gigs. Not bad. And we can examine this data structure across any combination of dimensions we like (push, job, timestamp, repository) making common queries to perfherder very fast.&lt;/p&gt;

&lt;p&gt;What about the initial assumption, that it would be faster to get a series out of the database if it&amp;#8217;s already pre-encoded? Nope, not really. If you have a good index and you&amp;#8217;re only fetching the data you need, the overhead of encoding a bunch of database rows to JSON is pretty minor. From my (remote) location in Toronto, I can fetch 30 days of &lt;a href="https://treeherder.mozilla.org/perf.html#/graphs?timerange=2592000&amp;amp;#038;series=[mozilla-inbound,c233ba1133abbd544002dfbc29d9e63ced42a20e,1]"&gt;tcheck2 data&lt;/a&gt; in 250 ms. Almost certainly most of that is network latency. If the original implementation was faster, it&amp;#8217;s not by a significant amount.&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2015/10/Screen-Shot-2015-10-23-at-1.55.09-PM.png"&gt;&lt;img src="/files/2015/10/Screen-Shot-2015-10-23-at-1.55.09-PM-300x188.png" alt="Screen Shot 2015-10-23 at 1.55.09 PM" width="300" height="188" class="alignnone size-medium wp-image-1259" srcset="/files/2015/10/Screen-Shot-2015-10-23-at-1.55.09-PM-300x188.png 300w, /files/2015/10/Screen-Shot-2015-10-23-at-1.55.09-PM-1024x643.png 1024w" sizes="(max-width: 300px) 100vw, 300px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lesson&lt;/strong&gt;: Sometimes using ancient technologies (SQL) in the most obvious way is the right thing to do. &lt;a href="http://c2.com/xp/DoTheSimplestThingThatCouldPossiblyWork.html"&gt;DoTheSimplestThingThatCouldPossiblyWork&lt;/a&gt;&lt;/p&gt;&lt;/html&gt;</description></item></channel></rss>