<?xml version="1.0" encoding="utf-8"?> 
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
 <title type="text">William Lachance's Log: Posts tagged 'Telemetry'</title>
 <link rel="self" href="https://wlach.github.io/feeds/Telemetry.atom.xml" />
 <link href="https://wlach.github.io/tags/Telemetry.html" />
 <id>urn:https-wlach-github-io:-tags-Telemetry-html</id>
 <updated>2019-10-30T15:11:17Z</updated>
 <entry>
  <title type="text">Using BigQuery JavaScript UDFs to analyze Firefox telemetry for fun &amp; profit</title>
  <link rel="alternate" href="https://wlach.github.io/blog/2019/10/using-bigquery-javascript-udfs-to-analyze-firefox-telemetry-for-fun-profit?utm_source=Telemetry&amp;utm_medium=Atom" />
  <id>urn:https-wlach-github-io:-blog-2019-10-using-bigquery-javascript-udfs-to-analyze-firefox-telemetry-for-fun-profit</id>
  <published>2019-10-30T15:11:17Z</published>
  <updated>2019-10-30T15:11:17Z</updated>
  <author>
   <name>William Lachance</name></author>
  <content type="html">
&lt;p&gt;For the last year, we&amp;rsquo;ve been gradually migrating our backend Telemetry systems from AWS to GCP. I&amp;rsquo;ve been helping out here and there with this effort, most recently porting a job we used to detect slow tab spinners in Firefox nightly, which produced a small dataset that feeds a &lt;a href="https://mikeconley.github.io/bug1310250/"&gt;small adhoc dashboard&lt;/a&gt; which Mike Conley maintains. This was a relatively small task as things go, but it highlighted some features and improvements which I think might be broadly interesting, so I decided to write up a small blog post about it.&lt;/p&gt;

&lt;p&gt;Essentially all this dashboard tells you is what percentage of the Firefox nightly population saw a tab spinner over the past 6 months. And of those that did see a tab spinner, what was the severity? Essentially weâ€™re just trying to make sure that there are no major regressions of user experience (and also that efforts to improve things bore fruit):&lt;/p&gt;

&lt;center&gt;&lt;img style="width:600px" srcset="/files/2019/10/tab-spinner-dash.png" /&gt;&lt;/center&gt;

&lt;p&gt;Pretty simple stuff, but getting the data necessary to produce this kind of dashboard used to be anything but trivial: while some common business/product questions could be answered by a quick query to &lt;a href="https://docs.telemetry.mozilla.org/datasets/batch_view/clients_daily/reference.html"&gt;clients_daily&lt;/a&gt;, getting engineering-specific metrics like this usually involved trawling through gigabytes of raw heka encoded blobs using an Apache Spark cluster and then extracting the relevant information out of the telemetry probe histograms (in this case, &lt;code&gt;FX_TAB_SWITCH_SPINNER_VISIBLE_MS&lt;/code&gt; and &lt;code&gt;FX_TAB_SWITCH_SPINNER_VISIBLE_LONG_MS&lt;/code&gt;) contained therein.&lt;/p&gt;

&lt;p&gt;The code itself was rather complicated (&lt;a href="https://github.com/mozilla/python_mozetl/blob/58dce245ce8012b338e8b102a8c2c0f00601be60/mozetl/tab_spinner/tab_spinner.py"&gt;take a look, if you dare&lt;/a&gt;) but even worse, running it could get &lt;em&gt;very expensive&lt;/em&gt;. We had a 14 node cluster churning through this script daily, and it took on average about an hour and a half to run! I don&amp;rsquo;t have the exact cost figures on hand (and am not sure if I&amp;rsquo;d be authorized to share them if I did), but based on a back of the envelope sketch, this one single script was probably costing us somewhere on the order of $10-$40 a day (that works out to between $3650-$14600 a year).&lt;/p&gt;

&lt;p&gt;With our move to &lt;a href="https://cloud.google.com/bigquery/"&gt;BigQuery&lt;/a&gt;, things get a lot simpler! Thanks to the combined effort of my team and data operations[1], we now produce &amp;ldquo;stable&amp;rdquo; ping tables on a daily basis with &lt;em&gt;all&lt;/em&gt; the relevant histogram data (stored as JSON blobs), queryable using relatively vanilla SQL. In this case, the data we care about is in &lt;code&gt;telemetry.main&lt;/code&gt; (named after the main ping, appropriately enough). With the help of a small &lt;a href="https://cloud.google.com/bigquery/docs/reference/standard-sql/user-defined-functions"&gt;JavaScript UDF&lt;/a&gt; function, all of this data can easily be extracted into a table inside a single SQL query scheduled by &lt;a href="https://docs.telemetry.mozilla.org/tools/stmo.html"&gt;sql.telemetry.mozilla.org&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE TEMP FUNCTION
  udf_js_json_extract_highest_long_spinner (input STRING)
  RETURNS INT64
  LANGUAGE js AS """
    if (input == null) {
      return 0;
    }
    var result = JSON.parse(input);
    var valuesMap = result.values;
    var highest = 0;
    for (var key in valuesMap) {
      var range = parseInt(key);
      if (valuesMap[key]) {
        highest = range &amp;gt; 0 ? range : 1;
      }
    }
    return highest;
""";

SELECT build_id,
sum (case when highest &amp;gt;= 64000 then 1 else 0 end) as v_64000ms_or_higher,
sum (case when highest &amp;gt;= 27856 and highest &amp;lt; 64000 then 1 else 0 end) as v_27856ms_to_63999ms,
sum (case when highest &amp;gt;= 12124 and highest &amp;lt; 27856 then 1 else 0 end) as v_12124ms_to_27855ms,
sum (case when highest &amp;gt;= 5277 and highest &amp;lt; 12124 then 1 else 0 end) as v_5277ms_to_12123ms,
sum (case when highest &amp;gt;= 2297 and highest &amp;lt; 5277 then 1 else 0 end) as v_2297ms_to_5276ms,
sum (case when highest &amp;gt;= 1000 and highest &amp;lt; 2297 then 1 else 0 end) as v_1000ms_to_2296ms,
sum (case when highest &amp;gt; 0 and highest &amp;lt; 50 then 1 else 0 end) as v_0ms_to_49ms,
sum (case when highest &amp;gt;= 50 and highest &amp;lt; 100 then 1 else 0 end) as v_50ms_to_99ms,
sum (case when highest &amp;gt;= 100 and highest &amp;lt; 200 then 1 else 0 end) as v_100ms_to_199ms,
sum (case when highest &amp;gt;= 200 and highest &amp;lt; 400 then 1 else 0 end) as v_200ms_to_399ms,
sum (case when highest &amp;gt;= 400 and highest &amp;lt; 800 then 1 else 0 end) as v_400ms_to_799ms,
count(*) as count
from
(select build_id, client_id, max(greatest(highest_long, highest_short)) as highest
from
(SELECT
    SUBSTR(application.build_id, 0, 8) as build_id,
    client_id,
    udf_js_json_extract_highest_long_spinner(payload.histograms.FX_TAB_SWITCH_SPINNER_VISIBLE_LONG_MS) AS highest_long,
    udf_js_json_extract_highest_long_spinner(payload.histograms.FX_TAB_SWITCH_SPINNER_VISIBLE_MS) as highest_short
FROM telemetry.main
WHERE
    application.channel='nightly'
    AND normalized_os='Windows'
    AND application.build_id &amp;gt; FORMAT_DATE("%Y%m%d", DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER))
    AND DATE(submission_timestamp) &amp;gt;= DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER))
group by build_id, client_id) group by build_id;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In addition to being much simpler, this new job is also &lt;em&gt;way&lt;/em&gt; cheaper. The last run of it scanned just over 1 TB of data, meaning it cost us just over $5. Not as cheap as I might like, but considerably less expensive than before: I&amp;rsquo;ve also scheduled it to only run once every other day, since Mike tells me he doesn&amp;rsquo;t need this data any more often than that.&lt;/p&gt;

&lt;p&gt;[1] I understand that Jeff Klukas, Frank Bertsch, Daniel Thorn, Anthony Miyaguchi, and Wesley Dawson are the principals involved - apologies if I&amp;rsquo;m forgetting someone.&lt;/p&gt;</content></entry></feed>