<?xml version="1.0" encoding="utf-8"?> 
<rss version="2.0">
 <channel>
  <title>William Lachance's Log: William Lachance's Log</title>
  <description>William Lachance's Log: William Lachance's Log</description>
  <link>https://wlach.github.io/index.html</link>
  <lastBuildDate>Tue, 10 Nov 2020 14:09:20 UT</lastBuildDate>
  <pubDate>Tue, 10 Nov 2020 14:09:20 UT</pubDate>
  <ttl>1800</ttl>
  <item>
   <title>iodide retrospective</title>
   <link>https://wlach.github.io/blog/2020/11/iodide-retrospective/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2020-11-iodide-retrospective</guid>
   <pubDate>Tue, 10 Nov 2020 14:09:20 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;A bit belatedly, I thought I&amp;rsquo;d write a bit about &lt;a href="https://alpha.iodide.io"&gt;Iodide&lt;/a&gt;: an effort to create a compelling, client-centred scientific notebook environment.&lt;/p&gt;

&lt;p&gt;Despite not writing a ton about it (the sole exception being &lt;a href="/blog/2019/03/new-ideas-old-buildings/"&gt;this brief essay&lt;/a&gt; about my conservative choice for the server backend) Iodide took up a very large chunk of my mental energy from late 2018 through 2019. It was also essentially my only attempt at working on something that was on its way to being an actual &lt;em&gt;product&lt;/em&gt; while at Mozilla: while I&amp;rsquo;ve led other projects that have been interesting and/or impactful in my 9 odd-years here (&lt;a href="https://mozilla.github.io/mozregression/"&gt;mozregression&lt;/a&gt; and &lt;a href="https://wiki.mozilla.org/EngineeringProductivity/Projects/Perfherder"&gt;perfherder&lt;/a&gt; being the biggest successes), they fall firmly into the &amp;ldquo;internal tools supporting another product&amp;rdquo; category.&lt;/p&gt;

&lt;p&gt;At this point it&amp;rsquo;s probably safe to say that most of the project has wound down: no one is being paid to work on Iodide and it&amp;rsquo;s essentially in extreme maintenance mode. Before it&amp;rsquo;s put to bed altogether, I&amp;rsquo;d like to write a few notes about its approach, where I think it had big advantanges, and where it seems to fall short. I&amp;rsquo;ll conclude with some areas I&amp;rsquo;d like to explore (or would like to see others explore!). I&amp;rsquo;d like to emphasize that this is &lt;em&gt;my opinion only&lt;/em&gt;: the rest of the Iodide core team no doubt have their own thoughts. That said, let&amp;rsquo;s jump in.&lt;/p&gt;

&lt;h2 id="what-is-iodide-anyway"&gt;What is Iodide, anyway?&lt;/h2&gt;

&lt;p&gt;One thing that&amp;rsquo;s interesting about a product like Iodide is that people tend to project their hopes and dreams onto it: if you asked any given member of the Iodide team to give a 200 word description of the project, you&amp;rsquo;d probably get a slightly different answer emphasizing different things. That said, a fair initial approximation of the project vision would be &amp;ldquo;a scientific notebook like Jupyter, but running in the browser&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;What does this mean? First, let&amp;rsquo;s talk about what Jupyter does: at heart, it&amp;rsquo;s basically a Python &amp;ldquo;kernel&amp;rdquo; (read: interpreter), fronted by a webserver. You send snipits of the code to the interpreter via a web interface and they would faithfully be run on the backend (either on your local machine in a separate process or a server in the cloud). Results are then be returned back to the web interface and then rendered by the browser in one form or another.&lt;/p&gt;

&lt;p&gt;Iodide&amp;rsquo;s model is quite similar, with one difference: instead of running the kernel in another process or somewhere in the cloud, all the heavy lifting happens &lt;em&gt;in the browser itself&lt;/em&gt;, using the local JavaScript and/or WebAssembly runtime. The very first version of the product that Hamilton Ulmer and Brendan Colloran came up with was &lt;em&gt;definitely&lt;/em&gt; in this category: it had no server-side component whatsoever.&lt;/p&gt;

&lt;p&gt;Truth be told, even applications like Jupyter do a fair amount of computation on the client side to render a user interface and the results of computations: the distinction is not as clear cut as you might think. But in general I think the premise holds: if you load an iodide notebook today (still possible on alpha.iodide.io! no account required), the only thing that comes from the server is a little bit of static JavaScript and a flat file delivered as a JSON payload. All the &amp;ldquo;magic&amp;rdquo; of whatever computation you might come up with happens on the client.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s take a quick look at the default tryit iodide notebook to give an idea of what I mean:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;%% fetch
// load a js library and a csv file with a fetch cell
js: https://cdnjs.cloudflare.com/ajax/libs/d3/4.10.2/d3.js
text: csvDataString = https://data.sfgov.org/api/views/5cei-gny5/rows.csv?accessType=DOWNLOAD

%% js
// parse the data using the d3 library (and show the value in the console)
parsedData = d3.csvParse(csvDataString)

%% js
// replace the text in the report element "htmlInMd"
document.getElementById('htmlInMd').innerText = parsedData[0].Address

%% py
# use python to select some of the data
from js import parsedData
[x['Address'] for x in parsedData if x['Lead Remediation'] == 'true']&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;%%&lt;/code&gt; delimiters indicate individual cells. The &lt;code&gt;fetch&lt;/code&gt; cell is an iodide-native cell with its own logic to load the specified resources into a JavaScript variables when evaluated. &lt;code&gt;js&lt;/code&gt; and &lt;code&gt;py&lt;/code&gt; cells direct the browser to interpret the code inside of them, which causes the DOM to mutate. From these building blocks (and a few others), you can build up an interactive report which can &lt;em&gt;also&lt;/em&gt; be freely forked and edited by anyone who cares to.&lt;/p&gt;

&lt;p&gt;In some ways, I think Iodide has more in common with services like Glitch or Codepen than with Jupyter. In effect, it&amp;rsquo;s mostly offering a way to build up a static web page (doing whatever) using web technologies&amp;mdash; even if the user interface affordances and cell-based computation model might remind you more of a scientific notebook.&lt;/p&gt;

&lt;h2 id="what-works-well-about-this-approach"&gt;What works well about this approach&lt;/h2&gt;

&lt;p&gt;There&amp;rsquo;s a few nifty things that come from doing things this way:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;The environment is easily extensible for those familiar with JavaScript or other client side technologies. There is no need to mess with strange plugin architectures or APIs or conform to the opinions of someone else on what options there are for data visualization and presentation. If you want to use jQuery inside your notebook for a user interface widget, just import it and go!&lt;/li&gt;
 &lt;li&gt;The architecture scales to many, many users: since all the computation happens on the client, the server&amp;rsquo;s only role is to store and retrieve notebook content. alpha.iodide.io has happily been running on Heroku&amp;rsquo;s least expensive dyno type for its entire existence.&lt;/li&gt;
 &lt;li&gt;Reproducibility: so long as the iodide notebook has no references to data on third party servers with authentiction, there is no stopping someone from being able to reproduce whatever computations led to your results.&lt;/li&gt;
 &lt;li&gt;Related to reproducibility, it&amp;rsquo;s easy to &lt;em&gt;build off&lt;/em&gt; of someone else&amp;rsquo;s notebook or exploration, since everything is so self-contained.&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;I continue to be surprised and impressed with what people come up with in the Jupyter ecosystem so I don&amp;rsquo;t want to claim these are things that &amp;ldquo;only Iodide&amp;rdquo; (or other tools like it) can do&amp;mdash; what I will say is that I haven&amp;rsquo;t seen many things that combine both the conciseness and expressiveness of iomd. The beauty of the web is that there is such an abundance of tutorials and resources to support creating interactive content: when building iodide notebooks, I would freely borrow from resources such as MDN and Stackoverflow, instead of being locked into what the authoring software thinks one should be able express.&lt;/p&gt;

&lt;h2 id="whats-awkward"&gt;What&amp;rsquo;s awkward&lt;/h2&gt;

&lt;p&gt;Every silver lining has a cloud and (unfortunately) Iodide has a pretty big one. Depending on how you use Iodide, you will almost certainly run into the following problems:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;You are limited by the computational affordances provided by the browser: there is no obvious way to offload long-running or expensive computations to a cluster of machines using a technology like &lt;a href="https://spark.apache.org/"&gt;Spark&lt;/a&gt;.&lt;/li&gt;
 &lt;li&gt;Long-running computations will block the main thread, causing your notebook to become extremely unresponsive for the duration.&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;The first, I (and most people?) can usually live with. Computers are quite powerful these days and most data tasks I&amp;rsquo;m interested in are easily within the range of capabilities of my laptop. In the cases where I need to do something larger scale, I&amp;rsquo;m quite happy to fire up a Jupyter Notebook, BigQuery Console, or &amp;lt;other tool of choice&amp;gt; to do the heavy-lifting, and then move back to a client-side approach to visualize and explore my results.&lt;/p&gt;

&lt;p&gt;The second is much harder to deal with, since it means that the process of exploration that is so key to scientific computing. I&amp;rsquo;m quite happy to wait 5 seconds, 15 seconds, even a minute for a longer-running computation to complete but if I see the slow script dialog and everything about my environment grinds to a halt, it not only blocks my work but causes me to lose faith in the system. How do I know if it&amp;rsquo;s even going to finish?&lt;/p&gt;

&lt;p&gt;This occurs way more often than you might think: even a simple notebook loading pandas (via pyodide) can cause Firefox to pop up the slow script dialog:&lt;/p&gt;

&lt;div class="figure"&gt;&lt;img src="/files/2020/11/iodide-slow-script.png" alt="" /&gt;
 &lt;p class="caption"&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;Why is this so? Aren&amp;rsquo;t browsers complex beasts which can do a million things in parallel these days? Only sort of: while the graphical and network portions of loading and using a web site are highly parallel, JavaScript and any changes to the DOM &lt;em&gt;can only occur synchronously&lt;/em&gt; by default. Let&amp;rsquo;s break down a simple iodide example which trivially hangs the browser:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;%% js

while (1) { true }&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href="https://alpha.iodide.io/notebooks/6487/"&gt;link&lt;/a&gt; (but you really don&amp;rsquo;t want to)&lt;/p&gt;

&lt;p&gt;What&amp;rsquo;s going on here? When you execute that cell, the web site&amp;rsquo;s sole thread is now devoted to running that infinite loop. No longer can any other JavaScript-based event handler be run, so for example the text editor (which uses &lt;a href="https://microsoft.github.io/monaco-editor/"&gt;Monaco&lt;/a&gt; under the hood) and menus are now completely unresponsive.&lt;/p&gt;

&lt;p&gt;The iodide team was aware of this issue since I joined the project. There were no end of discussions about how to work around it, but they never really come to a satisfying conclusion. The most obvious solution is to move the cell&amp;rsquo;s computation to a web worker, but workers don&amp;rsquo;t have synchronous access to the DOM which is required for web content to work as you&amp;rsquo;d expect. While there are projects like &lt;a href="https://github.com/GoogleChromeLabs/comlink"&gt;ComLink&lt;/a&gt; that attempt to bridge this divide, they require both a fair amount of skill and code to use effectively. As mentioned above, one of the key differentiators between iodide and other notebook environments is that tools like jQuery, d3, etc. &amp;ldquo;just work&amp;rdquo;. That is, you can take a code snipit off the web and run it inside an iodide notebook: there&amp;rsquo;s no workaround I&amp;rsquo;ve been able to find which maintains that behaviour &lt;em&gt;and&lt;/em&gt; ensures that the Iodide notebook environment is always responsive.&lt;/p&gt;

&lt;p&gt;It took a while for this to really hit home, but having some separation from the project, I&amp;rsquo;ve come to realize that the problem is that our underlying technology isn&amp;rsquo;t designed for the task we were asking of it, nor is it likely to ever be in the near future. While the web (by which I mean not only the browser, but the ecosystem of popular tooling and libraries that has been built on top of it) has certainly grown in scope to things it was never envisioned to handle like games and office applications, it&amp;rsquo;s just not optimized for what I&amp;rsquo;d call &amp;ldquo;editable content&amp;rdquo;. That is, the situation where a web page offers affordances for manipulating its own representation.&lt;/p&gt;

&lt;p&gt;While modern web browsers have evolved to (sort of) protect one errant site from bringing the whole thing down, they certainly haven&amp;rsquo;t evolved to protect a particular site against &lt;em&gt;itself&lt;/em&gt;. And why would they? Web developers usually work in the terminal and text editor: the assumption is that if their test code misbehaves, they&amp;rsquo;ll just kill their tab and start all over again. Application state is persisted either on-disk or inside an external database, so nothing will really be lost.&lt;/p&gt;

&lt;p&gt;Could this ever change? Possibly, but I think it would be a radical rethinking of what the web is, and I&amp;rsquo;m not really sure what would motivate it.&lt;/p&gt;

&lt;h2 id="a-way-forward"&gt;A way forward&lt;/h2&gt;

&lt;p&gt;While working on iodide, we were fond of looking at this diagram, which was taken from &lt;a href="https://dl.acm.org/doi/10.1145/3173574.3173606"&gt;this study of the data science workflow&lt;/a&gt;:&lt;/p&gt;

&lt;div class="figure"&gt;&lt;img src="/files/2020/11/ds-workflow.png" alt="" /&gt;
 &lt;p class="caption"&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;It describes how people typically perform computational inquiry: typically you would poke around with some raw data, run some transformations on it. Only after that process was complete would you start trying to build up a &amp;ldquo;presentation&amp;rdquo; of your results to your audience.&lt;/p&gt;

&lt;p&gt;Looking back, it&amp;rsquo;s clear that Iodide&amp;rsquo;s strong suit was the &lt;em&gt;explanation&lt;/em&gt; part of this workflow, rather than collaboration and exploration. My strong suspicion is that we actually want to use different tools for each one of these tasks. Coincidentally, this also maps to the bulk of my experience working with data at Mozilla, using iodide or not: my most successful front-end data visualization projects were typically the distilled result of a very large number of adhoc explorations (python scripts, SQL queries, Jupyter notebooks, &amp;hellip;). The actual visualization itself contains very little computational meat: basically &amp;ldquo;just enough&amp;rdquo; to let the user engage with the data fruitfully.&lt;/p&gt;

&lt;p&gt;Unfortunately much of my work in this area uses semi-sensitive internal Mozilla data so can&amp;rsquo;t be publicly shared, but here&amp;rsquo;s one example:&lt;/p&gt;

&lt;div class="figure"&gt;&lt;img src="/files/2020/11/bq-migration-burndown.png" alt="" /&gt;
 &lt;p class="caption"&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href="https://alpha.iodide.io/notebooks/3593/?viewMode=report"&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I built this dashboard in a single day to track our progress (resolved/open bugs) when we were migrating our data platform from AWS to GCP. It was useful: it let us quickly visualize which areas needed more attention and gave upper management more confidence that we were moving closer to our goal. However, there was little computation going on in the client: most of the backend work was happening in our &lt;a href="https://bugzilla.mozilla.org"&gt;Bugzilla instance&lt;/a&gt;: the iodide notebook just does a little bit of post-processing to visualize the results in a useful way.&lt;/p&gt;

&lt;p&gt;Along these lines, I still think there is a place in the world for an interactive &lt;em&gt;visualization&lt;/em&gt; environment built on principles similar to iodide: basically a markdown editor augmented with primitives oriented around data exploration (for example, charts and data tables), with allowances to bring in any JavaScript library you might want. However, any in-depth data processing would be assumed to have mostly been run elsewhere, in advance. The editing environment could either be the browser or a code editor running on your local machine, per user preference: in either case, there would not be any really important computational state running in the browser, so things like dynamic reloading (or just closing an errant tab) should not cause the user to lose any work.&lt;/p&gt;

&lt;p&gt;This would give you the best of both worlds: you could easily create compelling visualizations that are easy to share and remix at minimal cost (because all the work happens in the browser), but you could &lt;em&gt;also&lt;/em&gt; use whatever existing tools work best for your initial data exploration (whether that be JavaScript or the more traditional Python data science stack). And because the new tool has a reduced scope, I think building such an environment would be a much more tractable project for an individual or small group to pursue.&lt;/p&gt;

&lt;p&gt;More on this in the future, I hope.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Many thanks to &lt;a href="https://teonbrooks.com/"&gt;Teon Brooks&lt;/a&gt; and Devin Bayly for reviewing an early draft of this post&lt;/em&gt;&lt;/p&gt;</description></item>
  <item>
   <title>mozregression and older builds</title>
   <link>https://wlach.github.io/blog/2020/08/mozregression-and-older-builds/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2020-08-mozregression-and-older-builds</guid>
   <pubDate>Mon, 17 Aug 2020 19:01:57 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;Periodically the discussion comes up about pruning away old stored Firefox build artifacts in S3. Each build is tens of megabytes, multiply that by the number of platforms we support and the set of revisions we churn through on a daily basis, and pretty soon you&amp;rsquo;re talking about real money.&lt;/p&gt;

&lt;p&gt;This came up recently in a discussion about &lt;a href="https://groups.google.com/g/mozilla.dev.platform/c/0OV_M3b-cMM/m/kkknfFdECgAJ"&gt;removing the legacy taskcluster deployment&lt;/a&gt; &amp;mdash; what do we actually lose by cutting back our archive of integration builds? The main reason to keep them around is to facilitate bisection testing with &lt;a href="https://mozilla.github.io/mozregression/"&gt;mozregression&lt;/a&gt;, to find out when a bug was introduced. Up to now, discussions about this have been a bit hand-wavey: we do keep logs about who&amp;rsquo;s accessing old builds, but it&amp;rsquo;s never been clear whether it was mozregression accessing them or something else.&lt;/p&gt;

&lt;p&gt;Happily, now that &lt;a href="/blog/2020/05/this-week-in-glean-mozregression-telemetry-part-2/"&gt;mozregression has some telemetry&lt;/a&gt;, it&amp;rsquo;s a little easier to get some answers on what people are actually doing. This query gets the distribution of build ages (launched or bisected) over the past 6 months, at a month long granularity.&lt;sup&gt;&lt;a href="#2020-08-17-mozregression-and-older-builds-footnote-1-definition" name="2020-08-17-mozregression-and-older-builds-footnote-1-return"&gt;1&lt;/a&gt;&lt;/sup&gt; Ages are relative to the date mozregression was launched: for example, if someone asked for a build from May 2019 in June 2020, the number would be &amp;ldquo;13&amp;rdquo;.&lt;/p&gt;

&lt;div class="brush: sql"&gt;
 &lt;div class="colorful"&gt;
  &lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;usage_app&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
       &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;usage_build_type&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;build_type&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
       &lt;span class="n"&gt;DATE_DIFF&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;DATE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;submission_timestamp&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="k"&gt;IF&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;LENGTH&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;usage_bad_date&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PARSE_DATE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;%Y-%m-%d&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;substr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;usage_bad_date&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;PARSE_DATE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;%Y-%m-%d&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;substr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;usage_launch_date&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt; &lt;span class="k"&gt;MONTH&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;build_age&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="n"&gt;moz&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;fx&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;data&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;shared&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;prod&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;org_mozilla_mozregression&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;usage&lt;/span&gt;
&lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="nb"&gt;DATE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;submission_timestamp&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;DATE_SUB&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;CURRENT_DATE&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="nb"&gt;INTERVAL&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt; &lt;span class="k"&gt;MONTH&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="n"&gt;client_info&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;app_display_version&lt;/span&gt; &lt;span class="k"&gt;NOT&lt;/span&gt; &lt;span class="k"&gt;LIKE&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;%dev%&amp;#39;&lt;/span&gt;
  &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="k"&gt;LENGTH&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;usage_build_type&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
  &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;LENGTH&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;usage_bad_date&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
       &lt;span class="k"&gt;OR&lt;/span&gt; &lt;span class="k"&gt;LENGTH&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;usage_launch_date&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;I ran this query on sql.telemetry.mozilla.org and generated a box plot, broken down by product and build type:&lt;/p&gt;

&lt;div class="figure"&gt;&lt;img src="/files/2020/08/mozregression-build-age-box-plot.png" alt="" /&gt;
 &lt;p class="caption"&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href="https://sql.telemetry.mozilla.org/queries/73968#184980"&gt;link&lt;/a&gt; (requires Mozilla LDAP)&lt;/p&gt;

&lt;p&gt;Unsurprisingly, Firefox shippable builds are the number one thing people try to bisect. Let&amp;rsquo;s take a little bit of a closer look at what&amp;rsquo;s going on there:&lt;/p&gt;

&lt;div class="figure"&gt;&lt;img src="/files/2020/08/mozregression-build-age-box-plot-detail.png" alt="" /&gt;
 &lt;p class="caption"&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;The median value is 1, which indicates that most people are bisecting builds within one month of the day in which mozregression was run. And the &lt;a href="https://www.statisticshowto.com/upper-and-lower-fences/"&gt;upper fence&lt;/a&gt; result is 6, suggesting that most of the time people are looking at a regression range that is within a 6 month range. However, looking more closely at the data points themselves (the little points in the chart above), there are a considerable number of outliers where a range greater than 20 months was asked for.&lt;/p&gt;

&lt;p&gt;&amp;hellip; which brings up to the question that we want to answer. Given that getting old builds isn&amp;rsquo;t that common (which we sort of knew already, based on the access patterns in the S3 logs), what is the impact of the times that we do? And it&amp;rsquo;s here where I have to throw up my hands and say &amp;ldquo;I don&amp;rsquo;t know&amp;rdquo; and suggest that we go back to empirical observation and user research.&lt;/p&gt;

&lt;p&gt;You can go back to the thread I linked above, and see that core Firefox/Gecko developers find the ability to get a precise regression range for older revisions valuable. One thing that&amp;rsquo;s worth mentioning is that mozregression isn&amp;rsquo;t run that often, compared to a product that we ship: on the order of 50 to 100 times per a day. But when it comes to internal tooling, a small amount of use might have a big impact: if a mozregression invocation a developer a few hours (or more), that&amp;rsquo;s a real benefit to Firefox and Mozilla. The same argument might apply here, where a small number of bisections on older builds might have a disproportionate impact on the quality of the product.&lt;/p&gt;

&lt;div class="footnotes"&gt;
 &lt;ol&gt;
  &lt;li id="2020-08-17-mozregression-and-older-builds-footnote-1-definition" class="footnote-definition"&gt;
   &lt;p&gt;I only added the telemetry to capture this information &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1651401"&gt;relatively recently&lt;/a&gt;, so we&amp;rsquo;re actually only looking at about a month of data in this post. We&amp;rsquo;ll have more complete results later this year.&amp;nbsp;&lt;a href="#2020-08-17-mozregression-and-older-builds-footnote-1-return"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;</description></item>
  <item>
   <title>Mozilla Telemetry in 2020: From "Just Firefox" to a "Galaxy of Data"</title>
   <link>https://wlach.github.io/blog/2020/07/mozilla-telemetry-in-2020-from-just-firefox-to-a-galaxy-of-data/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2020-07-mozilla-telemetry-in-2020-from-just-firefox-to-a-galaxy-of-data</guid>
   <pubDate>Thu, 16 Jul 2020 18:42:44 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;&lt;em&gt;(“This Week in Glean” is a series of blog posts that the Glean Team at Mozilla is using to try to communicate better about our work. They could be release notes, documentation, hopes, dreams, or whatever: so long as it is inspired by Glean. You can find &lt;a href="https://mozilla.github.io/glean/book/appendix/twig.html"&gt;an index of all TWiG posts online.&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This is a special guest post by non-Glean-team member William Lachance!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In the last year or so, there&amp;rsquo;s been a significant shift in the way we (Data Engineering) think about application-submitted data @ Mozilla, but although we have a new application-based SDK based on these principles (&lt;a href="https://mozilla.github.io/glean/book/index.html"&gt;the Glean SDK&lt;/a&gt;), most of our &lt;a href="https://telemetry.mozilla.org"&gt;data tools&lt;/a&gt; and &lt;a href="https://docs.telemetry.mozilla.org"&gt;documentation&lt;/a&gt; have not yet been updated to reflect this new state of affairs.&lt;/p&gt;

&lt;p&gt;Much of this story is known &lt;em&gt;inside&lt;/em&gt; Mozilla Data Engineering, but I thought it might be worth jotting them down into a blog post as a point of reference for people outside the immediate team. Knowing this may provide some context for some our activities and efforts over the next year or two, at least until our tools, documentation, and tribal knowledge evolve.&lt;/p&gt;

&lt;p&gt;In sum, the key differences are:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Instead of just one application we care about, there are many.&lt;/li&gt;
 &lt;li&gt;Instead of just caring about (mostly&lt;sup&gt;&lt;a href="#2020-07-16-mozilla-telemetry-in-2020-footnote-1-definition" name="2020-07-16-mozilla-telemetry-in-2020-footnote-1-return"&gt;1&lt;/a&gt;&lt;/sup&gt;) one type of ping (the Firefox &lt;em&gt;main&lt;/em&gt; ping), an individual application may submit &lt;em&gt;many different&lt;/em&gt; types of pings in the course of their use.&lt;/li&gt;
 &lt;li&gt;Instead of having both probes (histogram, scalar, or other data type) &lt;em&gt;and&lt;/em&gt; bespoke parametric values in a JSON schema like the &lt;a href="https://firefox-source-docs.mozilla.org/toolkit/components/telemetry/data/environment.html"&gt;telemetry environment&lt;/a&gt;, there are now only &lt;em&gt;metric types&lt;/em&gt; which are explicitly defined as part of each ping.&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;The new world is pretty exciting and freeing, but there is some new domain complexity that we need to figure out how to navigate. I&amp;rsquo;ll discuss that in my last section.&lt;/p&gt;

&lt;h2 id="the-old-world-firefox-is-king"&gt;The Old World: Firefox is king&lt;/h2&gt;

&lt;p&gt;Up until roughly mid&amp;ndash;2019, Firefox was the centre of Mozilla&amp;rsquo;s data world (with the occasional nod to Firefox for Android, which uses the same source repository). The Data Platform (often called &amp;ldquo;Telemetry&amp;rdquo;) was explicitly designed to cater to the needs of Firefox Developers (and to a lesser extent, product/program managers) and a set of bespoke tooling was built on top of our data pipeline architecture - &lt;a href="https://ravitillo.wordpress.com/2017/01/23/an-overview-of-mozillas-data-pipeline/"&gt;this blog post from 2017 describes much of it&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In outline, the model is simple: on the client side, assuming a given user had not turned off Telemetry, during the course of a day&amp;rsquo;s operation Firefox would keep track of various measures, called &amp;ldquo;probes&amp;rdquo;. At the end of that duration, it would submit a JSON-encoded &amp;ldquo;main ping&amp;rdquo; to our servers with the probe information and &lt;a href="https://github.com/mozilla-services/mozilla-pipeline-schemas/blob/97bac7acaaa5cb328d7f0f7348f3ddaaae657eda/schemas/telemetry/main/main.4.schema.json"&gt;a bunch of other mostly hand-specified junk&lt;/a&gt;, which would then find its way to a &amp;ldquo;data lake&amp;rdquo; (read: an Amazon S3 bucket). On top of this, we provided a &lt;a href="https://github.com/mozilla/python_moztelemetry/"&gt;python API&lt;/a&gt; (built on top of &lt;a href="https://spark.apache.org/docs/latest/api/python/index.html"&gt;PySpark&lt;/a&gt;) which enabled people inside Mozilla to query all submitted pings across our usage population.&lt;/p&gt;

&lt;p&gt;The only type of low-level object that was hard to keep track of was the list of probes: Firefox is a complex piece of software and there are &lt;em&gt;many&lt;/em&gt; aspects of it we wanted to instrument to validate performance and quality of the product - especially on the more-experimental Nightly and Beta channels. To solve this problem, a &lt;a href="https://probes.telemetry.mozilla.org/"&gt;probe dictionary&lt;/a&gt; was created to help developers find measures that corresponded to the product area that they were interested in.&lt;/p&gt;

&lt;p&gt;On a higher-level, accessing this type of data using the python API quickly became slow and frustrating: the aggregation of years of Firefox ping data was hundreds of terabytes big, and even taking advantage of PySpark&amp;rsquo;s impressive capabilities, querying the data across any reasonably large timescale was slow and expensive. Here, the solution was to create derived datasets which enabled fast(er) access to pings and other derived measures, document them on docs.telemetry.mozilla.org, and then allow access to them through tools like &lt;a href="https://docs.telemetry.mozilla.org/tools/stmo.html"&gt;sql.telemetry.mozilla.org&lt;/a&gt; or the &lt;a href="https://telemetry.mozilla.org/new-pipeline/dist.html"&gt;Measurement Dashboard&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="the-new-world-more-of-everything"&gt;The New World: More of everything&lt;/h2&gt;

&lt;p&gt;Even in the old world, other products that submitted telemetry &lt;em&gt;existed&lt;/em&gt; (e.g. Firefox for Android, Firefox for iOS, the venerable FirefoxOS) but I would not call them first-class citizens. Most of our documentation treated them as (at best) weird edge cases. At the time of this writing, you can see this distinction clearly on docs.telemetry.mozilla.org where there is one (fairly detailed) tutorial called &amp;ldquo;Choosing a Desktop Dataset&amp;rdquo; while essentially all other products are lumped into &amp;ldquo;Choosing a Mobile Dataset&amp;rdquo;.&lt;/p&gt;

&lt;div class="figure"&gt;&lt;img src="/files/2020/07/docs-tmo-pic.png" alt="" /&gt;
 &lt;p class="caption"&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;While the new universe of mobile products are probably the most notable addition to our list of things we want to keep track of, they&amp;rsquo;re only one piece of the puzzle. Really we&amp;rsquo;re interested in measuring &lt;em&gt;all the things&lt;/em&gt; (in accordance with our &lt;a href="https://www.mozilla.org/en-US/about/policy/lean-data/"&gt;lean data practices&lt;/a&gt;, of course) including tools we use to &lt;em&gt;build our products&lt;/em&gt; like &lt;a href="https://wiki.mozilla.org/MozPhab"&gt;mozphab&lt;/a&gt; and &lt;a href="https://mozilla.github.io/mozregression"&gt;mozregression&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In expanding our scope, we&amp;rsquo;ve found that mobile (and other products) have different requirements that influence what data we would want to send and when. For example, sending one blob of JSON multiple times per day might make sense for performance metrics on a desktop product (which is usually on a fast, unmetered network) but is much less acceptable on mobile (where every byte counts). For this reason, it makes sense to have &lt;em&gt;different ping types&lt;/em&gt; for the same product, not just one. For example, Fenix (the new Firefox for Android) sends a tiny baseline ping&lt;sup&gt;&lt;a href="#2020-07-16-mozilla-telemetry-in-2020-footnote-2-definition" name="2020-07-16-mozilla-telemetry-in-2020-footnote-2-return"&gt;2&lt;/a&gt;&lt;/sup&gt; on every run to (roughly) measure daily active users and a larger metrics ping sent on a (roughly) daily interval to measure (for example) a distribution of page load times.&lt;/p&gt;

&lt;p&gt;Finally, we found that naively collecting certain types of data as raw histograms or inside the schema didn&amp;rsquo;t always work well. For example, encoding session lengths as plain integers &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1514392"&gt;would often produce weird results in the case of clock skew&lt;/a&gt;. For this reason, we decided to &lt;a href="https://mozilla.github.io/glean/book/user/metrics/index.html"&gt;standardize on a set of well-defined metrics&lt;/a&gt; using Glean, which tries to minimize footguns. We explicitly no longer allow clients to submit arbitrary JSON or values as part of a telemetry ping: if you have a use case not covered by the existing metrics, &lt;a href="https://wiki.mozilla.org/Glean/Adding_or_changing_Glean_metric_types"&gt;make a case for it and add it to the list&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;To illustrate this, let&amp;rsquo;s take a (subset) of what we might be looking at in terms of what the Fenix application sends:&lt;/p&gt;

&lt;div class="figure"&gt;&lt;img src="/files/2020/07/fenix-pings-diagram.png" alt="" /&gt;
 &lt;p class="caption"&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href="/files/2020/07/fenix-pings-diagram.mmd"&gt;mermaid source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;At the top level we segment based on the &amp;ldquo;application&amp;rdquo; (just Fenix in this example). Just below that, there are the pings that this application might submit (I listed three: the baseline and metrics pings described above, along with a &amp;ldquo;migration&amp;rdquo; ping, which tracks metrics when a user migrates from Fennec to Fenix). And below &lt;em&gt;that&lt;/em&gt; there are different types of metrics included in the pings: I listed a few that came out of a quick scan of the Fenix BigQuery tables using my &lt;a href="https://mozilla-schema-dictionary.netlify.app/#!/tables/org_mozilla_fenix.metrics"&gt;prototype schema dictionary&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This is actually only the surface-level: at the time of this writing, Fenix has no fewer than 12 different ping types and &lt;em&gt;many&lt;/em&gt; different metrics inside each of them.&lt;sup&gt;&lt;a href="#2020-07-16-mozilla-telemetry-in-2020-footnote-3-definition" name="2020-07-16-mozilla-telemetry-in-2020-footnote-3-return"&gt;3&lt;/a&gt;&lt;/sup&gt; On a client level, the new Glean SDK provides easy-to-use primitives to help developers collect this type of information in a principled, privacy-preserving way: for example, &lt;a href="https://github.com/mozilla/data-review"&gt;data review&lt;/a&gt; is built into every metric type. But what about after it hits our ingestion endpoints?&lt;/p&gt;

&lt;p&gt;Hand-crafting schemas, data ingestion pipelines, and individualized ETL scripts for such a large matrix of applications, ping types, and measurements would quickly become intractable. Instead, we (Mozilla Data Engineering) refactored our data pipeline to parse out the information from the Glean schemas and then create tables in our BigQuery datastore corresponding to what&amp;rsquo;s in them - this has proceeded as an extension to our (now somewhat misnamed) &lt;a href="https://github.com/mozilla/probe-scraper"&gt;probe-scraper&lt;/a&gt; tool.&lt;/p&gt;

&lt;p&gt;You can then query this data directly (see &lt;a href="https://docs.telemetry.mozilla.org/concepts/glean/accessing_glean_data.html"&gt;accessing glean data&lt;/a&gt;) or build up a derived dataset using our SQL-based ETL system, &lt;a href="https://github.com/mozilla/bigquery-etl/"&gt;BigQuery-ETL&lt;/a&gt;. This part of the equation has been working fairly well, I&amp;rsquo;d say: we now have a diverse set of products producing Glean telemetry and submitting it to our servers, and the amount of manual effort required to add each application was minimal (aside from adding new capabilities to the platform as we went along).&lt;/p&gt;

&lt;p&gt;What hasn&amp;rsquo;t quite kept pace is our tooling to make navigating and using this new collection of data tractable.&lt;/p&gt;

&lt;h2 id="what-could-bring-this-all-together"&gt;What could bring this all together?&lt;/h2&gt;

&lt;p&gt;As mentioned before, this new world is quite powerful and gives Mozilla a bunch of new capabilities but it isn&amp;rsquo;t yet well documented and we lack the tools to easily connect the dots from &amp;ldquo;I have a product question&amp;rdquo; to &amp;ldquo;I know how to write an SQL query / Spark Job to answer it&amp;rdquo; or (better yet) &amp;ldquo;this product dashboard will answer it&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Up until now, our defacto answer has been some combination of &amp;ldquo;Use the probe dictionary / telemetry.mozilla.org&amp;rdquo; and/or &amp;ldquo;refer to docs.telemetry.mozilla.org&amp;rdquo;. I submit that we&amp;rsquo;re at the point where these approaches break down: as mentioned above, there are many more types of data we now need to care about than just &amp;ldquo;probes&amp;rdquo; (or &amp;ldquo;metrics&amp;rdquo;, in Glean-parlance). When we just cared about the main ping, we could write dataset documentation for its recommended access point (&lt;a href="https://docs.telemetry.mozilla.org/datasets/batch_view/main_summary/reference.html"&gt;main_summary&lt;/a&gt;) and the raw number of derived datasets was managable. But in this new world, where we have &lt;em&gt;N&lt;/em&gt; applications times &lt;em&gt;M&lt;/em&gt; ping types, the number of canonical ping tables are now so many that documenting them all on docs.telemetry.mozilla.org no longer makes sense.&lt;/p&gt;

&lt;p&gt;A few months ago, I thought that &lt;a href="https://cloud.google.com/data-catalog"&gt;Google&amp;rsquo;s Data Catalog&lt;/a&gt; (billed as offering &amp;ldquo;a unified view of all your datasets&amp;rdquo;) might provide a solution, but on further examination it only solves part of the problem: it provides only a view on your BigQuery tables and it isn&amp;rsquo;t designed to provide detailed information on the domain objects we care about (products, pings, measures, and tools). You can map some of the properties from these objects onto the tables (e.g. adding a probe&amp;rsquo;s description field to the column representing it in the BigQuery table), but Data Calalog&amp;rsquo;s interface to surfacing and filtering through this information is rather slow and clumsy and requires detailed knowledge of how these higher level concepts relate to BigQuery primitives.&lt;/p&gt;

&lt;p&gt;Instead, what I think we need is a &lt;em&gt;new system&lt;/em&gt; which allows a data practitioner (Data Scientist, Firefox Engineer, Data Engineer, Product Manager, whoever) to visualize the relevant set of domain objects relevant to their product/feature of interest &lt;em&gt;quickly&lt;/em&gt; then map them to specific BigQuery tables and other resources (e.g. visualizations using tools like &lt;a href="https://github.com/mozilla/glam"&gt;GLAM&lt;/a&gt;) which allow people to quickly answer questions so we can make better products. Basically, I am thinking of some combination of:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;The existing probe dictionary (derived from existing product metadata)&lt;/li&gt;
 &lt;li&gt;A new &amp;ldquo;application&amp;rdquo; dictionary (derived from some simple to-be-defined application metadata description)&lt;/li&gt;
 &lt;li&gt;A new &amp;ldquo;ping&amp;rdquo; dictionary (derived from existing product metadata)&lt;/li&gt;
 &lt;li&gt;A BigQuery schema dictionary (I wrote up a &lt;a href="https://mozilla-schema-dictionary.netlify.app/"&gt;prototype of this a couple weeks ago&lt;/a&gt;) to map between these higher-level objects and what&amp;rsquo;s in our low-level data store&lt;/li&gt;
 &lt;li&gt;Documentation for derived datasets generated by BigQuery-ETL (ideally stored alongside the ETL code itself, so it&amp;rsquo;s easy to keep up to date)&lt;/li&gt;
 &lt;li&gt;A data tool dictionary describing how to easily &lt;em&gt;access&lt;/em&gt; the above data in various ways (e.g. SQL query, dashboard plot, etc.)&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;This might sound ambitious, but it&amp;rsquo;s basically just a system for collecting and visualizing various types of documentation&amp;mdash; something we have proven we know how to do. And I think a product like this could be incredibly empowering, not only for the internal audience at Mozilla but also the &lt;em&gt;external&lt;/em&gt; audience who wants to support us but has valid concerns about what we&amp;rsquo;re collecting and why: since this system is based entirely on systems which are already open (inside GitHub or Mercurial repositories), there is no reason we can&amp;rsquo;t make it available to the public.&lt;/p&gt;

&lt;div class="footnotes"&gt;
 &lt;ol&gt;
  &lt;li id="2020-07-16-mozilla-telemetry-in-2020-footnote-1-definition" class="footnote-definition"&gt;
   &lt;p&gt;Technically, &lt;a href="https://docs.telemetry.mozilla.org/datasets/pings.html"&gt;there are various other types of pings&lt;/a&gt; submitted by Firefox, but the main ping is the one 99% of people care about.&amp;nbsp;&lt;a href="#2020-07-16-mozilla-telemetry-in-2020-footnote-1-return"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
  &lt;li id="2020-07-16-mozilla-telemetry-in-2020-footnote-2-definition" class="footnote-definition"&gt;
   &lt;p&gt;This is actually a capability that the Glean SDK provides, so other products (e.g. Lockwise, Firefox for iOS) also benefit from this capability.&amp;nbsp;&lt;a href="#2020-07-16-mozilla-telemetry-in-2020-footnote-2-return"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
  &lt;li id="2020-07-16-mozilla-telemetry-in-2020-footnote-3-definition" class="footnote-definition"&gt;
   &lt;p&gt;The scope of this data collection comes from the fact that Fenix is a &lt;em&gt;very&lt;/em&gt; large and complex application. rather than a desire to collect everything just because we can&amp;mdash; smaller efforts like mozregression collect a &lt;a href="https://mozilla.github.io/mozregression/documentation/telemetry.html"&gt;much more limited set of data&lt;/a&gt;.&amp;nbsp;&lt;a href="#2020-07-16-mozilla-telemetry-in-2020-footnote-3-return"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;</description></item>
  <item>
   <title>mozregression GUI: now available for Linux</title>
   <link>https://wlach.github.io/blog/2020/06/mozregression-gui-now-available-for-linux/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2020-06-mozregression-gui-now-available-for-linux</guid>
   <pubDate>Mon, 29 Jun 2020 20:39:06 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;Thanks to &lt;a href="https://github.com/AnAverageHuman"&gt;@AnAverageHuman&lt;/a&gt;, &lt;a href="https://mozilla.github.io/mozregression"&gt;mozregression&lt;/a&gt; once again has an easy to use and install GUI version for Linux! This used to work a few years ago, but got broken with some changes in the mozregression-python2 era and didn&amp;rsquo;t get resolved until now:&lt;/p&gt;

&lt;div class="figure"&gt;&lt;img src="/files/2020/06/mozregression-on-linux.png" alt="" /&gt;
 &lt;p class="caption"&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;This is an area where using telemetry in mozregression can help us measure the impact of a change like this: although Windows still dominates in terms of marketshare, Linux is very widely used by contributors &amp;mdash; of the usage of mozregression in the past 2 months, fully 30% of the sessions were on Linux (and it is possible we were undercounting that due to &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1646402"&gt;bug 1646402&lt;/a&gt;):&lt;/p&gt;

&lt;div class="figure"&gt;&lt;img src="/files/2020/06/mozregression-usage-share-by-os.png" alt="" /&gt;
 &lt;p class="caption"&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href="https://sql.telemetry.mozilla.org/queries/72363/source"&gt;link to query&lt;/a&gt; (internal-only)&lt;/p&gt;

&lt;p&gt;It will be interesting to watch the usage numbers for Linux evolve over the next few months. In particular, I&amp;rsquo;m curious to see what percentage of users on that platform prefer a GUI.&lt;/p&gt;

&lt;h2 id="appendix-reducing-mozregression-guis-massive-size"&gt;Appendix: reducing mozregression-GUI&amp;rsquo;s massive size&lt;/h2&gt;

&lt;p&gt;One thing that&amp;rsquo;s bothered me a bunch lately is that the mozregression GUI&amp;rsquo;s size is &lt;em&gt;massive&lt;/em&gt; and this is even more apparent on Linux, where the initial distribution of the GUI came in at over 120 megabytes! Why so big? There were a few reasons:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;&lt;a href="https://pypi.org/project/PySide2/"&gt;PySide2&lt;/a&gt; (the GUI library we use) is very large (10s of megabytes), and &lt;a href="https://www.pyinstaller.org/"&gt;PyInstaller&lt;/a&gt; packages &lt;em&gt;all of it&lt;/em&gt; by default into your application distribution.&lt;/li&gt;
 &lt;li&gt;The binary/rust portions of the &lt;a href="https://pypi.org/project/glean-sdk/"&gt;Glean Python SDK&lt;/a&gt; were been built with debugging information included (basically as a carry-over when it was a pre-alpha product), which made it 38 megabytes big (!) on Linux.&lt;/li&gt;
 &lt;li&gt;On Linux at least, a large number of other system libraries are packaged into the distribution.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;A few aspects of this were under our control: Ian Moody (:Kwan) and myself crafted a script to manually remove unneeded PySide2 libraries as part of the packaging process. The Glean team was awesome-as-always and quickly rebuilt Glean without debugging information (this was basically an oversight). Finally, I managed to shave off a few more megabytes by reverting the Linux build to an earlier version of Ubuntu (Xenial), which is something I had been meaning to do anyway.&lt;/p&gt;

&lt;p&gt;Even after doing all of these things, the end result is still a little underwhelming: the mozregression GUI distribution on Linux is still 79.5 megabytes big. There are probably other things we could do, but we&amp;rsquo;re definitely entering the land of diminishing returns.&lt;/p&gt;

&lt;p&gt;Honestly, my main takeaway is just not to build an application like this in Python unless you absolutely have to (e.g. you&amp;rsquo;re building an application which needs system-level access). The web is a pretty wonderful medium for creating graphical applications these days, and by using it you sidestep these type of installation issues.&lt;/p&gt;</description></item>
  <item>
   <title>The humble blog</title>
   <link>https://wlach.github.io/blog/2020/05/the-humble-blog/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2020-05-the-humble-blog</guid>
   <pubDate>Sun, 24 May 2020 18:01:24 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;I&amp;rsquo;ve been thinking a lot about markdown, presentation, dashboards, and other frontendy sorts of things lately, partly inspired by my work on &lt;a href="https://iodide.io"&gt;Iodide&lt;/a&gt; last year, partly inspired by my recent efforts on improving &lt;a href="https://docs.telemetry.mozilla.org"&gt;docs.telemetry.mozilla.org&lt;/a&gt;. I haven&amp;rsquo;t fully fleshed out my thoughts on this yet, but in general I think blogs (for some value of &amp;ldquo;blog&amp;rdquo;) are still a great way to communicate ideas and concepts to an interested audience.&lt;/p&gt;

&lt;p&gt;Like many organizations, Mozilla&amp;rsquo;s gone down the path of Google Docs, Zoom and Slack which makes me more than a little sad: good ideas disappear down the memory hole &lt;em&gt;super&lt;/em&gt; quickly with these tools, not to mention the fact that they are closed-by-default (even to people inside Mozilla!). My view on &amp;ldquo;open&amp;rdquo; is a bit more nuanced than it used to be: I no longer think everything need be all-public, all-the-time&amp;mdash; but I still think talking about and through our ideas (even if imperfectly formed or stated) with a broad audience builds trust and leads to better outcomes.&lt;/p&gt;

&lt;p&gt;Is there some way we can blend some of these old school ideas (blogs, newsgroups, open discussion forums) with better technology and &lt;a href="https://www.mozilla.org/en-US/about/governance/policies/participation/"&gt;social practices&lt;/a&gt;? Let&amp;rsquo;s find out.&lt;/p&gt;</description></item>
  <item>
   <title>A principled reorganization of docs.telemetry.mozilla.org</title>
   <link>https://wlach.github.io/blog/2020/05/a-principled-reorganization-of-docs-telemetry-mozilla-org/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2020-05-a-principled-reorganization-of-docs-telemetry-mozilla-org</guid>
   <pubDate>Mon, 11 May 2020 15:44:45 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;(this post is aimed primarily at an internal audience, but I thought I&amp;rsquo;d go ahead and make it public as a blog post)&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been thinking a bunch over the past few months about the Mozilla data organization&amp;rsquo;s documentation story. We have a first class data platform here at Mozilla, but using it to answer questions, especially for newer employees, can be quite intimidating. As we continue our collective journey to becoming a modern data-driven organization, part of the formula for unlocking this promise is making the tools and platforms we create accessible to a broad internal audience.&lt;/p&gt;

&lt;p&gt;My data peers are a friendly group of people and we have historically been good at answering questions on forums like the #fx-metrics slack channel: we&amp;rsquo;ll keep doing this. That said, our time is limited: we need a common resource for helping bring people up to speed on how to use the data platform to answer common questions.&lt;/p&gt;

&lt;p&gt;Our documentation site, &lt;a href="https://docs.telemetry.mozilla.org"&gt;docs.telemetry.mozilla.org&lt;/a&gt;, was meant to be this resource: however in the last couple of years an understanding of its purpose has been (at least partially) lost and it has become somewhat overgrown with content that isn&amp;rsquo;t very relevant to those it&amp;rsquo;s intended to help.&lt;/p&gt;

&lt;p&gt;This post&amp;rsquo;s goal is to re-establish a mission for our documentation site &amp;mdash; towards the end, some concrete proposals on what to change are also outlined.&lt;/p&gt;

&lt;h2 id="setting-the-audience"&gt;Setting the audience&lt;/h2&gt;

&lt;p&gt;docs.telemetry.mozilla.org was and is meant to be a resource useful for &lt;em&gt;data practitioners&lt;/em&gt; within Mozilla.&lt;/p&gt;

&lt;p&gt;Examples of different data practioners and their use cases:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;A data scientist performing an experiment analysis&lt;/li&gt;
 &lt;li&gt;A data analyst producing a report on the effectiveness of a recent marketing campaign&lt;/li&gt;
 &lt;li&gt;A Firefox engineer trying to understand the performance characteristics of a new feature&lt;/li&gt;
 &lt;li&gt;A technical product manager trying to understand the characteristics of a particular user segment&lt;/li&gt;
 &lt;li&gt;A quality assurance engineer trying to understand the severity of a Firefox crash&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;There are a range of skills that these different groups bring to the table, but there are some common things we expect a data practitioner to have, whatever their formal job description:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;At least some programming and analytical experience&lt;/li&gt;
 &lt;li&gt;Comfortable working with and understanding complex data systems with multiple levels of abstraction (for example: the relationship between the Firefox browser which produces Telemetry data and the backend system which processes it)&lt;/li&gt;
 &lt;li&gt;The time necessary to dig into details&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;This also &lt;em&gt;excludes&lt;/em&gt; a few groups:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Senior leadership or executives: they are of course free to use docs.telemetry.mozilla.org if helpful, but it is expected that the type of analytical work covered by the documentation will normally be done by a data practitioner and that relevant concepts and background information will be explained &lt;em&gt;to them&lt;/em&gt; (in the form of high-level dashboards, presentations, etc.).&lt;/li&gt;
 &lt;li&gt;Data Engineering: some of the material on docs.telemetry.mozilla.org may be incidentally useful to this internal audience, but explaining the full details of the data platform itself belongs elsewhere.&lt;/li&gt;&lt;/ul&gt;

&lt;h2 id="what-do-these-users-need"&gt;What do these users need?&lt;/h2&gt;

&lt;p&gt;In general, a data practitioner is trying to answer a specific set of questions in the context of an exploration. There are a few things that they need:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;A working knowledge of how to use the technological tools to answer the questions they might have: for example, how to create a SQL query counting the median value of a particular histogram for a particular usage segment.&lt;/li&gt;
 &lt;li&gt;A set of guidelines on best practices on how to measure specific things: for example, we want our people using our telemetry systems to use well-formulated practices for measuring things like &amp;ldquo;Monthly Active Users&amp;rdquo; rather than re-inventing such things themselves.&lt;/li&gt;&lt;/ul&gt;

&lt;h2 id="what-serves-this-need"&gt;What serves this need?&lt;/h2&gt;

&lt;p&gt;A few years ago, Ryan Harter &lt;a href="https://blog.harterrt.com/lit-review.html"&gt;did an extensive literature review&lt;/a&gt; on writing documentation on technical subjects - the take away from this exploration is that the global consensus is that we should focus most of our attention on writing practical tutorials which enables our users to perform specific tasks in the service of the above objective.&lt;/p&gt;

&lt;p&gt;There is a proverb, allegedly attributed to Confucius which goes something like this:&lt;/p&gt;

&lt;blockquote&gt;
 &lt;p&gt;&amp;ldquo;I hear and I forget. I see and I remember. I do and I understand.&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The understanding we want to build is how to use our data systems and tools to answer questions. Some knowledge of how our data platform works is no doubt necessary to accomplish this, but it is mostly functional knowledge we care about imparting to data practitioners: the best way to build this understanding is to guide users in performing tasks.&lt;/p&gt;

&lt;p&gt;This makes sense intuitively, but it is also borne out by the &lt;em&gt;data&lt;/em&gt; that this is what our users are looking for. Looking through the top pages on Google Analytics, virtually all of them&lt;sup&gt;&lt;a href="#2020-05-11-a-principled-reorganization-of-docs-telemetry-mozilla-org-footnote-1-definition" name="2020-05-11-a-principled-reorganization-of-docs-telemetry-mozilla-org-footnote-1-return"&gt;1&lt;/a&gt;&lt;/sup&gt; refer either to a cookbook or howto guide:&lt;/p&gt;

&lt;div class="figure"&gt;&lt;img src="/files/2020/05/docs-tmo-ga.png" alt="" /&gt;
 &lt;p class="caption"&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;Happily, this allows us to significantly narrow our focus for docs.telemetry.mozilla.org. We no longer need to worry about:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Providing lists or summaries of data tools available inside Mozilla&lt;sup&gt;&lt;a href="#2020-05-11-a-principled-reorganization-of-docs-telemetry-mozilla-org-footnote-2-definition" name="2020-05-11-a-principled-reorganization-of-docs-telemetry-mozilla-org-footnote-2-return"&gt;2&lt;/a&gt;&lt;/sup&gt;: we can talk about tools only as needed in the context of tasks they want to accomplish. We may want to keep such a list handy elsewhere for some other reason (e.g. internal auditing purposes), but we can safely say that it belongs somewhere else, like the Mozilla wiki, mana, or the telemetry.mozilla.org portal.&lt;/li&gt;
 &lt;li&gt;Detailed reference on the technical details of the data platform implementation. Links to this kind of material can be surfaced inside the documentation where relevant, but it is expected that an implementation reference will normally be stored and presented within the context of the tools themselves (a good example would be &lt;a href="https://mozilla.github.io/gcp-ingestion/"&gt;the existing documentation for GCP ingestion&lt;/a&gt;).&lt;/li&gt;
 &lt;li&gt;Detailed reference on all data sets, ping types, or probes: hand-written documentation for this kind of information is difficult to keep up to date with manual processes&lt;sup&gt;&lt;a href="#2020-05-11-a-principled-reorganization-of-docs-telemetry-mozilla-org-footnote-3-definition" name="2020-05-11-a-principled-reorganization-of-docs-telemetry-mozilla-org-footnote-3-return"&gt;3&lt;/a&gt;&lt;/sup&gt; and is best generated automatically and browsed with tools like the &lt;a href="https://probes.telemetry.mozilla.org/"&gt;probe dictionary&lt;/a&gt;.&lt;/li&gt;
 &lt;li&gt;Detailed reference material on how to submit Telemetry. While an overview of how to &lt;em&gt;think about&lt;/em&gt; submitting telemetry may be in scope (recall that we consider Firefox engineers a kind of data practitioner), the details are really a seperate topic that is better left to another resource which is closer to the implementation (for example, the &lt;a href="https://firefox-source-docs.mozilla.org/"&gt;Firefox Source Documentation&lt;/a&gt; or the &lt;a href="https://mozilla.github.io/glean/book/index.html"&gt;Glean SDK reference&lt;/a&gt;).&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Scanning through the above, you&amp;rsquo;ll see a common theme: avoid overly detailed reference material. The above is not to say that we should avoid background documentation altogether. For example, an understanding of how our data pipeline works is key to understanding how up-to-date a dashboard is expected to be. However, this type of documentation should be written bearing in mind the audience (focusing on what they need to know as data practitioners) and should be surfaced towards the end of the documentation as &lt;em&gt;supporting material&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;As an exception, there is also a very small amount of reference documentation which we want to put at top-level because it is so important: for example the &lt;a href="https://docs.telemetry.mozilla.org/metrics/index.html"&gt;standard metrics&lt;/a&gt; page describes how we define &amp;ldquo;MAU&amp;rdquo; and &amp;ldquo;DAU&amp;rdquo;: these are measures that we want to standardize in the organization, and not have someone re-invent every time they produce an analysis or report. However, we should be very cautious about how much of this &amp;ldquo;front facing&amp;rdquo; material we include: if we overwhelm our audience with details right out of the gate, they are apt to ignore them.&lt;/p&gt;

&lt;h2 id="concrete-actions"&gt;Concrete actions&lt;/h2&gt;

&lt;ul&gt;
 &lt;li&gt;We should continue working on tutorials on how to perform common tasks: this includes not only the low-level guides that we currently have (e.g. BigQuery and SQL tutorials) but also information on how to effectively use our higher-level, more accessible tools like GLAM and GUD to answer questions.&lt;/li&gt;
 &lt;li&gt;Medium term, we should remove the per-dataset documentation and replace it with a graphical tool for browsing this type of information (perhaps &lt;a href="https://cloud.google.com/data-catalog"&gt;Google Data Catalog&lt;/a&gt;). Since this is likely to be a rather involved project, we can keep the existing documentation for now &amp;mdash; but for new datasets, we should encourage their authors to write tutorials on how to use them effectively (assuming they are of broad general interest) instead of hand-creating schema definitions that are likely to go out of date quickly.&lt;/li&gt;
 &lt;li&gt;We should set clear expectations and guidelines of what does and doesn&amp;rsquo;t belong on docs.telemetry.mozilla.org as part of a larger style guide. This style guide should be referenced somewhere prominent (perhaps as part of a &lt;a href="https://help.github.com/en/github/building-a-strong-community/creating-a-pull-request-template-for-your-repository"&gt;pull request template&lt;/a&gt;) so that historical knowledge of what this resource is for isn&amp;rsquo;t lost.&lt;/li&gt;&lt;/ul&gt;

&lt;h2 id="footnotes"&gt;Footnotes&lt;/h2&gt;

&lt;div class="footnotes"&gt;
 &lt;ol&gt;
  &lt;li id="2020-05-11-a-principled-reorganization-of-docs-telemetry-mozilla-org-footnote-1-definition" class="footnote-definition"&gt;
   &lt;p&gt;For some reason which I cannot fathom, a broad (non-Mozilla) audience seems unusually interested in our SQL style guide.&amp;nbsp;&lt;a href="#2020-05-11-a-principled-reorganization-of-docs-telemetry-mozilla-org-footnote-1-return"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
  &lt;li id="2020-05-11-a-principled-reorganization-of-docs-telemetry-mozilla-org-footnote-2-definition" class="footnote-definition"&gt;
   &lt;p&gt;The current Firefox data documentation has a &lt;a href="https://docs.telemetry.mozilla.org/tools/projects.html"&gt;project glossary&lt;/a&gt; that is riddled with links to obsolete and unused projects.&amp;nbsp;&lt;a href="#2020-05-11-a-principled-reorganization-of-docs-telemetry-mozilla-org-footnote-2-return"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
  &lt;li id="2020-05-11-a-principled-reorganization-of-docs-telemetry-mozilla-org-footnote-3-definition" class="footnote-definition"&gt;
   &lt;p&gt;docs.telemetry.mozilla.org has a huge section devoted to derived datasets (20+), many of which are obsolete or not recommended. At the same time, we are missing explicit reference material for the most commonly used tables in BigQuery (e.g. &lt;code&gt;telemetry.main&lt;/code&gt;).&amp;nbsp;&lt;a href="#2020-05-11-a-principled-reorganization-of-docs-telemetry-mozilla-org-footnote-3-return"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;</description></item>
  <item>
   <title>This Week in Glean: mozregression telemetry (part 2)</title>
   <link>https://wlach.github.io/blog/2020/05/this-week-in-glean-mozregression-telemetry-part-2/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2020-05-this-week-in-glean-mozregression-telemetry-part-2</guid>
   <pubDate>Fri, 08 May 2020 14:32:40 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;&lt;em&gt;(“This Week in Glean” is a series of blog posts that the Glean Team at Mozilla is using to try to communicate better about our work. They could be release notes, documentation, hopes, dreams, or whatever: so long as it is inspired by Glean. You can find &lt;a href="https://mozilla.github.io/glean/book/appendix/twig.html"&gt;an index of all TWiG posts online.&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This is a special guest post by non-Glean-team member William Lachance!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This is a continuation of an exploration of adding Glean-based telemetry to a python application, in this case &lt;a href="https://mozilla.github.io/mozregression"&gt;mozregression&lt;/a&gt;, a tool for automatically finding the source of Firefox regressions (breakage).&lt;/p&gt;

&lt;p&gt;When we left off &lt;a href="/blog/2020/02/this-week-in-glean-special-guest-post-mozregression-telemetry-part-1/"&gt;last time&lt;/a&gt;, we had written some test scripts and verified that the data was visible in the debug viewer.&lt;/p&gt;

&lt;h2 id="adding-telemetry-to-mozregression-itself"&gt;Adding Telemetry to mozregression itself&lt;/h2&gt;

&lt;p&gt;In many ways, this is pretty similar to what I did inside the sample application: the only significant difference is that these are shipped inside a Python application that is meant to be be installable via &lt;a href="https://pypi.org/project/pip/"&gt;pip&lt;/a&gt;. This means we need to specify the &lt;code&gt;pings.yaml&lt;/code&gt; and &lt;code&gt;metrics.yaml&lt;/code&gt; (located inside the &lt;code&gt;mozregression&lt;/code&gt; subirectory) as package data inside &lt;code&gt;setup.py&lt;/code&gt;:&lt;/p&gt;

&lt;div class="brush: py"&gt;
 &lt;div class="colorful"&gt;
  &lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;setup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"mozregression"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="n"&gt;package_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;"mozregression"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"*.yaml"&lt;/span&gt;&lt;span class="p"&gt;]},&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;There were also a number of Glean SDK enhancements which we determined were necessary. Most notably, Michael Droettboom added 32-bit Windows wheels to the Glean SDK, which we need to make building the &lt;a href="https://mozilla.github.io/mozregression/quickstart.html#gui"&gt;mozregression GUI&lt;/a&gt; on Windows possible. In addition, some minor changes needed to be made to Glean&amp;rsquo;s behaviour for it to work correctly with a command-line tool like mozregression &amp;mdash; for example, Glean used to assume that Telemetry would always be disabled via a GUI action so that it would send a deletion ping, but this would obviously not work in an application like mozregression where there is only a configuration file &amp;mdash; so for this case, Glean needed to be modified to check if it had been disabled &lt;em&gt;between&lt;/em&gt; runs.&lt;/p&gt;

&lt;p&gt;Many thanks to Mike (and others on the Glean team) for so patiently listening to my concerns and modifying Glean accordingly.&lt;/p&gt;

&lt;h2 id="getting-data-review"&gt;Getting Data Review&lt;/h2&gt;

&lt;p&gt;At Mozilla, we don&amp;rsquo;t just allow random engineers like myself to start collecting data in a product that we ship (even a semi-internal like mozregression). We have &lt;a href="https://wiki.mozilla.org/Firefox/Data_Collection"&gt;a process&lt;/a&gt;, overseen by Data Stewards to make sure the information we gather is actually answering important questions and doesn&amp;rsquo;t unnecessarily collect personally identifiable information (e.g. email addresses).&lt;/p&gt;

&lt;p&gt;You can see the specifics of how this worked out in the case of mozregression in &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1581647#c9"&gt;bug 1581647&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="documentation"&gt;Documentation&lt;/h2&gt;

&lt;p&gt;Glean has some fantastic utilities for generating markdown-based documentation on what information is being collected, which I have made available on GitHub:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://github.com/mozilla/mozregression/blob/master/docs/glean/metrics.md"&gt;https://github.com/mozilla/mozregression/blob/master/docs/glean/metrics.md&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The generation of this documentation is &lt;a href="https://github.com/mozilla/mozregression/blob/3454e1eafe83f53a84cb6b10f46649320d5ed097/.travis.yml#L57"&gt;hooked up to mozregression&amp;rsquo;s continuous integration&lt;/a&gt;, so we can sure it&amp;rsquo;s up to date.&lt;/p&gt;

&lt;p&gt;I also added &lt;a href="https://mozilla.github.io/mozregression/documentation/telemetry.html"&gt;a quick note&lt;/a&gt; to mozregression&amp;rsquo;s web site describing the feature, along with (very importantly) instructions on how to turn it off.&lt;/p&gt;

&lt;h2 id="enabling-data-ingestion"&gt;Enabling Data Ingestion&lt;/h2&gt;

&lt;p&gt;Once a Glean-based project has passed data review, getting our infrastructure to ingest it is pretty straightforward. Normally &lt;a href="https://mozilla.github.io/glean/book/user/adding-glean-to-your-project.html#adding-metadata-about-your-project-to-the-pipeline"&gt;we would suggest just filing a bug&lt;/a&gt; and let us (the data team) handle the details, but since I&amp;rsquo;m &lt;em&gt;on&lt;/em&gt; that team, I&amp;rsquo;m going to go a (little bit) of detail into how the sausage is made.&lt;/p&gt;

&lt;p&gt;Behind the scenes, we have a collection of ETL (extract-transform-load) scripts in the &lt;a href="https://github.com/mozilla/probe-scraper/"&gt;probe-scraper repository&lt;/a&gt; which are responsible for parsing the ping and probe metadata files that I added to mozregression in the step above and then automatically creating BigQuery tables and updating our ingestion machinery to insert data passed to us there.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s quite a bit of complicated machinery being the scenes to make this all work, but since it&amp;rsquo;s already in place, adding a new thing like this is relatively simple. The changeset I submitted as part of a &lt;a href="https://github.com/mozilla/probe-scraper/pull/184"&gt;pull request&lt;/a&gt; to probe-scraper was all of 9 lines long:&lt;/p&gt;

&lt;div class="brush: diff"&gt;
 &lt;div class="colorful"&gt;
  &lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="gh"&gt;diff --git a/repositories.yaml b/repositories.yaml&lt;/span&gt;
&lt;span class="gh"&gt;index dffcccf..6212e55 100644&lt;/span&gt;
&lt;span class="gd"&gt;--- a/repositories.yaml&lt;/span&gt;
&lt;span class="gi"&gt;+++ b/repositories.yaml&lt;/span&gt;
&lt;span class="gu"&gt;@@ -239,3 +239,12 @@ firefox-android-release:&lt;/span&gt;
     - org.mozilla.components:browser-engine-gecko-beta
     - org.mozilla.appservices:logins
     - org.mozilla.components:support-migration
&lt;span class="gi"&gt;+mozregression:&lt;/span&gt;
&lt;span class="gi"&gt;+  app_id: org-mozilla-mozregression&lt;/span&gt;
&lt;span class="gi"&gt;+  notification_emails:&lt;/span&gt;
&lt;span class="gi"&gt;+    - wlachance@mozilla.com&lt;/span&gt;
&lt;span class="gi"&gt;+  url: &amp;#39;https://github.com/mozilla/mozregression&amp;#39;&lt;/span&gt;
&lt;span class="gi"&gt;+  metrics_files:&lt;/span&gt;
&lt;span class="gi"&gt;+    - &amp;#39;mozregression/metrics.yaml&amp;#39;&lt;/span&gt;
&lt;span class="gi"&gt;+  ping_files:&lt;/span&gt;
&lt;span class="gi"&gt;+    - &amp;#39;mozregression/pings.yaml&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;h2 id="a-pretty-graph"&gt;A Pretty Graph&lt;/h2&gt;

&lt;p&gt;With the probe scraper change merged and deployed, we can now start querying! A number of tables are automatically created according to the schema outlined above: notably &amp;ldquo;live&amp;rdquo; and &amp;ldquo;stable&amp;rdquo; tables corresponding to the usage ping. Using &lt;a href="https://docs.telemetry.mozilla.org/tools/stmo.html"&gt;sql.telemetry.mozilla.org&lt;/a&gt; we can start exploring what&amp;rsquo;s out there. Here&amp;rsquo;s a quick query I wrote up:&lt;/p&gt;

&lt;div class="brush: sql"&gt;
 &lt;div class="colorful"&gt;
  &lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="nb"&gt;DATE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;submission_timestamp&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="nb"&gt;date&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
       &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;usage_variant&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;variant&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
       &lt;span class="k"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="n"&gt;moz&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;fx&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;data&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;shared&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;prod&lt;/span&gt;&lt;span class="o"&gt;`&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;org_mozilla_mozregression_stable&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;usage_v1&lt;/span&gt;
&lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="nb"&gt;DATE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;submission_timestamp&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;2020-04-14&amp;#39;&lt;/span&gt;
  &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="n"&gt;client_info&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;app_display_version&lt;/span&gt; &lt;span class="k"&gt;NOT&lt;/span&gt; &lt;span class="k"&gt;LIKE&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;%.dev%&amp;#39;&lt;/span&gt;
&lt;span class="k"&gt;GROUP&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt; &lt;span class="nb"&gt;date&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;variant&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;&amp;hellip; which generates a chart like this:&lt;/p&gt;

&lt;div class="figure"&gt;&lt;img src="/files/2020/05/mozregression-variant-usage.png" alt="" /&gt;
 &lt;p class="caption"&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;This chart represents the absolute volume of mozregression usage since April 14th 2020 (around the time when we first released a version of mozregression with Glean telemetry), grouped by mozregression &amp;ldquo;variant&amp;rdquo; (GUI, console, and mach) and date - you can see that (unsurprisingly?) the GUI has the highest usage. I&amp;rsquo;ll talk about this more in an upcoming installment, speaking of&amp;hellip;&lt;/p&gt;

&lt;h2 id="next-steps"&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;re not done yet! Next time, we&amp;rsquo;ll look into making a public-facing dashboard demonstrating these results and making an aggregated version of the mozregression telemetry data publicly accessible to researchers and the general public. If we&amp;rsquo;re lucky, there might even be a bit of &lt;em&gt;data science&lt;/em&gt;. Stay tuned!&lt;/p&gt;</description></item>
  <item>
   <title>mozregression for MacOS</title>
   <link>https://wlach.github.io/blog/2020/04/mozregression-for-macos/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2020-04-mozregression-for-macos</guid>
   <pubDate>Fri, 24 Apr 2020 14:59:04 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;Just a quick note that, as a side-effect of the work I mentioned a while ago to &lt;a href="/blog/2020/02/this-week-in-glean-special-guest-post-mozregression-telemetry-part-1/"&gt;add telemetry to mozregression&lt;/a&gt;, mozregression now has a graphical Mac client! It&amp;rsquo;s a bit of a pain to install (since it&amp;rsquo;s unsigned), but likely worlds easier for the average person to get going than the command-line version. Please feel free to &lt;a href="https://mozilla.github.io/mozregression/install.html"&gt;point people to it&lt;/a&gt; if you&amp;rsquo;re looking to get a regression range for a MacOS-specific problem with Firefox.&lt;/p&gt;

&lt;center&gt;&lt;img style="width:600px" src="/files/2020/04/mozregression-gui-mac.png" /&gt;&lt;/center&gt;

&lt;p&gt;More details: The &lt;a href="https://mozilla.github.io/glean/book/dev/python/index.html"&gt;Glean Python SDK&lt;/a&gt;, which &lt;a href="https://mozilla.github.io/mozregression/documentation/telemetry.html"&gt;mozregression now uses for telemetry&lt;/a&gt;, requires Python 3. This provided the impetus to port the GUI itself to Python 3 and PySide2 (the modern incarnation of PyQt), which brought with it a much easier installation/development experience for the GUI on platforms like Mac and Linux.&lt;/p&gt;

&lt;p&gt;I haven&amp;rsquo;t gotten around to producing GUI binaries for the Linux yet, but it should not be much work.&lt;/p&gt;

&lt;p&gt;Speaking of Glean, mozregression, and Telemetry, stay tuned for more updates on that soon. It&amp;rsquo;s been an adventure!&lt;/p&gt;</description></item>
  <item>
   <title>This week in Glean (special guest post): mozregression telemetry (part 1)</title>
   <link>https://wlach.github.io/blog/2020/02/this-week-in-glean-special-guest-post-mozregression-telemetry-part-1/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2020-02-this-week-in-glean-special-guest-post-mozregression-telemetry-part-1</guid>
   <pubDate>Fri, 28 Feb 2020 15:50:58 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;&lt;em&gt;(“This Week in Glean” is a series of blog posts that the Glean Team at Mozilla is using to try to communicate better about our work. They could be release notes, documentation, hopes, dreams, or whatever: so long as it is inspired by Glean. You can find &lt;a href="https://mozilla.github.io/glean/book/appendix/twig.html"&gt;an index of all TWiG posts online.&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This is a special guest post by non-Glean-team member William Lachance!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As I &lt;a href="/blog/2019/09/mozregression-update-python-3-edition/"&gt;mentioned last time&lt;/a&gt; I talked about &lt;a href="https://mozilla.github.io/mozregression/"&gt;mozregression&lt;/a&gt;, I have been thinking about adding some telemetry to the system to better understand the usage of this tool, to justify some part of Mozilla spending some cycles maintaining and improving it (assuming my intuition that this tool is heavily used is confirmed).&lt;/p&gt;

&lt;p&gt;Coincidentally, the Telemetry client team has been working on a new library for measuring these types of things in a principled way called &lt;a href="https://mozilla.github.io/glean/book/index.html"&gt;Glean&lt;/a&gt;, which even has python bindings! Using this has the potential in saving a lot of work: not only does Glean provide a framework for submitting data, our backend systems are automatically set up to process data submitted via into Glean into &lt;a href="https://cloud.google.com/bigquery"&gt;BigQuery&lt;/a&gt; tables, which can then easily be queried using tools like &lt;a href="https://docs.telemetry.mozilla.org/tools/stmo.html"&gt;sql.telemetry.mozilla.org&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I thought it might be useful to go through some of what I&amp;rsquo;ve been exploring, in case others at Mozilla are interested in instrumenting their pet internal tools or projects. If this effort is successful, I&amp;rsquo;ll distill these notes into a tutorial in the Glean documentation.&lt;/p&gt;

&lt;h2 id="initial-steps-defining-pings-and-metrics"&gt;Initial steps: defining pings and metrics&lt;/h2&gt;

&lt;p&gt;The initial step in setting up a Glean project of any type is to define explicitly the types of pings and metrics. You can look at a &amp;ldquo;ping&amp;rdquo; as being a small bucket of data submitted by a piece of software in the field. A &amp;ldquo;metric&amp;rdquo; is something we&amp;rsquo;re measuring and including in a ping.&lt;/p&gt;

&lt;p&gt;Most of the Glean documentation focuses on browser-based use-cases where we might want to sample lots of different things on an ongoing basis, but for mozregression our needs are considerably simpler: we just want to know when someone &lt;em&gt;has&lt;/em&gt; used it along with a small number of non-personally identifiable characteristics of their usage, e.g. the mozregression version number and the name of the application they are bisecting.&lt;/p&gt;

&lt;p&gt;Glean has &lt;a href="https://mozilla.github.io/glean/book/user/pings/events.html"&gt;the concept of event pings&lt;/a&gt;, but it seems like those are there more for a fine-grained view of what&amp;rsquo;s going on during an application&amp;rsquo;s use. So let&amp;rsquo;s define a new ping just for ourselves, giving it the unimaginative name &amp;ldquo;usage&amp;rdquo;. This goes in a file called &lt;code&gt;pings.yaml&lt;/code&gt;:&lt;/p&gt;

&lt;div class="brush: yaml"&gt;
 &lt;div class="colorful"&gt;
  &lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nn"&gt;---&lt;/span&gt;
&lt;span class="nt"&gt;$schema&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;moz://mozilla.org/schemas/glean/pings/1-0-0&lt;/span&gt;

&lt;span class="nt"&gt;usage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="nt"&gt;description&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p p-Indicator"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="no"&gt;A ping to record usage of mozregression&lt;/span&gt;
  &lt;span class="nt"&gt;include_client_id&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;true&lt;/span&gt;
  &lt;span class="nt"&gt;notification_emails&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;wlachance@mozilla.com&lt;/span&gt;
  &lt;span class="nt"&gt;bugs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;http://bugzilla.mozilla.org/123456789/&lt;/span&gt;
  &lt;span class="nt"&gt;data_reviews&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;http://example.com/path/to/data-review&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;We also need to define a list of things we want to measure. To start with, let&amp;rsquo;s just test with one piece of sample information: the app we&amp;rsquo;re bisecting (e.g. &amp;ldquo;Firefox&amp;rdquo; or &amp;ldquo;Gecko View Example&amp;rdquo;). This goes in a file called &lt;code&gt;metrics.yaml&lt;/code&gt;:&lt;/p&gt;

&lt;div class="brush: yaml"&gt;
 &lt;div class="colorful"&gt;
  &lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nn"&gt;---&lt;/span&gt;
&lt;span class="nt"&gt;$schema&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;moz://mozilla.org/schemas/glean/metrics/1-0-0&lt;/span&gt;

&lt;span class="nt"&gt;usage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="nt"&gt;app&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nt"&gt;type&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;string&lt;/span&gt;
    &lt;span class="nt"&gt;description&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p p-Indicator"&gt;&amp;gt;&lt;/span&gt;
      &lt;span class="no"&gt;The name of the app being bisected&lt;/span&gt;
    &lt;span class="nt"&gt;notification_emails&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
      &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;wlachance@mozilla.com&lt;/span&gt;
    &lt;span class="nt"&gt;bugs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
      &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;https://bugzilla.mozilla.org/show_bug.cgi?id=1581647&lt;/span&gt;
    &lt;span class="nt"&gt;data_reviews&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
      &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;http://example.com/path/to/data-review&lt;/span&gt;
    &lt;span class="nt"&gt;expires&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;never&lt;/span&gt;
    &lt;span class="nt"&gt;send_in_pings&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;usage&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;data_reviews&lt;/code&gt; sections in both of the above are obviously bogus, we will need to actually get data review before landing and using this code, to make sure that we&amp;rsquo;re in conformance with Mozilla&amp;rsquo;s &lt;a href="https://wiki.mozilla.org/Firefox/Data_Collection"&gt;data collection policies&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="testing-it-out"&gt;Testing it out&lt;/h2&gt;

&lt;p&gt;But in the mean time, we can test our setup with the &lt;a href="https://docs.telemetry.mozilla.org/concepts/glean/debug_ping_view.html"&gt;Glean debug pings viewer&lt;/a&gt; by setting a special tag (&lt;code&gt;mozregression-test-tag&lt;/code&gt;) on our output. Here&amp;rsquo;s a small python script which does just that:&lt;/p&gt;

&lt;div class="brush: py"&gt;
 &lt;div class="colorful"&gt;
  &lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pathlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;glean&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Glean&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Configuration&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;glean&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;load_metrics&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                   &lt;span class="n"&gt;load_pings&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;mozregression_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;home&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;.mozilla2&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;mozregression&amp;#39;&lt;/span&gt;

&lt;span class="n"&gt;Glean&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initialize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;application_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"mozregression"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;application_version&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"0.1.1"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;upload_enabled&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;configuration&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Configuration&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
      &lt;span class="n"&gt;ping_tag&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"mozregression-test-tag"&lt;/span&gt;
    &lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;data_dir&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mozregression_path&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;"data"&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Glean&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_upload_enabled&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;pings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_pings&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"pings.yaml"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;metrics&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"metrics.yaml"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;usage&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"reality"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pings&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;usage&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;submit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;Running this script on my laptop, I see that a respectable JSON payload was delivered to and processed by our servers:&lt;/p&gt;

&lt;p&gt;&lt;img style="width:600px" src="/files/2020/02/glean-debug-ping-viewer.png" /&gt;&lt;/p&gt;

&lt;p&gt;As you can see, we&amp;rsquo;re successfully processing both the &amp;ldquo;version&amp;rdquo; number of mozregression, some characteristics of the machine sending the information (my MacBook in this case), as well as our single measure. We also have a client id, which should tell us roughly how many distinct installations of mozregression are sending pings. This should be more than sufficient for an initial &amp;ldquo;mozregression usage dashboard&amp;rdquo;.&lt;/p&gt;

&lt;h2 id="next-steps"&gt;Next steps&lt;/h2&gt;

&lt;p&gt;There are a bunch of things I still need to work through before landing this inside mozregression itself. Notably, the Glean python bindings are python3-only, so we&amp;rsquo;ll need to &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1426766"&gt;port the mozregression GUI to python 3&lt;/a&gt; before we can start measuring usage there. But I&amp;rsquo;m excited at how quickly this work is coming together: stay tuned for part 2 in a few weeks.&lt;/p&gt;</description></item>
  <item>
   <title>Conda is pretty great</title>
   <link>https://wlach.github.io/blog/2020/01/conda-is-pretty-great/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2020-01-conda-is-pretty-great</guid>
   <pubDate>Mon, 13 Jan 2020 16:08:57 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;Lately the data engineering team has been looking into productionizing (i.e. running in Airflow) a bunch of models that the data science team has been producing. This often involves languages and environments that are a bit outside of our comfort zone &amp;mdash; for example, &lt;a href="https://github.com/mozilla/missioncontrol-v2"&gt;the next version of Mission Control&lt;/a&gt; relies on the &lt;a href="https://mc-stan.org/users/interfaces/rstan"&gt;R-stan library&lt;/a&gt; to produce a model of expected crash behaviour as Firefox is released.&lt;/p&gt;

&lt;p&gt;To make things as simple and deterministic as possible, we&amp;rsquo;ve been building up Docker containers to run/execute this code along with their dependencies, which makes things nice and reproducible. My initial thought was to use just the language-native toolchains to build up my container for the above project, but quickly found a number of problems:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;For local testing, Docker on Mac is &lt;em&gt;slow&lt;/em&gt;: when doing a large number of statistical calculations (as above), you can count on your testing iterations taking 3 to 4 (or more) times longer.&lt;/li&gt;
 &lt;li&gt;On initial setup, the default R packaging strategy is to have the user of a package like R-stan recompile from source. This can take &lt;em&gt;forever&lt;/em&gt; if you have a long list of dependencies with C-compiled extensions (pretty much a given if you&amp;rsquo;re working in the data space): rebuilding my initial docker environment for missioncontrol-v2 took almost an hour. This isn&amp;rsquo;t just a problem for local development: it also makes continuous integration using a service like Circle or Travis expensive and painful.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;I had been vaguely aware of &lt;a href="https://docs.conda.io/en/latest/"&gt;Conda&lt;/a&gt; for a few years, but didn&amp;rsquo;t really understand its value proposition until I started working on the above project: why bother with a heavyweight package manager when you already have Docker to virtualize things? The answer is that it solves both of the above problems: for local development, you can get something more-or-less identical to what you&amp;rsquo;re running inside Docker with no performance penalty whatsoever. And for building the docker container itself, Conda&amp;rsquo;s package repository contains pre-compiled versions of all the dependencies you&amp;rsquo;d want to use for something like this (even somewhat esoteric libraries like R-stan are available on &lt;a href="https://conda-forge.org/"&gt;conda-forge&lt;/a&gt;), which brought my build cycle times down to less than 5 minutes.&lt;/p&gt;

&lt;p&gt;tl;dr: If you have a bunch of R / python code you want to run in a reproducible manner, consider Conda.&lt;/p&gt;</description></item>
  <item>
   <title>Using BigQuery JavaScript UDFs to analyze Firefox telemetry for fun &amp; profit</title>
   <link>https://wlach.github.io/blog/2019/10/using-bigquery-javascript-udfs-to-analyze-firefox-telemetry-for-fun-profit/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2019-10-using-bigquery-javascript-udfs-to-analyze-firefox-telemetry-for-fun-profit</guid>
   <pubDate>Wed, 30 Oct 2019 15:11:17 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;For the last year, we&amp;rsquo;ve been gradually migrating our backend Telemetry systems from AWS to GCP. I&amp;rsquo;ve been helping out here and there with this effort, most recently porting a job we used to detect slow tab spinners in Firefox nightly, which produced a small dataset that feeds a &lt;a href="https://mikeconley.github.io/bug1310250/"&gt;small adhoc dashboard&lt;/a&gt; which Mike Conley maintains. This was a relatively small task as things go, but it highlighted some features and improvements which I think might be broadly interesting, so I decided to write up a small blog post about it.&lt;/p&gt;

&lt;p&gt;Essentially all this dashboard tells you is what percentage of the Firefox nightly population saw a tab spinner over the past 6 months. And of those that did see a tab spinner, what was the severity? Essentially we’re just trying to make sure that there are no major regressions of user experience (and also that efforts to improve things bore fruit):&lt;/p&gt;

&lt;center&gt;&lt;img style="width:600px" srcset="/files/2019/10/tab-spinner-dash.png" /&gt;&lt;/center&gt;

&lt;p&gt;Pretty simple stuff, but getting the data necessary to produce this kind of dashboard used to be anything but trivial: while some common business/product questions could be answered by a quick query to &lt;a href="https://docs.telemetry.mozilla.org/datasets/batch_view/clients_daily/reference.html"&gt;clients_daily&lt;/a&gt;, getting engineering-specific metrics like this usually involved trawling through gigabytes of raw heka encoded blobs using an Apache Spark cluster and then extracting the relevant information out of the telemetry probe histograms (in this case, &lt;code&gt;FX_TAB_SWITCH_SPINNER_VISIBLE_MS&lt;/code&gt; and &lt;code&gt;FX_TAB_SWITCH_SPINNER_VISIBLE_LONG_MS&lt;/code&gt;) contained therein.&lt;/p&gt;

&lt;p&gt;The code itself was rather complicated (&lt;a href="https://github.com/mozilla/python_mozetl/blob/58dce245ce8012b338e8b102a8c2c0f00601be60/mozetl/tab_spinner/tab_spinner.py"&gt;take a look, if you dare&lt;/a&gt;) but even worse, running it could get &lt;em&gt;very expensive&lt;/em&gt;. We had a 14 node cluster churning through this script daily, and it took on average about an hour and a half to run! I don&amp;rsquo;t have the exact cost figures on hand (and am not sure if I&amp;rsquo;d be authorized to share them if I did), but based on a back of the envelope sketch, this one single script was probably costing us somewhere on the order of $10-$40 a day (that works out to between $3650-$14600 a year).&lt;/p&gt;

&lt;p&gt;With our move to &lt;a href="https://cloud.google.com/bigquery/"&gt;BigQuery&lt;/a&gt;, things get a lot simpler! Thanks to the combined effort of my team and data operations[1], we now produce &amp;ldquo;stable&amp;rdquo; ping tables on a daily basis with &lt;em&gt;all&lt;/em&gt; the relevant histogram data (stored as JSON blobs), queryable using relatively vanilla SQL. In this case, the data we care about is in &lt;code&gt;telemetry.main&lt;/code&gt; (named after the main ping, appropriately enough). With the help of a small &lt;a href="https://cloud.google.com/bigquery/docs/reference/standard-sql/user-defined-functions"&gt;JavaScript UDF&lt;/a&gt; function, all of this data can easily be extracted into a table inside a single SQL query scheduled by &lt;a href="https://docs.telemetry.mozilla.org/tools/stmo.html"&gt;sql.telemetry.mozilla.org&lt;/a&gt;.&lt;/p&gt;

&lt;div class="brush: sql"&gt;
 &lt;div class="colorful"&gt;
  &lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;CREATE&lt;/span&gt; &lt;span class="n"&gt;TEMP&lt;/span&gt; &lt;span class="k"&gt;FUNCTION&lt;/span&gt;
  &lt;span class="n"&gt;udf_js_json_extract_highest_long_spinner&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;input&lt;/span&gt; &lt;span class="n"&gt;STRING&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;RETURNS&lt;/span&gt; &lt;span class="n"&gt;INT64&lt;/span&gt;
  &lt;span class="k"&gt;LANGUAGE&lt;/span&gt; &lt;span class="n"&gt;js&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="ss"&gt;"""&lt;/span&gt;
&lt;span class="ss"&gt;    if (input == null) {&lt;/span&gt;
&lt;span class="ss"&gt;      return 0;&lt;/span&gt;
&lt;span class="ss"&gt;    }&lt;/span&gt;
&lt;span class="ss"&gt;    var result = JSON.parse(input);&lt;/span&gt;
&lt;span class="ss"&gt;    var valuesMap = result.values;&lt;/span&gt;
&lt;span class="ss"&gt;    var highest = 0;&lt;/span&gt;
&lt;span class="ss"&gt;    for (var key in valuesMap) {&lt;/span&gt;
&lt;span class="ss"&gt;      var range = parseInt(key);&lt;/span&gt;
&lt;span class="ss"&gt;      if (valuesMap[key]) {&lt;/span&gt;
&lt;span class="ss"&gt;        highest = range &amp;gt; 0 ? range : 1;&lt;/span&gt;
&lt;span class="ss"&gt;      }&lt;/span&gt;
&lt;span class="ss"&gt;    }&lt;/span&gt;
&lt;span class="ss"&gt;    return highest;&lt;/span&gt;
&lt;span class="ss"&gt;"""&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="n"&gt;build_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="k"&gt;sum&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="k"&gt;when&lt;/span&gt; &lt;span class="n"&gt;highest&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;64000&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;v_64000ms_or_higher&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="k"&gt;sum&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="k"&gt;when&lt;/span&gt; &lt;span class="n"&gt;highest&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;27856&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;highest&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;64000&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;v_27856ms_to_63999ms&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="k"&gt;sum&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="k"&gt;when&lt;/span&gt; &lt;span class="n"&gt;highest&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;12124&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;highest&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;27856&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;v_12124ms_to_27855ms&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="k"&gt;sum&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="k"&gt;when&lt;/span&gt; &lt;span class="n"&gt;highest&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;5277&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;highest&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;12124&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;v_5277ms_to_12123ms&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="k"&gt;sum&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="k"&gt;when&lt;/span&gt; &lt;span class="n"&gt;highest&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;2297&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;highest&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;5277&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;v_2297ms_to_5276ms&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="k"&gt;sum&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="k"&gt;when&lt;/span&gt; &lt;span class="n"&gt;highest&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;highest&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;2297&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;v_1000ms_to_2296ms&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="k"&gt;sum&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="k"&gt;when&lt;/span&gt; &lt;span class="n"&gt;highest&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;highest&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;v_0ms_to_49ms&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="k"&gt;sum&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="k"&gt;when&lt;/span&gt; &lt;span class="n"&gt;highest&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;highest&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;v_50ms_to_99ms&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="k"&gt;sum&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="k"&gt;when&lt;/span&gt; &lt;span class="n"&gt;highest&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;highest&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;v_100ms_to_199ms&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="k"&gt;sum&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="k"&gt;when&lt;/span&gt; &lt;span class="n"&gt;highest&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;highest&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;400&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;v_200ms_to_399ms&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="k"&gt;sum&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="k"&gt;when&lt;/span&gt; &lt;span class="n"&gt;highest&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;400&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;highest&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;800&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;v_400ms_to_799ms&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="k"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="k"&gt;count&lt;/span&gt;
&lt;span class="k"&gt;from&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;select&lt;/span&gt; &lt;span class="n"&gt;build_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;client_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;greatest&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;highest_long&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;highest_short&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;highest&lt;/span&gt;
&lt;span class="k"&gt;from&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt;
    &lt;span class="n"&gt;SUBSTR&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;application&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;build_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;client_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;udf_js_json_extract_highest_long_spinner&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;payload&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;histograms&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FX_TAB_SWITCH_SPINNER_VISIBLE_LONG_MS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;highest_long&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;udf_js_json_extract_highest_long_spinner&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;payload&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;histograms&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FX_TAB_SWITCH_SPINNER_VISIBLE_MS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;highest_short&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;telemetry&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;main&lt;/span&gt;
&lt;span class="k"&gt;WHERE&lt;/span&gt;
    &lt;span class="n"&gt;application&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;channel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;nightly&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="n"&gt;normalized_os&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Windows&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="n"&gt;application&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build_id&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;FORMAT_DATE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;"%Y%m%d"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;DATE_SUB&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;CURRENT_DATE&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="nb"&gt;INTERVAL&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="n"&gt;QUARTER&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="nb"&gt;DATE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;submission_timestamp&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;DATE_SUB&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;CURRENT_DATE&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="nb"&gt;INTERVAL&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="n"&gt;QUARTER&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;group&lt;/span&gt; &lt;span class="k"&gt;by&lt;/span&gt; &lt;span class="n"&gt;build_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;client_id&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;group&lt;/span&gt; &lt;span class="k"&gt;by&lt;/span&gt; &lt;span class="n"&gt;build_id&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;In addition to being much simpler, this new job is also &lt;em&gt;way&lt;/em&gt; cheaper. The last run of it scanned just over 1 TB of data, meaning it cost us just over $5. Not as cheap as I might like, but considerably less expensive than before: I&amp;rsquo;ve also scheduled it to only run once every other day, since Mike tells me he doesn&amp;rsquo;t need this data any more often than that.&lt;/p&gt;

&lt;p&gt;[1] I understand that Jeff Klukas, Frank Bertsch, Daniel Thorn, Anthony Miyaguchi, and Wesley Dawson are the principals involved - apologies if I&amp;rsquo;m forgetting someone.&lt;/p&gt;</description></item>
  <item>
   <title>Metrics Graphics: Stepping back for a while</title>
   <link>https://wlach.github.io/blog/2019/09/metrics-graphics-stepping-back-for-a-while/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2019-09-metrics-graphics-stepping-back-for-a-while</guid>
   <pubDate>Thu, 26 Sep 2019 20:33:54 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;Just a note that I&amp;rsquo;ve decided to step back from &lt;a href="https://metricsgraphicsjs.org"&gt;metrics graphics&lt;/a&gt; maintenance for the time being, which means that the project is essentially unowned. This has sort of been the case for a while, but I figured I should probably make it official.&lt;/p&gt;

&lt;p&gt;If you follow the link to the &lt;a href="https://github.com/metricsgraphics/metrics-graphics/"&gt;metrics graphics repository&lt;/a&gt;, you&amp;rsquo;ll note that the version has been bumped to &amp;ldquo;3.0-alpha3&amp;rdquo;. I was &lt;em&gt;this close&lt;/em&gt; to making one last new release this afternoon but decided I didn&amp;rsquo;t want to potentially break existing users who were fine using the last &amp;ldquo;official&amp;rdquo; version (v3.0 bumps the version of d3 used to &amp;ldquo;5&amp;rdquo;, among other breaking changes). I&amp;rsquo;d encourage people who want to continue using the library to make a fork and publish a copy under their user or organization name on npm.&lt;/p&gt;</description></item>
  <item>
   <title>mozregression update: python 3 edition</title>
   <link>https://wlach.github.io/blog/2019/09/mozregression-update-python-3-edition/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2019-09-mozregression-update-python-3-edition</guid>
   <pubDate>Mon, 16 Sep 2019 15:29:04 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;For those who are still wondering, yup, I am still maintaining &lt;a href="https://mozilla.github.io/mozregression/"&gt;mozregression&lt;/a&gt;, though increasingly reluctantly. Given how important this project is to the development of Firefox (getting a regression window using mozregression is standard operating procedure whenever a new bug is reported in Firefox), it feels like this project is pretty vital, so I continue out of some sense of obligation &amp;mdash; but really, someone more interested in Mozilla&amp;rsquo;a build, automation and testing systems would be better suited to this task: over the past few years, my interests/focus have shifted away from this area to building up Mozilla&amp;rsquo;s data storage and visualization platform.&lt;/p&gt;

&lt;p&gt;This post will describe some of the things that have happened in the last year and where I see the project going. My hope is to attract some new blood to add some needed features to the project and maybe take on some of the maintainership duties.&lt;/p&gt;

&lt;h2 id="python-3"&gt;python 3&lt;/h2&gt;

&lt;p&gt;The most important update is that, as of today, the command-line version of mozregression (v3.0.1) should work with python 3.5+. &lt;a href="https://python-modernize.readthedocs.io/en/latest/"&gt;modernize&lt;/a&gt; did most of the work for us, though there were some unit tests that needed updating: special thanks to &lt;a href="https://github.com/gloomy-ghost"&gt;@gloomy-ghost&lt;/a&gt; for helping with that.&lt;/p&gt;

&lt;p&gt;For now, we will continue to support python 2.7 in parallel, mainly because the GUI has not yet been ported to python 3 (more on that later) and we have CI to make sure it doesn&amp;rsquo;t break.&lt;/p&gt;

&lt;h2 id="other-updates"&gt;other updates&lt;/h2&gt;

&lt;p&gt;The last year has mostly been one of maintenance. Thanks in particular to Ian Moody (:kwan) for his work throughout the year &amp;mdash; including patches to adapt mozregression support to our new updates policy and shippable builds (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1532412"&gt;bug 1532412&lt;/a&gt;), and Kartikaya Gupta (:kats) for adding support for bisecting the GeckoView example app (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1507225"&gt;bug 1507225&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id="future-work"&gt;future work&lt;/h2&gt;

&lt;p&gt;There are a bunch of things I see us wanting to add or change with mozregression over the next year or so. I might get to some of these if I have some spare cycles, but probably best not to count on it:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Port the mozregression GUI to Python 3 (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1581633"&gt;bug  1581633&lt;/a&gt;) As mentioned  above, the command-line client works with python 3, but we have yet to port  the &lt;a href=""&gt;GUI&lt;/a&gt;. We should do that. This probably also entails porting the GUI to  use PyQT5 (which is pip-installable and thus much easier to integrate into a  CI process), see &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1426766"&gt;bug 1426766&lt;/a&gt;.&lt;/li&gt;
 &lt;li&gt;Make self-contained GUI builds available for MacOS X (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1425105"&gt;bug  1425105&lt;/a&gt;) and Linux  (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1581643"&gt;bug 1581643&lt;/a&gt;).&lt;/li&gt;
 &lt;li&gt;Improve our mechanism for producing a standalone version of the GUI in  general. We&amp;rsquo;ve used &lt;a href="https://github.com/anthony-tuininga/cx_Freeze"&gt;cx_Freeze&lt;/a&gt;  which mostly works ok, but has a number of problems (e.g. it pulls in a bunch of unnecessary dependencies, which  bloats the size of the installer). Upgrading the GUI to use python 3 may  alleviate some of these issues, but it might be worth considering other  options in this space, like Gregory Szorc&amp;rsquo;s &lt;a href="https://github.com/indygreg/PyOxidizer"&gt;pyoxidizer&lt;/a&gt;.&lt;/li&gt;
 &lt;li&gt;Add some kind of telemetry to mozregression to measure usage of this tool  (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1581647"&gt;bug 1581647&lt;/a&gt;).  My anecdotal experience is that this tool is pretty invaluable for Firefox  development and QA, but this is not immediately apparent to Mozilla&amp;rsquo;s  leadership and it&amp;rsquo;s thus very difficult to convince people to spend their  cycles on maintaining and improving this tool. Field data may help change  that story.&lt;/li&gt;
 &lt;li&gt;Supporting new Mozilla products which aren&amp;rsquo;t built (entirely) out of mozilla-central,  most especially Fenix (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1556042"&gt;bug 1556042&lt;/a&gt;)  and Firefox Reality (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1568488"&gt;bug 1568488&lt;/a&gt;).  This is probably rather involved (mozregression has a big pile of assumptions about how  the builds it pulls down are stored and organized) but that doesn&amp;rsquo;t mean that  this work isn&amp;rsquo;t necessary.&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;If you&amp;rsquo;re interested in working on any of the above, please feel free to dive in on one of the above bugs. I can&amp;rsquo;t offer formal mentorship, but am happy to help out where I can.&lt;/p&gt;</description></item>
  <item>
   <title>Time for some project updates</title>
   <link>https://wlach.github.io/blog/2019/09/time-for-some-project-updates/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2019-09-time-for-some-project-updates</guid>
   <pubDate>Mon, 16 Sep 2019 14:41:39 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;I&amp;rsquo;ve been a bit bad about updating this blog over the past year or so, though this hasn&amp;rsquo;t meant there haven&amp;rsquo;t been things to talk about. For the next couple weeks, I&amp;rsquo;m going to try to give some updates on the projects I have been spending time on in the past year, both old and new. I&amp;rsquo;m going to begin with some of the less-loved things I&amp;rsquo;ve been working on, partially in an attempt to motivate some forward-motion on things that I believe are rather important to Mozilla.&lt;/p&gt;

&lt;p&gt;More to come.&lt;/p&gt;</description></item>
  <item>
   <title>Goodbye, noble stead</title>
   <link>https://wlach.github.io/blog/2019/05/goodbye-noble-stead/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2019-05-goodbye-noble-stead</guid>
   <pubDate>Wed, 22 May 2019 00:52:21 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;With lots of sadness, I just gave away my vintage 1970s velo-sport. This was my first bike I owned as a mid-twentysomething adult (I temporarily gave up biking and took up smoking when I was 19, two of the worst decisions of my life). Picked it up at the suggestion of my now ex-wife after frustration with the Halifax bus system. Used it to &lt;a href="https://www.thecoast.ca/halifax/beta-the-public-transit-day-tripper/Content?oid=1098826"&gt;hack said transit system&lt;/a&gt; amidst riding hundreds and hundreds of kilometres on the Halifax penisula and beyond. A few years later, it came with me to Montreal, where we rode even more - near-daily commutes from NDG to Mile End and rides for pie to Mont St Hilaire. Finally, it was fated to come with me to Toronto, where it served to take me on long rides on the waterfront trail, east to Pickering, west as far as Niagara Falls, as well as serving as a daily commuter from Leslieville to my office in the Fashion District, not to mention early-morning jaunts across town for 6am sittings at the Toronto Zen Centre.&lt;/p&gt;

&lt;p&gt;&lt;img style="width:400px" srcset="/files/2019/05/wills_bike_1.jpg 2x" /&gt;&lt;/p&gt;

&lt;p&gt;Over the years, I&amp;rsquo;ve probably sunk over two thousand dollars into repairs (as well as doing a fair number of work on it myself). An issue with the front wheel puncturing three tubes in succession finally convinced me that this was a losing battle, unless I wanted to put down a large sum into an overhaul. If I used it less I could probably justify some more minor repairs, but I think most of it (with the exception of the frame and headset) is on its last legs. Given the amount that I bicycle, it just seemed to make sense to get a new one, and see what a modern commuter has to offer. So I decided to cut my losses and buy a brand new one at Urbane Cycling.&lt;/p&gt;

&lt;p&gt;It was a hard decision to replace it, and harder still to give it away. I &lt;em&gt;loved&lt;/em&gt; this old bike, far more than any inanimate thing that has been in my presence. I am grateful for the good care it took of me (I have not had a single major accident riding it), and the adventures it enabled me to have. I took this picture of it in the donation rack at Bikesauce, which is almost certainly the last I&amp;rsquo;ll see of it:&lt;/p&gt;

&lt;p&gt;&lt;img style="width:400px" srcset="/files/2019/05/wills_bike_2.jpg 2x" /&gt;&lt;/p&gt;

&lt;p&gt;I left a note with some pointers on what needs work and asking whoever fixes it up to give it a lot of love, but honestly this bike&amp;rsquo;s fate is out of my control at this point. So it is with all things.&lt;/p&gt;</description></item>
  <item>
   <title>New ideas, old buildings</title>
   <link>https://wlach.github.io/blog/2019/03/new-ideas-old-buildings/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2019-03-new-ideas-old-buildings</guid>
   <pubDate>Fri, 22 Mar 2019 19:08:11 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;Last week, Brendan Colloran &lt;a href="https://hacks.mozilla.org/2019/03/iodide-an-experimental-tool-for-scientific-communicatiodide-for-scientific-communication-exploration-on-the-web/"&gt;announced Iodide&lt;/a&gt;, a new take on scientific collaboration and reporting that I&amp;rsquo;ve been really happy to contribute to over the past year-and-a-bit. I&amp;rsquo;ve been describing it to people I meet as kind of "&lt;a href="https://glitch.com/"&gt;glitch&lt;/a&gt; meets &lt;a href="https://jupyter.org/"&gt;jupyter&lt;/a&gt; " but that doesn&amp;rsquo;t quite do it justice. I&amp;rsquo;d recommend reading Brendan&amp;rsquo;s blog post (and taking a look at our &lt;a href="https://alpha.iodide.io"&gt;demonstration site&lt;/a&gt;) to get the full picture.&lt;/p&gt;

&lt;p&gt;One question that I&amp;rsquo;ve heard asked (including on Brendan&amp;rsquo;s post) is why we chose a rather conventional and old technology (&lt;a href="https://www.djangoproject.com/"&gt;Django&lt;/a&gt;) for the server backend. Certainly, Iodide has not been shy about building with relatively new or experimental technologies for other parts (e.g. Python on WebAssembly for the notebooks, React/Redux for the frontend). Why not complete the cycle by using a new-fangled JavaScript web server like, I don&amp;rsquo;t know, &lt;a href="https://nestjs.com/"&gt;NestJS&lt;/a&gt;? And while we&amp;rsquo;re at it, what&amp;rsquo;s with iodide&amp;rsquo;s ridiculous &lt;a href="https://en.wikipedia.org/wiki/Representational_state_transfer"&gt;REST API&lt;/a&gt;? Don&amp;rsquo;t you know &lt;a href="https://graphql.org/"&gt;GraphQL&lt;/a&gt; is the only legitimate way to expose your backend to the world in 2019?&lt;/p&gt;

&lt;p&gt;The great urban theorist of the twentieth century, &lt;a href="https://en.wikipedia.org/wiki/Jane_Jacobs"&gt;Jane Jacobs&lt;/a&gt; has a quote I love:&lt;/p&gt;

&lt;p&gt;
 &lt;i&gt;“Old ideas can sometimes use new buildings. New ideas must use old buildings.”&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;Laura Thompson (an engineering director at Mozilla) has restated this wisdom in a software development context as &lt;a href="https://speakerdeck.com/lauraxt/build-exciting-things-with-boring-technologies"&gt;&amp;ldquo;Build exciting things with boring technologies&amp;rdquo;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It so happened that the server was not an area Iodide was focusing on for innovation (at least initially), so it made much, much more sense to use something proven and battle-tested for the server side deployment. I&amp;rsquo;d used Django for a number of projects at Mozilla before this one (&lt;a href="https://github.com/mozilla/treeherder"&gt;Treeherder/Perfherder&lt;/a&gt; and &lt;a href="https://github.com/mozilla/missioncontrol/"&gt;Mission Control&lt;/a&gt;) and have been wildly impressed by the project&amp;rsquo;s excellent &lt;a href="https://docs.djangoproject.com/"&gt;documentation&lt;/a&gt;, &lt;a href="https://docs.djangoproject.com/en/2.1/topics/db/"&gt;database access layer&lt;/a&gt;, and support for building a standardized API via the &lt;a href="https://www.django-rest-framework.org/"&gt;Django REST Framework&lt;/a&gt; add-on. Not to mention the fact that so much of Mozilla&amp;rsquo;s in-house ops and web development expertise is based around this framework (I could name off probably 5 or 6 internal business systems based around the Django stack, in addition to Treeherder), so deploying Iodide and getting help building it would be something of a known quantity.&lt;/p&gt;

&lt;p&gt;Only slightly more than half a year since I began work on the iodide server, we now have both a publicly accessible site for others to experiment with &lt;em&gt;and&lt;/em&gt; an internal one for Mozilla&amp;rsquo;s business needs. It&amp;rsquo;s hard to say what would have happened had I chosen something more experimental to build Iodide&amp;rsquo;s server piece, but at the very least there would have been a substantial learning curve involved &amp;mdash; in addition to engineering effort to fill in the gaps where the new technology is not yet complete &amp;mdash; which would have meant less time to innovate where it really mattered. Django&amp;rsquo;s &lt;a href="https://docs.djangoproject.com/en/2.1/topics/migrations/"&gt;database migration system&lt;/a&gt;, for example, took years to come to fruition and I&amp;rsquo;m not aware of anything comparable in the world of JavaScript web frameworks.&lt;/p&gt;

&lt;p&gt;As we move ahead, we may find places where applying new backend server technologies makes sense. Heck, maybe we&amp;rsquo;ll chose to rewrite the whole thing at some point. But to get to launch, chosing a bunch of boring, tested software for this portion of Iodide was (in my view) absolutely the right decision and I make no apologies for it.&lt;/p&gt;</description></item>
  <item>
   <title>Making contribution work for Firefox tooling and data projects</title>
   <link>https://wlach.github.io/blog/2018/11/making-contribution-work-for-firefox-tooling-and-data-projects/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2018-11-making-contribution-work-for-firefox-tooling-and-data-projects</guid>
   <pubDate>Mon, 26 Nov 2018 16:13:46 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;One of my favorite parts about Mozilla is mentoring and working alongside third party contributors. Somewhat surprisingly since I work on internal tools, I&amp;rsquo;ve had a fair amount of luck finding people to help work on projects within my purview: &lt;a href="https://mozilla.github.io/mozregression"&gt;mozregression&lt;/a&gt;, &lt;a href="https://wiki.mozilla.org/EngineeringProductivity/Projects/Perfherder"&gt;perfherder&lt;/a&gt;, &lt;a href="https://metricsgraphics.org"&gt;metrics graphics&lt;/a&gt;, and others have all benefited from the contributions of people outside of Mozilla.&lt;/p&gt;

&lt;p&gt;In most cases (a notable exception being metrics graphics), these have been internal-tooling projects used by others to debug, develop, or otherwise understand the behaviour of Firefox. On the face of it, none of the things I work on are exactly &amp;ldquo;high profile cutting edge stuff&amp;rdquo; in the way, say, Firefox or the Rust Programming Language are. So why do they bother? The exact formula varies depending on contributor, but I think it usually comes down to some combination of these two things:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;A desire to learn and demonstrate competence with industry standard  tooling (the python programming language, frontend web development, backend  databases, &amp;ldquo;big data&amp;rdquo; technologies like Parquet, &amp;hellip;).&lt;/li&gt;
 &lt;li&gt;A desire to work with and gain recognition inside of a community of  like-minded people.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;Pretty basic, obvious stuff &amp;mdash; there is an appeal here to basic human desires like the need for security and a sense of belonging. Once someone&amp;rsquo;s &amp;ldquo;in the loop&amp;rdquo;, so to speak, generally things take care of themselves. The real challenge, I&amp;rsquo;ve found, is getting people from the &amp;ldquo;I am potentially interested in doing something with Mozilla internal tools&amp;rdquo; to the stage that they are confident and competent enough to work in a reasonably self-directed way. When I was on the A-Team, we classified this transition in terms of a &lt;a href="https://ateam-bootcamp.readthedocs.io/en/latest/guide/curve.html"&gt;commitment curve&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img srcset="/files/2018/11/commitment-curve-visualization.png 2x" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1080119"&gt;prototype commitment curve graphic by Steven Brown&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The hardest part, in my experience, is the initial part of that curve. At this point, people are just dipping their toe in the water. Some may not have a ton of experience with software development yet. In other cases, my projects may just not be the right fit for them. But of course, sometimes there is a fit, or at least one could be developed! What I&amp;rsquo;ve found most helpful is &amp;ldquo;clearing a viable path&amp;rdquo; forward for the right kind of contributor. That is, some kind of initial hypothesis of what a successful contribution experience would look like as a new person transitions from &amp;ldquo;explorer&amp;rdquo; stage in the chart above to &amp;ldquo;associate&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t exactly have a perfect template for what &amp;ldquo;clearing a path&amp;rdquo; looks like in every case. It depends quite a bit on the nature of the contributor. But there are some common themes that I&amp;rsquo;ve found effective:&lt;/p&gt;

&lt;p&gt;First, provide good, concise documentation both on the project&amp;rsquo;s purpose and vision and how to get started easily and keep it up to date. For projects with a front-end web component, I try to decouple the front end parts from the backend services so that people can &lt;code&gt;yarn install &amp;amp;&amp;amp; yarn start&lt;/code&gt; &lt;a href="/blog/2014/09/hacking-on-the-treeherder-front-end-refreshingly-easy/"&gt;their way to success&lt;/a&gt;. Being able to &lt;em&gt;see&lt;/em&gt; the project in action quickly (and not getting stuck on some mundane getting started step) is key in maintaining initial interest.&lt;/p&gt;

&lt;p&gt;Second, provide a set of good starter issues (sometimes called &amp;ldquo;good first bugs&amp;rdquo;) for people to work on. Generally these would be non-critical-path type issues that have straightforward instructions to resolve and fix. Again, the idea here is to give people a sense of quick progress and resolution, a &amp;ldquo;yes I can actually do this&amp;rdquo; sort of feeling. But be careful not to let a contributor get stuck here! These bugs take a disproportionate amount of effort to file and mentor compared to their actual value &amp;mdash; the key is to progress the contributor to the next level once it&amp;rsquo;s clear they can handle the basics involved in solving such an issue (checking out the source code, applying a fix, submitting a patch, etc). Otherwise you&amp;rsquo;re going to feel frustrated and wonder why you&amp;rsquo;re on an endless treadmill of writing up trivial bugs.&lt;/p&gt;

&lt;p&gt;Third, once a contributor has established themselves by fixing a few of these simple issues, I try to get to know them a little better. Send them an email, learn where they&amp;rsquo;re from, invite them to chat on the project channel if they can. At the same time, this is an opportunity to craft a somewhat larger piece of work (a sort of mini-project) that they can do, tailored to the interests. For example, a new contributor on the Mission Control has recently been working on adding &lt;a href="https://jestjs.io/"&gt;Jest&lt;/a&gt; tests to the project &amp;mdash; I provided some basic guidance of things to look at, but did not dictate exactly &lt;em&gt;how&lt;/em&gt; to perform the task. They figured that out for themselves.&lt;/p&gt;

&lt;p&gt;As time goes by, you just continue this process. Depending on the contributor, they may start coming up with their own ideas for how a project might be improved or they might still want to follow your lead (or that of the team), but at the least I generally see an improvement in their self-directedness and confidence after a period of sustained contribution. In either case, the key to success remains the same: sustained and positive communication and sharing of goals and aspirations, making sure that both parties are getting something positive out of the experience. Where possible, I try to include contributors in team meetings. Where there&amp;rsquo;s an especially close working relationship (e.g. &lt;a href="https://summerofcode.withgoogle.com/archive/"&gt;Google Summer of Code&lt;/a&gt;). I try to set up a weekly one on one. Regardless, I make reviewing code, answering questions, and providing suggestions on how to move forward a top priority (i.e. not something I&amp;rsquo;ll leave for a few days). It&amp;rsquo;s the least I can do if someone is willing to take time out to contribute to my project.&lt;/p&gt;

&lt;p&gt;If this seems similar to the best practices for how members of a team should onboard each other and work together, that&amp;rsquo;s not really a coincidence. Obviously the relationship is a little different because we&amp;rsquo;re not operating with a formal managerial structure and usually the work is unpaid: I try to bear that mind and make double sure that contributors are really getting some useful skills and habits that they can take with them to future jobs and other opportunities, while also emphasizing that their code contributions are their own, not Mozilla&amp;rsquo;s. So far it seems to have worked out pretty well for all concerned (me, Mozilla, and the contributors).&lt;/p&gt;</description></item>
  <item>
   <title>pydata nyc 2018</title>
   <link>https://wlach.github.io/blog/2018/10/pydata-nyc-2018/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2018-10-pydata-nyc-2018</guid>
   <pubDate>Mon, 29 Oct 2018 13:19:23 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;Went to &lt;a href="https://pydata.org/nyc2018/"&gt;PyData NYC&lt;/a&gt; a couple weeks ago, and figured I ought to write up my thoughts for the benefits of the others on my extended team. Why not publish as a blog post while I&amp;rsquo;m at it?&lt;/p&gt;

&lt;p&gt;This is actually the first conference I&amp;rsquo;d been to in my capacity as a &amp;ldquo;data engineer&amp;rdquo; at Mozilla, a team I joined about a year and a half ago after specializing in the same area on the (now-defunct) &lt;a href="https://wiki.mozilla.org/EngineeringProductivity"&gt;a-team&lt;/a&gt;. I&amp;rsquo;ve felt a special affinity for the Python community, particularly its data science offshoots (pandas, numpy, and jupyter notebooks) so it was great to finally go to a conference that specializes in these topics.&lt;/p&gt;

&lt;p&gt;Overall, the conference was a bit of a mix between people talking about the status of their projects, theoretical talks on specific statistical approaches to data, general talks on how people are doing &amp;ldquo;data science&amp;rdquo; (I would say the largest majority of attendees at the conference were users of python data science tools, rather than developers), and case studies of how people are using python data science tools in their research or work. This being New York, many (probably the majority) were using data science tools in fields like quantitative finance, sales, marketing, and health care.&lt;/p&gt;

&lt;p&gt;As a side note, it was really satisfying to be able to tell Mozilla&amp;rsquo;s story about how we collect and use data without violating the privacy of our users. This is becoming more and more of an issue (especailly in Europe with the GPDR) and it really makes me happy that we have a really positive story to tell, not a bunch of dirty secrets that we need to hide.&lt;/p&gt;

&lt;p&gt;In general I found the last two types of talks the most rewarding to go to: most of the work I do at Mozilla currently involves larger-scale data where, I&amp;rsquo;m sad to say, Python is usually not (currently) an applicable tool, at least not by itself (though maybe &lt;a href="https://iodide.io"&gt;iodide&lt;/a&gt; will help change that! see below). And I don&amp;rsquo;t usually find a 60 minute talk really enough time for me to be able to properly absorb new mathematical or statistical concepts, though I can sometimes get little tidbits of information from them that come in handy later.&lt;/p&gt;

&lt;p&gt;Some talks that made an impression on me:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="https://pydata.org/nyc2018/schedule/presentation/83/"&gt;Open source and quantitative finance&lt;/a&gt;: Keynote  talk, was a great introduction to the paranoia of the world of quantitative finance.  I think the main message was that things are gradually moving to a (slightly less)  paranoid model where generally-useful modifications done to numerical/ml software as  part of a trading platform may now be upstreamed&amp;hellip; but my main takeaway is that I&amp;rsquo;m  really glad I&amp;rsquo;m not working in that industry.&lt;/li&gt;
 &lt;li&gt;&lt;a href="https://pydata.org/nyc2018/schedule/presentation/9/"&gt;Words in Space&lt;/a&gt;: Introduced  an interesting-soundingl library called &lt;a href="http://www.scikit-yb.org/en/latest/"&gt;Yellow Brick&lt;/a&gt; for visualizing the results  of machine learning models.&lt;/li&gt;
 &lt;li&gt;&lt;a href="https://pydata.org/nyc2018/schedule/presentation/2/"&gt;Creating a data-driven product culture&lt;/a&gt;:  General talk on how to create a positive and useful data science culture at a company. I think  Mozilla already checks most of the boxes outlined in the talk.&lt;/li&gt;
 &lt;li&gt;&lt;a href="https://pydata.org/nyc2018/schedule/presentation/39/"&gt;What Data Scientists Really Do&lt;/a&gt;: Quite entertaining talk on the future of &amp;ldquo;data science&amp;rdquo;,  by Hugo Bowne-Anderson (who also has a &lt;a href="https://www.datacamp.com/community/podcast"&gt;podcast&lt;/a&gt; which sounds cool). The most  interesting takeaway from the talk was the speculation that within 10 years the term  &amp;ldquo;data scientist&amp;rdquo; might have the same meaning as the word &amp;ldquo;webmaster&amp;rdquo; now. It&amp;rsquo;s a  hyper-generalist job description which will almost inevitably be split into a number  of other more specialized roles.&lt;/li&gt;
 &lt;li&gt;&lt;a href="https://pydata.org/nyc2018/schedule/presentation/77/"&gt;Master Class: Bayesian Statistics&lt;/a&gt;: This falls under the &amp;ldquo;technical talk which I couldn&amp;rsquo;t  grasp in 60 minutes&amp;rdquo; category, but I think I finally do understand a little bit more  of what people mean when they say &amp;ldquo;Bayesian Statistics&amp;rdquo; now. It actually doesn&amp;rsquo;t have much  to do with &lt;a href="https://en.wikipedia.org/wiki/Bayes%27_theorem"&gt;Baye&amp;rsquo;s Theorem&lt;/a&gt;, rather it seems to be more of a philosophical approach to  data analysis which acknowledges the limitations of human capacity to understand the  world and asks us to more explicitly state our assumptions when developing models (probably  over-simplifying here). I think I can get behind that &amp;mdash; want to learn more. They  provided a &lt;a href="https://betanalpha.github.io/workshops/pydata/"&gt;bunch of material&lt;/a&gt; to work through,  which I&amp;rsquo;ve been meaning to take a look at.&lt;/li&gt;
 &lt;li&gt;&lt;a href="https://pydata.org/nyc2018/schedule/presentation/30/"&gt;Data Science in Health Care: Beyond the Hype&lt;/a&gt;:  Great presentations in how data science can be used to improve health care outcomes. Lots of relevant  insights that I think are also applicable to &amp;ldquo;product health&amp;rdquo; here at Mozilla. I  particularly liked the way the presenter framed requirements when deciding whether or not  to do a type of analysis: &amp;ldquo;if i knew [information], i would do [intervention], which would  have [measurable outcome]&amp;rdquo;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Of course, this post wouldn&amp;rsquo;t be complete without a mention of &lt;a href="http://droettboom.com/"&gt;Mike Droettboom&lt;/a&gt;&amp;rsquo;s &lt;a href="https://pydata.org/nyc2018/schedule/presentation/3/"&gt;talk&lt;/a&gt; on &lt;a href="https://iodide.io"&gt;iodide&lt;/a&gt;, a project I&amp;rsquo;ve been spending some considerable cycles helping with over the last couple of quarters. I need to write some longer thoughts on iodide at some point in the near future, but in a nutshell it&amp;rsquo;s a scientific notebook environment where the computational kernel lives entirely inside the browser. It was well received and we had a great followup session afterwards with people interested in using it for various things. Being able to show a python environment in the browser which &amp;ldquo;just works&amp;rdquo;, with no installation or other steps makes a &lt;em&gt;great&lt;/em&gt; tech demo. I&amp;rsquo;m really excited about the public launch of our server-based environment, which will hopefully be coming in the next couple of months.&lt;/p&gt;</description></item>
  <item>
   <title>Mission Control 1.0</title>
   <link>https://wlach.github.io/blog/2018/06/mission-control-1-0/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2018-06-mission-control-1-0</guid>
   <pubDate>Tue, 05 Jun 2018 21:50:32 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;Just a quick announcement that the first &amp;ldquo;production-ready&amp;rdquo; version of Mission Control just went live yesterday, at this easy-to-remember URL:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://missioncontrol.telemetry.mozilla.org"&gt;https://missioncontrol.telemetry.mozilla.org&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For those not yet familiar with the project, Mission Control aims to track release stability and quality across Firefox releases. It is similar in spirit to arewestableyet and other crash dashboards, with the following new and exciting properties:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Uses the full set of crash counts gathered via telemetry, rather than the arbitrary  sample that users decide to submit to crash-stats&lt;/li&gt;
 &lt;li&gt;Results are available within minutes of ingestion by telemetry (although be warned  &lt;a href="/blog/2017/10/better-or-worse-by-what-measure/"&gt;initial results for a release always look bad&lt;/a&gt;)&lt;/li&gt;
 &lt;li&gt;The denominator in our crash rate is usage hours, rather than the probably-incorrect  calculation of active-daily-installs used by  &lt;a href="https://arewestableyet.com"&gt;arewestableyet&lt;/a&gt; (not a knock on the people who  wrote that tool, there was nothing better available at the time)&lt;/li&gt;
 &lt;li&gt;We have a detailed breakdown of the results by platform (rather than letting Windows  results dominate the overall rates due to its high volume of usage)&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;In general, my hope is that this tool will provide a more scientific and accurate idea of release stability and quality over time. There&amp;rsquo;s lots more to do, but I think this is a promising start. Much gratitude to &lt;a href="https://home.kairo.at/blog/2014-04/how_effective_is_the_stability_program"&gt;kairo&lt;/a&gt;, calixte, &lt;a href="https://chuttenblog.wordpress.com/"&gt;chutten&lt;/a&gt; and others who helped build my understanding of this area.&lt;/p&gt;

&lt;p&gt;The dashboard itself an easier thing to show than talk about, so I recorded a quick demonstration of some of the dashboard&amp;rsquo;s capabilities and published it on air mozilla:&lt;/p&gt;

&lt;iframe src="https://air.mozilla.org/mission-control-dashboard-intro/video/" width="896" height="524" frameborder="0" allowfullscreen="allowfullscreen"&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;a href="https://air.mozilla.org/mission-control-dashboard-intro/"&gt;link&lt;/a&gt;&lt;/p&gt;</description></item>
  <item>
   <title>Some thoughts on opinion polling in the Ontario 2018 election</title>
   <link>https://wlach.github.io/blog/2018/05/some-thoughts-on-opinion-polling-in-the-ontario-2018-election/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2018-05-some-thoughts-on-opinion-polling-in-the-ontario-2018-election</guid>
   <pubDate>Sun, 27 May 2018 16:57:30 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;Like many, I&amp;rsquo;ve been a bit worried about the Ontario election, and have been rather obsessively checking a site called the &lt;a href="https://newsinteractives.cbc.ca/onvotes/poll-tracker/"&gt;Ontario Poll Tracker&lt;/a&gt;.&lt;/p&gt;

&lt;center&gt;&lt;img style="width:400px" srcset="/files/2018/05/CBC-poll-tracker-main.png 2x" /&gt;&lt;/center&gt;

&lt;p&gt;It has nice and shiny graphs and uses authoritative language and purports to provide a scientific analysis which predicts the election. Despite this, it&amp;rsquo;s my assertion that this kind of predictive modelling is nothing more than snake oil. I keep on reminding myself that I shouldn&amp;rsquo;t take it too seriously, but haven&amp;rsquo;t been too successful so far. This blog post is a reminder to myself on why I should stop reloading that site so much, but maybe it will be helpful to others as well. As a warning, it&amp;rsquo;s not going to say anything particularly novel. If you have any kind of background in statistics at all, this is probably going to be quite boring.&lt;/p&gt;

&lt;p&gt;First, a story. Way back when I had just graduated from university in 2003, I worked briefly at an &amp;ldquo;opinion research company&amp;rdquo;, telephoning people for various opinion surveys. It was easily the worst job I ever had, horrible for both the people doing the calling and those who were being called.&lt;/p&gt;

&lt;p&gt;The work was mind-numbingly repetitive. Get assigned a poll. Telephone people using an autodialer, work through the script using the DOS-based software the call center was using where they would answer multiple-choice questions. Repeat as many times as you can over the course of an hour. The topics were varied, but roughly 50/50 political parties doing private polling and businesses trying to get marketing data. In either case, the questions were definitely of the &amp;ldquo;lowest common denominator&amp;rdquo; type question (i.e. &amp;ldquo;Which products are you likely to buy in the next 12 months&amp;rdquo;, &amp;ldquo;If an election were held today, would you vote for party A, B, or C?&amp;rdquo;)&lt;/p&gt;

&lt;p&gt;One of the few benefits of tedious jobs is that they give you time to think about things. In this case, one of my distinct experiential take aways as that the results that we were getting were incredibly unrepresentative.&lt;/p&gt;

&lt;p&gt;For a poll to be valid it is supposed to be &amp;ldquo;reasonably&amp;rdquo; reflective of the general population. Over the quantities that we&amp;rsquo;re talking about, that means anywhere from hundreds of thousands to millions of people. If we were able to truly randomly sample a small number from this group, the results are likely to be &amp;ldquo;representative of the whole&amp;rdquo; (within some confidence interval). Let&amp;rsquo;s write up a small python script to confirm this intuition:&lt;/p&gt;

&lt;div class="brush: py"&gt;
 &lt;div class="colorful"&gt;
  &lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 100,000 random numbers between 0 and 1&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;full_population_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100000&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;full_sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;full_population_size&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="c1"&gt;# average over entire result&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;full_sample&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;full_population_size&lt;/span&gt;
&lt;span class="mf"&gt;0.501036568906331&lt;/span&gt;

&lt;span class="c1"&gt;# pull out 100 randomly selected values from the full sample and&lt;/span&gt;
&lt;span class="c1"&gt;# get their average&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;random_subset_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;random_subset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;full_sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;100000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt;
                     &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random_subset_size&lt;/span&gt;&lt;span class="p"&gt;)]]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random_subset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;random_subset_size&lt;/span&gt;
&lt;span class="mf"&gt;0.4924555517866068&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;Only a small fraction of the total population, but a result within 1% of the true value. Expressing it this way makes random sampling almost like a tautology. You probably learned this in high school. Great right?&lt;/p&gt;

&lt;p&gt;Unfortunately, real life always comes in to disturb these assumptions, as it always does. You see, there were all sorts of factors that would affect who we would be talking to and thus get datapoints from. At that time, most of the population still had a land-line telephone but there were a wealth of other factors that meant that we weren&amp;rsquo;t getting a truly randoms sample of data. Men (at least men under 60 or so) were much less likely to answer a telephone survey than women. For general opinion surveys, we were calling at a specific time of day when &lt;em&gt;most&lt;/em&gt; people were likely to be available &amp;mdash; but that certainly wouldn&amp;rsquo;t apply to everyone. Some people would work night shifts, etc., etc. In our example above, this would be like taking out half the results over (say) 0.75 from our sample &amp;mdash; the end result would tend to skew much lower than the true value.&lt;/p&gt;

&lt;p&gt;Just for fun, let&amp;rsquo;s try doing that and see how it affects the results:&lt;/p&gt;

&lt;div class="brush: py"&gt;
 &lt;div class="colorful"&gt;
  &lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# if we take away approximately half the results with a value of&lt;/span&gt;
&lt;span class="c1"&gt;# &amp;gt;0.75, the population we are sampling from is reduced proportionally&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;full_sample_with_half75_removed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;full_sample&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mf"&gt;0.75&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;full_sample_with_half75_removed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;87607&lt;/span&gt;

&lt;span class="c1"&gt;# the sampled value is then proportionally skewed downwards (because&lt;/span&gt;
&lt;span class="c1"&gt;# a large percentage of the high values are no longer available)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;random_subset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;full_sample_with_half75_removed&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt;
                     &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;full_sample_with_half75_removed&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt;
                      &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random_subset_size&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
                    &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random_subset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;random_subset_size&lt;/span&gt;
&lt;span class="mf"&gt;0.4585241853943395&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;To try and get around this problem, the opinion polling company would try to demographically restrict who we were surveying past a certain point, so that the overall sample of the poll would reasonably reflect the characteristics of the population. This probably helped, but there&amp;rsquo;s only so much you can do here. For example, if you correct for the fact that men aged 20 to 60 are less likely to answer an opinion survey, your sample is going to now consist of those weird men who &lt;em&gt;do&lt;/em&gt; answer opinion surveys. Who knows what effect that&amp;rsquo;s going to have on your results?&lt;/p&gt;

&lt;p&gt;I want to be clear here: this is a methodological problem. Running more opinion polls doesn&amp;rsquo;t help. Probably some samples will be more affected by errors than others, but the problem remains regardless. Actually, let&amp;rsquo;s show this trivially for our small example:&lt;/p&gt;

&lt;div class="brush: py"&gt;
 &lt;div class="colorful"&gt;
  &lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;skewed_averages&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;   &lt;span class="n"&gt;full_sample_with_half75_removed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;full_sample&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mf"&gt;0.75&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt;
                                         &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;   &lt;span class="n"&gt;random_subset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;full_sample_with_half75_removed&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt;
          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;full_sample_with_half75_removed&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt;
           &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random_subset_size&lt;/span&gt;&lt;span class="p"&gt;)]]&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;   &lt;span class="n"&gt;skewed_averages&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random_subset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random_subset&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;skewed_averages&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.4585241853943395&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.4271412530288919&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.46414511969024697&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.4360740890986547&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="mf"&gt;0.4779021127791633&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.38419133106708714&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.48688298744651576&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.41076028280889915&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="mf"&gt;0.47975630795860363&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.4381467970818846&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;Each time we resampled from the main population and got a different result, but the end result was still one which was &lt;em&gt;far&lt;/em&gt; off from what we know in this case was the true value (0.5). Sampling from bad data doesn&amp;rsquo;t make up for these problems, it just gives you more bad results.&lt;/p&gt;

&lt;p&gt;Now, flash forward to 2018. Almost no one under 50 has a land-line anymore, people who have cell phones most often don&amp;rsquo;t answer to unknown callers. And don&amp;rsquo;t even get me started on how to find a representative set of people to participate in an &amp;ldquo;online panel&amp;rdquo;. What validity do polls have under these circumstances? I would say very little and probably more importantly we don&amp;rsquo;t even have a clear idea of &lt;em&gt;how&lt;/em&gt; our modern polls are skewed.&lt;/p&gt;

&lt;p&gt;There has been no shortage of thinking on how to correct for these problems but in my opinion it&amp;rsquo;s all just speculative and largely invalid. You can&amp;rsquo;t definitively solve the kind of uncertainty we&amp;rsquo;re talking about here by coming up with &amp;ldquo;just so&amp;rdquo; stories about how you&amp;rsquo;ve corrected for it. We might have some ideas about how our data is biased, but short of sampling the entire population and then seeing how our sampling method falls into that superset (which is impossible) there is no way of confirming that our efforts to correct for that bias were effective.&lt;/p&gt;

&lt;p&gt;With respect to the Ontario election which I alluded to above, the one thing that I am getting from the data is that support for the NDP (across the highly unrepresentative sample used in the polls) is increasing precipitously and that for the PC&amp;rsquo;s is decreasing almost as sharply. That seems to be a real phenomenon. We don&amp;rsquo;t know whether that crosses over to the general population but it doesn&amp;rsquo;t seem unreasonable to think it does. Exactly how is another question, and I make no assertions there.&lt;/p&gt;

&lt;center&gt;&lt;img style="width:400px" srcset="/files/2018/05/CBC-poll-tracker-trend.png 2x" /&gt;&lt;/center&gt;

&lt;p&gt;tl;dr If you don&amp;rsquo;t like the idea of &lt;a href="https://www.theguardian.com/world/2018/apr/30/doug-ford-ontario-conservative-trump-comparison-canada"&gt;Doug Ford&lt;/a&gt; in power, there is no reason to panic based on sites like the Ontario Poll Tracker. Spend your time doing something more productive, like convincing your friends and relatives to vote for someone who is not Conservative.&lt;/p&gt;</description></item></channel></rss>