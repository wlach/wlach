<?xml version="1.0" encoding="utf-8"?> 
<rss version="2.0">
 <channel>
  <title>William Lachance's Log: William Lachance's Log</title>
  <description>William Lachance's Log: William Lachance's Log</description>
  <link>https://wlach.github.io/index.html</link>
  <lastBuildDate>Fri, 24 Apr 2020 14:59:04 UT</lastBuildDate>
  <pubDate>Fri, 24 Apr 2020 14:59:04 UT</pubDate>
  <ttl>1800</ttl>
  <item>
   <title>mozregression for MacOS</title>
   <link>https://wlach.github.io/blog/2020/04/mozregression-for-macos/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2020-04-mozregression-for-macos</guid>
   <pubDate>Fri, 24 Apr 2020 14:59:04 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;Just a quick note that, as a side-effect of the work I mentioned a while ago to &lt;a href="/blog/2020/02/this-week-in-glean-special-guest-post-mozregression-telemetry-part-1/"&gt;add telemetry to mozregression&lt;/a&gt;, mozregression now has a graphical Mac client! It&amp;rsquo;s a bit of a pain to install (since it&amp;rsquo;s unsigned), but likely worlds easier for the average person to get going than the command-line version. Please feel free to &lt;a href="https://mozilla.github.io/mozregression/install.html"&gt;point people to it&lt;/a&gt; if you&amp;rsquo;re looking to get a regression range for a MacOS-specific problem with Firefox.&lt;/p&gt;

&lt;center&gt;&lt;img style="width:600px" src="/files/2020/04/mozregression-gui-mac.png" /&gt;&lt;/center&gt;

&lt;p&gt;More details: The &lt;a href="https://mozilla.github.io/glean/book/dev/python/index.html"&gt;Glean Python SDK&lt;/a&gt;, which &lt;a href="https://mozilla.github.io/mozregression/documentation/telemetry.html"&gt;mozregression now uses for telemetry&lt;/a&gt;, requires Python 3. This provided the impetus to port the GUI itself to Python 3 and PySide2 (the modern incarnation of PyQt), which brought with it a much easier installation/development experience for the GUI on platforms like Mac and Linux.&lt;/p&gt;

&lt;p&gt;I haven&amp;rsquo;t gotten around to producing GUI binaries for the Linux yet, but it should not be much work.&lt;/p&gt;

&lt;p&gt;Speaking of Glean, mozregression, and Telemetry, stay tuned for more updates on that soon. It&amp;rsquo;s been an adventure!&lt;/p&gt;</description></item>
  <item>
   <title>This week in Glean (special guest post): mozregression telemetry (part 1)</title>
   <link>https://wlach.github.io/blog/2020/02/this-week-in-glean-special-guest-post-mozregression-telemetry-part-1/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2020-02-this-week-in-glean-special-guest-post-mozregression-telemetry-part-1</guid>
   <pubDate>Fri, 28 Feb 2020 15:50:58 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;&lt;em&gt;(“This Week in Glean” is a series of blog posts that the Glean Team at Mozilla is using to try to communicate better about our work. They could be release notes, documentation, hopes, dreams, or whatever: so long as it is inspired by Glean. You can find &lt;a href="https://mozilla.github.io/glean/book/appendix/twig.html"&gt;an index of all TWiG posts online.&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This is a special guest post by non-Glean-team member William Lachance!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As I &lt;a href="/blog/2019/09/mozregression-update-python-3-edition/"&gt;mentioned last time&lt;/a&gt; I talked about &lt;a href="https://mozilla.github.io/mozregression/"&gt;mozregression&lt;/a&gt;, I have been thinking about adding some telemetry to the system to better understand the usage of this tool, to justify some part of Mozilla spending some cycles maintaining and improving it (assuming my intuition that this tool is heavily used is confirmed).&lt;/p&gt;

&lt;p&gt;Coincidentally, the Telemetry client team has been working on a new library for measuring these types of things in a principled way called &lt;a href="https://mozilla.github.io/glean/book/index.html"&gt;Glean&lt;/a&gt;, which even has python bindings! Using this has the potential in saving a lot of work: not only does Glean provide a framework for submitting data, our backend systems are automatically set up to process data submitted via into Glean into &lt;a href="https://cloud.google.com/bigquery"&gt;BigQuery&lt;/a&gt; tables, which can then easily be queried using tools like &lt;a href="https://docs.telemetry.mozilla.org/tools/stmo.html"&gt;sql.telemetry.mozilla.org&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I thought it might be useful to go through some of what I&amp;rsquo;ve been exploring, in case others at Mozilla are interested in instrumenting their pet internal tools or projects. If this effort is successful, I&amp;rsquo;ll distill these notes into a tutorial in the Glean documentation.&lt;/p&gt;

&lt;h2 id="initial-steps-defining-pings-and-metrics"&gt;Initial steps: defining pings and metrics&lt;/h2&gt;

&lt;p&gt;The initial step in setting up a Glean project of any type is to define explicitly the types of pings and metrics. You can look at a &amp;ldquo;ping&amp;rdquo; as being a small bucket of data submitted by a piece of software in the field. A &amp;ldquo;metric&amp;rdquo; is something we&amp;rsquo;re measuring and including in a ping.&lt;/p&gt;

&lt;p&gt;Most of the Glean documentation focuses on browser-based use-cases where we might want to sample lots of different things on an ongoing basis, but for mozregression our needs are considerably simpler: we just want to know when someone &lt;em&gt;has&lt;/em&gt; used it along with a small number of non-personally identifiable characteristics of their usage, e.g. the mozregression version number and the name of the application they are bisecting.&lt;/p&gt;

&lt;p&gt;Glean has &lt;a href="https://mozilla.github.io/glean/book/user/pings/events.html"&gt;the concept of event pings&lt;/a&gt;, but it seems like those are there more for a fine-grained view of what&amp;rsquo;s going on during an application&amp;rsquo;s use. So let&amp;rsquo;s define a new ping just for ourselves, giving it the unimaginative name &amp;ldquo;usage&amp;rdquo;. This goes in a file called &lt;code&gt;pings.yaml&lt;/code&gt;:&lt;/p&gt;

&lt;div class="brush: yaml"&gt;
 &lt;div class="colorful"&gt;
  &lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nn"&gt;---&lt;/span&gt;
&lt;span class="nt"&gt;$schema&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;moz://mozilla.org/schemas/glean/pings/1-0-0&lt;/span&gt;

&lt;span class="nt"&gt;usage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="nt"&gt;description&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p p-Indicator"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="no"&gt;A ping to record usage of mozregression&lt;/span&gt;
  &lt;span class="nt"&gt;include_client_id&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;true&lt;/span&gt;
  &lt;span class="nt"&gt;notification_emails&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;wlachance@mozilla.com&lt;/span&gt;
  &lt;span class="nt"&gt;bugs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;http://bugzilla.mozilla.org/123456789/&lt;/span&gt;
  &lt;span class="nt"&gt;data_reviews&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;http://example.com/path/to/data-review&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;We also need to define a list of things we want to measure. To start with, let&amp;rsquo;s just test with one piece of sample information: the app we&amp;rsquo;re bisecting (e.g. &amp;ldquo;Firefox&amp;rdquo; or &amp;ldquo;Gecko View Example&amp;rdquo;). This goes in a file called &lt;code&gt;metrics.yaml&lt;/code&gt;:&lt;/p&gt;

&lt;div class="brush: yaml"&gt;
 &lt;div class="colorful"&gt;
  &lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nn"&gt;---&lt;/span&gt;
&lt;span class="nt"&gt;$schema&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;moz://mozilla.org/schemas/glean/metrics/1-0-0&lt;/span&gt;

&lt;span class="nt"&gt;usage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="nt"&gt;app&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nt"&gt;type&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;string&lt;/span&gt;
    &lt;span class="nt"&gt;description&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p p-Indicator"&gt;&amp;gt;&lt;/span&gt;
      &lt;span class="no"&gt;The name of the app being bisected&lt;/span&gt;
    &lt;span class="nt"&gt;notification_emails&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
      &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;wlachance@mozilla.com&lt;/span&gt;
    &lt;span class="nt"&gt;bugs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
      &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;https://bugzilla.mozilla.org/show_bug.cgi?id=1581647&lt;/span&gt;
    &lt;span class="nt"&gt;data_reviews&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
      &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;http://example.com/path/to/data-review&lt;/span&gt;
    &lt;span class="nt"&gt;expires&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;never&lt;/span&gt;
    &lt;span class="nt"&gt;send_in_pings&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;usage&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;data_reviews&lt;/code&gt; sections in both of the above are obviously bogus, we will need to actually get data review before landing and using this code, to make sure that we&amp;rsquo;re in conformance with Mozilla&amp;rsquo;s &lt;a href="https://wiki.mozilla.org/Firefox/Data_Collection"&gt;data collection policies&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="testing-it-out"&gt;Testing it out&lt;/h2&gt;

&lt;p&gt;But in the mean time, we can test our setup with the &lt;a href="https://docs.telemetry.mozilla.org/concepts/glean/debug_ping_view.html"&gt;Glean debug pings viewer&lt;/a&gt; by setting a special tag (&lt;code&gt;mozregression-test-tag&lt;/code&gt;) on our output. Here&amp;rsquo;s a small python script which does just that:&lt;/p&gt;

&lt;div class="brush: py"&gt;
 &lt;div class="colorful"&gt;
  &lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pathlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;glean&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Glean&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Configuration&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;glean&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;load_metrics&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                   &lt;span class="n"&gt;load_pings&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;mozregression_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;home&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;.mozilla2&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;mozregression&amp;#39;&lt;/span&gt;

&lt;span class="n"&gt;Glean&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initialize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;application_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"mozregression"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;application_version&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"0.1.1"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;upload_enabled&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;configuration&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Configuration&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
      &lt;span class="n"&gt;ping_tag&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"mozregression-test-tag"&lt;/span&gt;
    &lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;data_dir&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mozregression_path&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;"data"&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Glean&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_upload_enabled&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;pings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_pings&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"pings.yaml"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;metrics&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_metrics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"metrics.yaml"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;usage&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"reality"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pings&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;usage&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;submit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;Running this script on my laptop, I see that a respectable JSON payload was delivered to and processed by our servers:&lt;/p&gt;

&lt;p&gt;&lt;img style="width:600px" src="/files/2020/02/glean-debug-ping-viewer.png" /&gt;&lt;/p&gt;

&lt;p&gt;As you can see, we&amp;rsquo;re successfully processing both the &amp;ldquo;version&amp;rdquo; number of mozregression, some characteristics of the machine sending the information (my MacBook in this case), as well as our single measure. We also have a client id, which should tell us roughly how many distinct installations of mozregression are sending pings. This should be more than sufficient for an initial &amp;ldquo;mozregression usage dashboard&amp;rdquo;.&lt;/p&gt;

&lt;h2 id="next-steps"&gt;Next steps&lt;/h2&gt;

&lt;p&gt;There are a bunch of things I still need to work through before landing this inside mozregression itself. Notably, the Glean python bindings are python3-only, so we&amp;rsquo;ll need to &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1426766"&gt;port the mozregression GUI to python 3&lt;/a&gt; before we can start measuring usage there. But I&amp;rsquo;m excited at how quickly this work is coming together: stay tuned for part 2 in a few weeks.&lt;/p&gt;</description></item>
  <item>
   <title>Conda is pretty great</title>
   <link>https://wlach.github.io/blog/2020/01/conda-is-pretty-great/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2020-01-conda-is-pretty-great</guid>
   <pubDate>Mon, 13 Jan 2020 16:08:57 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;Lately the data engineering team has been looking into productionizing (i.e. running in Airflow) a bunch of models that the data science team has been producing. This often involves languages and environments that are a bit outside of our comfort zone &amp;mdash; for example, &lt;a href="https://github.com/mozilla/missioncontrol-v2"&gt;the next version of Mission Control&lt;/a&gt; relies on the &lt;a href="https://mc-stan.org/users/interfaces/rstan"&gt;R-stan library&lt;/a&gt; to produce a model of expected crash behaviour as Firefox is released.&lt;/p&gt;

&lt;p&gt;To make things as simple and deterministic as possible, we&amp;rsquo;ve been building up Docker containers to run/execute this code along with their dependencies, which makes things nice and reproducible. My initial thought was to use just the language-native toolchains to build up my container for the above project, but quickly found a number of problems:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;For local testing, Docker on Mac is &lt;em&gt;slow&lt;/em&gt;: when doing a large number of statistical calculations (as above), you can count on your testing iterations taking 3 to 4 (or more) times longer.&lt;/li&gt;
 &lt;li&gt;On initial setup, the default R packaging strategy is to have the user of a package like R-stan recompile from source. This can take &lt;em&gt;forever&lt;/em&gt; if you have a long list of dependencies with C-compiled extensions (pretty much a given if you&amp;rsquo;re working in the data space): rebuilding my initial docker environment for missioncontrol-v2 took almost an hour. This isn&amp;rsquo;t just a problem for local development: it also makes continuous integration using a service like Circle or Travis expensive and painful.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;I had been vaguely aware of &lt;a href="https://docs.conda.io/en/latest/"&gt;Conda&lt;/a&gt; for a few years, but didn&amp;rsquo;t really understand its value proposition until I started working on the above project: why bother with a heavyweight package manager when you already have Docker to virtualize things? The answer is that it solves both of the above problems: for local development, you can get something more-or-less identical to what you&amp;rsquo;re running inside Docker with no performance penalty whatsoever. And for building the docker container itself, Conda&amp;rsquo;s package repository contains pre-compiled versions of all the dependencies you&amp;rsquo;d want to use for something like this (even somewhat esoteric libraries like R-stan are available on &lt;a href="https://conda-forge.org/"&gt;conda-forge&lt;/a&gt;), which brought my build cycle times down to less than 5 minutes.&lt;/p&gt;

&lt;p&gt;tl;dr: If you have a bunch of R / python code you want to run in a reproducible manner, consider Conda.&lt;/p&gt;</description></item>
  <item>
   <title>Using BigQuery JavaScript UDFs to analyze Firefox telemetry for fun &amp; profit</title>
   <link>https://wlach.github.io/blog/2019/10/using-bigquery-javascript-udfs-to-analyze-firefox-telemetry-for-fun-profit/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2019-10-using-bigquery-javascript-udfs-to-analyze-firefox-telemetry-for-fun-profit</guid>
   <pubDate>Wed, 30 Oct 2019 15:11:17 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;For the last year, we&amp;rsquo;ve been gradually migrating our backend Telemetry systems from AWS to GCP. I&amp;rsquo;ve been helping out here and there with this effort, most recently porting a job we used to detect slow tab spinners in Firefox nightly, which produced a small dataset that feeds a &lt;a href="https://mikeconley.github.io/bug1310250/"&gt;small adhoc dashboard&lt;/a&gt; which Mike Conley maintains. This was a relatively small task as things go, but it highlighted some features and improvements which I think might be broadly interesting, so I decided to write up a small blog post about it.&lt;/p&gt;

&lt;p&gt;Essentially all this dashboard tells you is what percentage of the Firefox nightly population saw a tab spinner over the past 6 months. And of those that did see a tab spinner, what was the severity? Essentially we’re just trying to make sure that there are no major regressions of user experience (and also that efforts to improve things bore fruit):&lt;/p&gt;

&lt;center&gt;&lt;img style="width:600px" srcset="/files/2019/10/tab-spinner-dash.png" /&gt;&lt;/center&gt;

&lt;p&gt;Pretty simple stuff, but getting the data necessary to produce this kind of dashboard used to be anything but trivial: while some common business/product questions could be answered by a quick query to &lt;a href="https://docs.telemetry.mozilla.org/datasets/batch_view/clients_daily/reference.html"&gt;clients_daily&lt;/a&gt;, getting engineering-specific metrics like this usually involved trawling through gigabytes of raw heka encoded blobs using an Apache Spark cluster and then extracting the relevant information out of the telemetry probe histograms (in this case, &lt;code&gt;FX_TAB_SWITCH_SPINNER_VISIBLE_MS&lt;/code&gt; and &lt;code&gt;FX_TAB_SWITCH_SPINNER_VISIBLE_LONG_MS&lt;/code&gt;) contained therein.&lt;/p&gt;

&lt;p&gt;The code itself was rather complicated (&lt;a href="https://github.com/mozilla/python_mozetl/blob/58dce245ce8012b338e8b102a8c2c0f00601be60/mozetl/tab_spinner/tab_spinner.py"&gt;take a look, if you dare&lt;/a&gt;) but even worse, running it could get &lt;em&gt;very expensive&lt;/em&gt;. We had a 14 node cluster churning through this script daily, and it took on average about an hour and a half to run! I don&amp;rsquo;t have the exact cost figures on hand (and am not sure if I&amp;rsquo;d be authorized to share them if I did), but based on a back of the envelope sketch, this one single script was probably costing us somewhere on the order of $10-$40 a day (that works out to between $3650-$14600 a year).&lt;/p&gt;

&lt;p&gt;With our move to &lt;a href="https://cloud.google.com/bigquery/"&gt;BigQuery&lt;/a&gt;, things get a lot simpler! Thanks to the combined effort of my team and data operations[1], we now produce &amp;ldquo;stable&amp;rdquo; ping tables on a daily basis with &lt;em&gt;all&lt;/em&gt; the relevant histogram data (stored as JSON blobs), queryable using relatively vanilla SQL. In this case, the data we care about is in &lt;code&gt;telemetry.main&lt;/code&gt; (named after the main ping, appropriately enough). With the help of a small &lt;a href="https://cloud.google.com/bigquery/docs/reference/standard-sql/user-defined-functions"&gt;JavaScript UDF&lt;/a&gt; function, all of this data can easily be extracted into a table inside a single SQL query scheduled by &lt;a href="https://docs.telemetry.mozilla.org/tools/stmo.html"&gt;sql.telemetry.mozilla.org&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE TEMP FUNCTION
  udf_js_json_extract_highest_long_spinner (input STRING)
  RETURNS INT64
  LANGUAGE js AS """
    if (input == null) {
      return 0;
    }
    var result = JSON.parse(input);
    var valuesMap = result.values;
    var highest = 0;
    for (var key in valuesMap) {
      var range = parseInt(key);
      if (valuesMap[key]) {
        highest = range &amp;gt; 0 ? range : 1;
      }
    }
    return highest;
""";

SELECT build_id,
sum (case when highest &amp;gt;= 64000 then 1 else 0 end) as v_64000ms_or_higher,
sum (case when highest &amp;gt;= 27856 and highest &amp;lt; 64000 then 1 else 0 end) as v_27856ms_to_63999ms,
sum (case when highest &amp;gt;= 12124 and highest &amp;lt; 27856 then 1 else 0 end) as v_12124ms_to_27855ms,
sum (case when highest &amp;gt;= 5277 and highest &amp;lt; 12124 then 1 else 0 end) as v_5277ms_to_12123ms,
sum (case when highest &amp;gt;= 2297 and highest &amp;lt; 5277 then 1 else 0 end) as v_2297ms_to_5276ms,
sum (case when highest &amp;gt;= 1000 and highest &amp;lt; 2297 then 1 else 0 end) as v_1000ms_to_2296ms,
sum (case when highest &amp;gt; 0 and highest &amp;lt; 50 then 1 else 0 end) as v_0ms_to_49ms,
sum (case when highest &amp;gt;= 50 and highest &amp;lt; 100 then 1 else 0 end) as v_50ms_to_99ms,
sum (case when highest &amp;gt;= 100 and highest &amp;lt; 200 then 1 else 0 end) as v_100ms_to_199ms,
sum (case when highest &amp;gt;= 200 and highest &amp;lt; 400 then 1 else 0 end) as v_200ms_to_399ms,
sum (case when highest &amp;gt;= 400 and highest &amp;lt; 800 then 1 else 0 end) as v_400ms_to_799ms,
count(*) as count
from
(select build_id, client_id, max(greatest(highest_long, highest_short)) as highest
from
(SELECT
    SUBSTR(application.build_id, 0, 8) as build_id,
    client_id,
    udf_js_json_extract_highest_long_spinner(payload.histograms.FX_TAB_SWITCH_SPINNER_VISIBLE_LONG_MS) AS highest_long,
    udf_js_json_extract_highest_long_spinner(payload.histograms.FX_TAB_SWITCH_SPINNER_VISIBLE_MS) as highest_short
FROM telemetry.main
WHERE
    application.channel='nightly'
    AND normalized_os='Windows'
    AND application.build_id &amp;gt; FORMAT_DATE("%Y%m%d", DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER))
    AND DATE(submission_timestamp) &amp;gt;= DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER))
group by build_id, client_id) group by build_id;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In addition to being much simpler, this new job is also &lt;em&gt;way&lt;/em&gt; cheaper. The last run of it scanned just over 1 TB of data, meaning it cost us just over $5. Not as cheap as I might like, but considerably less expensive than before: I&amp;rsquo;ve also scheduled it to only run once every other day, since Mike tells me he doesn&amp;rsquo;t need this data any more often than that.&lt;/p&gt;

&lt;p&gt;[1] I understand that Jeff Klukas, Frank Bertsch, Daniel Thorn, Anthony Miyaguchi, and Wesley Dawson are the principals involved - apologies if I&amp;rsquo;m forgetting someone.&lt;/p&gt;</description></item>
  <item>
   <title>Metrics Graphics: Stepping back for a while</title>
   <link>https://wlach.github.io/blog/2019/09/metrics-graphics-stepping-back-for-a-while/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2019-09-metrics-graphics-stepping-back-for-a-while</guid>
   <pubDate>Thu, 26 Sep 2019 20:33:54 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;Just a note that I&amp;rsquo;ve decided to step back from &lt;a href="https://metricsgraphicsjs.org"&gt;metrics graphics&lt;/a&gt; maintenance for the time being, which means that the project is essentially unowned. This has sort of been the case for a while, but I figured I should probably make it official.&lt;/p&gt;

&lt;p&gt;If you follow the link to the &lt;a href="https://github.com/metricsgraphics/metrics-graphics/"&gt;metrics graphics repository&lt;/a&gt;, you&amp;rsquo;ll note that the version has been bumped to &amp;ldquo;3.0-alpha3&amp;rdquo;. I was &lt;em&gt;this close&lt;/em&gt; to making one last new release this afternoon but decided I didn&amp;rsquo;t want to potentially break existing users who were fine using the last &amp;ldquo;official&amp;rdquo; version (v3.0 bumps the version of d3 used to &amp;ldquo;5&amp;rdquo;, among other breaking changes). I&amp;rsquo;d encourage people who want to continue using the library to make a fork and publish a copy under their user or organization name on npm.&lt;/p&gt;</description></item>
  <item>
   <title>mozregression update: python 3 edition</title>
   <link>https://wlach.github.io/blog/2019/09/mozregression-update-python-3-edition/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2019-09-mozregression-update-python-3-edition</guid>
   <pubDate>Mon, 16 Sep 2019 15:29:04 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;For those who are still wondering, yup, I am still maintaining &lt;a href="https://mozilla.github.io/mozregression/"&gt;mozregression&lt;/a&gt;, though increasingly reluctantly. Given how important this project is to the development of Firefox (getting a regression window using mozregression is standard operating procedure whenever a new bug is reported in Firefox), it feels like this project is pretty vital, so I continue out of some sense of obligation &amp;mdash; but really, someone more interested in Mozilla&amp;rsquo;a build, automation and testing systems would be better suited to this task: over the past few years, my interests/focus have shifted away from this area to building up Mozilla&amp;rsquo;s data storage and visualization platform.&lt;/p&gt;

&lt;p&gt;This post will describe some of the things that have happened in the last year and where I see the project going. My hope is to attract some new blood to add some needed features to the project and maybe take on some of the maintainership duties.&lt;/p&gt;

&lt;h2 id="python-3"&gt;python 3&lt;/h2&gt;

&lt;p&gt;The most important update is that, as of today, the command-line version of mozregression (v3.0.1) should work with python 3.5+. &lt;a href="https://python-modernize.readthedocs.io/en/latest/"&gt;modernize&lt;/a&gt; did most of the work for us, though there were some unit tests that needed updating: special thanks to &lt;a href="https://github.com/gloomy-ghost"&gt;@gloomy-ghost&lt;/a&gt; for helping with that.&lt;/p&gt;

&lt;p&gt;For now, we will continue to support python 2.7 in parallel, mainly because the GUI has not yet been ported to python 3 (more on that later) and we have CI to make sure it doesn&amp;rsquo;t break.&lt;/p&gt;

&lt;h2 id="other-updates"&gt;other updates&lt;/h2&gt;

&lt;p&gt;The last year has mostly been one of maintenance. Thanks in particular to Ian Moody (:kwan) for his work throughout the year &amp;mdash; including patches to adapt mozregression support to our new updates policy and shippable builds (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1532412"&gt;bug 1532412&lt;/a&gt;), and Kartikaya Gupta (:kats) for adding support for bisecting the GeckoView example app (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1507225"&gt;bug 1507225&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id="future-work"&gt;future work&lt;/h2&gt;

&lt;p&gt;There are a bunch of things I see us wanting to add or change with mozregression over the next year or so. I might get to some of these if I have some spare cycles, but probably best not to count on it:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Port the mozregression GUI to Python 3 (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1581633"&gt;bug  1581633&lt;/a&gt;) As mentioned  above, the command-line client works with python 3, but we have yet to port  the &lt;a href=""&gt;GUI&lt;/a&gt;. We should do that. This probably also entails porting the GUI to  use PyQT5 (which is pip-installable and thus much easier to integrate into a  CI process), see &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1426766"&gt;bug 1426766&lt;/a&gt;.&lt;/li&gt;
 &lt;li&gt;Make self-contained GUI builds available for MacOS X (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1425105"&gt;bug  1425105&lt;/a&gt;) and Linux  (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1581643"&gt;bug 1581643&lt;/a&gt;).&lt;/li&gt;
 &lt;li&gt;Improve our mechanism for producing a standalone version of the GUI in  general. We&amp;rsquo;ve used &lt;a href="https://github.com/anthony-tuininga/cx_Freeze"&gt;cx_Freeze&lt;/a&gt;  which mostly works ok, but has a number of problems (e.g. it pulls in a bunch of unnecessary dependencies, which  bloats the size of the installer). Upgrading the GUI to use python 3 may  alleviate some of these issues, but it might be worth considering other  options in this space, like Gregory Szorc&amp;rsquo;s &lt;a href="https://github.com/indygreg/PyOxidizer"&gt;pyoxidizer&lt;/a&gt;.&lt;/li&gt;
 &lt;li&gt;Add some kind of telemetry to mozregression to measure usage of this tool  (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1581647"&gt;bug 1581647&lt;/a&gt;).  My anecdotal experience is that this tool is pretty invaluable for Firefox  development and QA, but this is not immediately apparent to Mozilla&amp;rsquo;s  leadership and it&amp;rsquo;s thus very difficult to convince people to spend their  cycles on maintaining and improving this tool. Field data may help change  that story.&lt;/li&gt;
 &lt;li&gt;Supporting new Mozilla products which aren&amp;rsquo;t built (entirely) out of mozilla-central,  most especially Fenix (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1556042"&gt;bug 1556042&lt;/a&gt;)  and Firefox Reality (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1568488"&gt;bug 1568488&lt;/a&gt;).  This is probably rather involved (mozregression has a big pile of assumptions about how  the builds it pulls down are stored and organized) but that doesn&amp;rsquo;t mean that  this work isn&amp;rsquo;t necessary.&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;If you&amp;rsquo;re interested in working on any of the above, please feel free to dive in on one of the above bugs. I can&amp;rsquo;t offer formal mentorship, but am happy to help out where I can.&lt;/p&gt;</description></item>
  <item>
   <title>Time for some project updates</title>
   <link>https://wlach.github.io/blog/2019/09/time-for-some-project-updates/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2019-09-time-for-some-project-updates</guid>
   <pubDate>Mon, 16 Sep 2019 14:41:39 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;I&amp;rsquo;ve been a bit bad about updating this blog over the past year or so, though this hasn&amp;rsquo;t meant there haven&amp;rsquo;t been things to talk about. For the next couple weeks, I&amp;rsquo;m going to try to give some updates on the projects I have been spending time on in the past year, both old and new. I&amp;rsquo;m going to begin with some of the less-loved things I&amp;rsquo;ve been working on, partially in an attempt to motivate some forward-motion on things that I believe are rather important to Mozilla.&lt;/p&gt;

&lt;p&gt;More to come.&lt;/p&gt;</description></item>
  <item>
   <title>Goodbye, noble stead</title>
   <link>https://wlach.github.io/blog/2019/05/goodbye-noble-stead/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2019-05-goodbye-noble-stead</guid>
   <pubDate>Wed, 22 May 2019 00:52:21 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;With lots of sadness, I just gave away my vintage 1970s velo-sport. This was my first bike I owned as a mid-twentysomething adult (I temporarily gave up biking and took up smoking when I was 19, two of the worst decisions of my life). Picked it up at the suggestion of my now ex-wife after frustration with the Halifax bus system. Used it to &lt;a href="https://www.thecoast.ca/halifax/beta-the-public-transit-day-tripper/Content?oid=1098826"&gt;hack said transit system&lt;/a&gt; amidst riding hundreds and hundreds of kilometres on the Halifax penisula and beyond. A few years later, it came with me to Montreal, where we rode even more - near-daily commutes from NDG to Mile End and rides for pie to Mont St Hilaire. Finally, it was fated to come with me to Toronto, where it served to take me on long rides on the waterfront trail, east to Pickering, west as far as Niagara Falls, as well as serving as a daily commuter from Leslieville to my office in the Fashion District, not to mention early-morning jaunts across town for 6am sittings at the Toronto Zen Centre.&lt;/p&gt;

&lt;p&gt;&lt;img style="width:400px" srcset="/files/2019/05/wills_bike_1.jpg 2x" /&gt;&lt;/p&gt;

&lt;p&gt;Over the years, I&amp;rsquo;ve probably sunk over two thousand dollars into repairs (as well as doing a fair number of work on it myself). An issue with the front wheel puncturing three tubes in succession finally convinced me that this was a losing battle, unless I wanted to put down a large sum into an overhaul. If I used it less I could probably justify some more minor repairs, but I think most of it (with the exception of the frame and headset) is on its last legs. Given the amount that I bicycle, it just seemed to make sense to get a new one, and see what a modern commuter has to offer. So I decided to cut my losses and buy a brand new one at Urbane Cycling.&lt;/p&gt;

&lt;p&gt;It was a hard decision to replace it, and harder still to give it away. I &lt;em&gt;loved&lt;/em&gt; this old bike, far more than any inanimate thing that has been in my presence. I am grateful for the good care it took of me (I have not had a single major accident riding it), and the adventures it enabled me to have. I took this picture of it in the donation rack at Bikesauce, which is almost certainly the last I&amp;rsquo;ll see of it:&lt;/p&gt;

&lt;p&gt;&lt;img style="width:400px" srcset="/files/2019/05/wills_bike_2.jpg 2x" /&gt;&lt;/p&gt;

&lt;p&gt;I left a note with some pointers on what needs work and asking whoever fixes it up to give it a lot of love, but honestly this bike&amp;rsquo;s fate is out of my control at this point. So it is with all things.&lt;/p&gt;</description></item>
  <item>
   <title>New ideas, old buildings</title>
   <link>https://wlach.github.io/blog/2019/03/new-ideas-old-buildings/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2019-03-new-ideas-old-buildings</guid>
   <pubDate>Fri, 22 Mar 2019 19:08:11 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;Last week, Brendan Colloran &lt;a href="https://hacks.mozilla.org/2019/03/iodide-an-experimental-tool-for-scientific-communicatiodide-for-scientific-communication-exploration-on-the-web/"&gt;announced Iodide&lt;/a&gt;, a new take on scientific collaboration and reporting that I&amp;rsquo;ve been really happy to contribute to over the past year-and-a-bit. I&amp;rsquo;ve been describing it to people I meet as kind of "&lt;a href="https://glitch.com/"&gt;glitch&lt;/a&gt; meets &lt;a href="https://jupyter.org/"&gt;jupyter&lt;/a&gt; " but that doesn&amp;rsquo;t quite do it justice. I&amp;rsquo;d recommend reading Brendan&amp;rsquo;s blog post (and taking a look at our &lt;a href="https://alpha.iodide.io"&gt;demonstration site&lt;/a&gt;) to get the full picture.&lt;/p&gt;

&lt;p&gt;One question that I&amp;rsquo;ve heard asked (including on Brendan&amp;rsquo;s post) is why we chose a rather conventional and old technology (&lt;a href="https://www.djangoproject.com/"&gt;Django&lt;/a&gt;) for the server backend. Certainly, Iodide has not been shy about building with relatively new or experimental technologies for other parts (e.g. Python on WebAssembly for the notebooks, React/Redux for the frontend). Why not complete the cycle by using a new-fangled JavaScript web server like, I don&amp;rsquo;t know, &lt;a href="https://nestjs.com/"&gt;NestJS&lt;/a&gt;? And while we&amp;rsquo;re at it, what&amp;rsquo;s with iodide&amp;rsquo;s ridiculous &lt;a href="https://en.wikipedia.org/wiki/Representational_state_transfer"&gt;REST API&lt;/a&gt;? Don&amp;rsquo;t you know &lt;a href="https://graphql.org/"&gt;GraphQL&lt;/a&gt; is the only legitimate way to expose your backend to the world in 2019?&lt;/p&gt;

&lt;p&gt;The great urban theorist of the twentieth century, &lt;a href="https://en.wikipedia.org/wiki/Jane_Jacobs"&gt;Jane Jacobs&lt;/a&gt; has a quote I love:&lt;/p&gt;

&lt;p&gt;
 &lt;i&gt;“Old ideas can sometimes use new buildings. New ideas must use old buildings.”&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;Laura Thompson (an engineering director at Mozilla) has restated this wisdom in a software development context as &lt;a href="https://speakerdeck.com/lauraxt/build-exciting-things-with-boring-technologies"&gt;&amp;ldquo;Build exciting things with boring technologies&amp;rdquo;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It so happened that the server was not an area Iodide was focusing on for innovation (at least initially), so it made much, much more sense to use something proven and battle-tested for the server side deployment. I&amp;rsquo;d used Django for a number of projects at Mozilla before this one (&lt;a href="https://github.com/mozilla/treeherder"&gt;Treeherder/Perfherder&lt;/a&gt; and &lt;a href="https://github.com/mozilla/missioncontrol/"&gt;Mission Control&lt;/a&gt;) and have been wildly impressed by the project&amp;rsquo;s excellent &lt;a href="https://docs.djangoproject.com/"&gt;documentation&lt;/a&gt;, &lt;a href="https://docs.djangoproject.com/en/2.1/topics/db/"&gt;database access layer&lt;/a&gt;, and support for building a standardized API via the &lt;a href="https://www.django-rest-framework.org/"&gt;Django REST Framework&lt;/a&gt; add-on. Not to mention the fact that so much of Mozilla&amp;rsquo;s in-house ops and web development expertise is based around this framework (I could name off probably 5 or 6 internal business systems based around the Django stack, in addition to Treeherder), so deploying Iodide and getting help building it would be something of a known quantity.&lt;/p&gt;

&lt;p&gt;Only slightly more than half a year since I began work on the iodide server, we now have both a publicly accessible site for others to experiment with &lt;em&gt;and&lt;/em&gt; an internal one for Mozilla&amp;rsquo;s business needs. It&amp;rsquo;s hard to say what would have happened had I chosen something more experimental to build Iodide&amp;rsquo;s server piece, but at the very least there would have been a substantial learning curve involved &amp;mdash; in addition to engineering effort to fill in the gaps where the new technology is not yet complete &amp;mdash; which would have meant less time to innovate where it really mattered. Django&amp;rsquo;s &lt;a href="https://docs.djangoproject.com/en/2.1/topics/migrations/"&gt;database migration system&lt;/a&gt;, for example, took years to come to fruition and I&amp;rsquo;m not aware of anything comparable in the world of JavaScript web frameworks.&lt;/p&gt;

&lt;p&gt;As we move ahead, we may find places where applying new backend server technologies makes sense. Heck, maybe we&amp;rsquo;ll chose to rewrite the whole thing at some point. But to get to launch, chosing a bunch of boring, tested software for this portion of Iodide was (in my view) absolutely the right decision and I make no apologies for it.&lt;/p&gt;</description></item>
  <item>
   <title>Making contribution work for Firefox tooling and data projects</title>
   <link>https://wlach.github.io/blog/2018/11/making-contribution-work-for-firefox-tooling-and-data-projects/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2018-11-making-contribution-work-for-firefox-tooling-and-data-projects</guid>
   <pubDate>Mon, 26 Nov 2018 16:13:46 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;One of my favorite parts about Mozilla is mentoring and working alongside third party contributors. Somewhat surprisingly since I work on internal tools, I&amp;rsquo;ve had a fair amount of luck finding people to help work on projects within my purview: &lt;a href="https://mozilla.github.io/mozregression"&gt;mozregression&lt;/a&gt;, &lt;a href="https://wiki.mozilla.org/EngineeringProductivity/Projects/Perfherder"&gt;perfherder&lt;/a&gt;, &lt;a href="https://metricsgraphics.org"&gt;metrics graphics&lt;/a&gt;, and others have all benefited from the contributions of people outside of Mozilla.&lt;/p&gt;

&lt;p&gt;In most cases (a notable exception being metrics graphics), these have been internal-tooling projects used by others to debug, develop, or otherwise understand the behaviour of Firefox. On the face of it, none of the things I work on are exactly &amp;ldquo;high profile cutting edge stuff&amp;rdquo; in the way, say, Firefox or the Rust Programming Language are. So why do they bother? The exact formula varies depending on contributor, but I think it usually comes down to some combination of these two things:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;A desire to learn and demonstrate competence with industry standard  tooling (the python programming language, frontend web development, backend  databases, &amp;ldquo;big data&amp;rdquo; technologies like Parquet, &amp;hellip;).&lt;/li&gt;
 &lt;li&gt;A desire to work with and gain recognition inside of a community of  like-minded people.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;Pretty basic, obvious stuff &amp;mdash; there is an appeal here to basic human desires like the need for security and a sense of belonging. Once someone&amp;rsquo;s &amp;ldquo;in the loop&amp;rdquo;, so to speak, generally things take care of themselves. The real challenge, I&amp;rsquo;ve found, is getting people from the &amp;ldquo;I am potentially interested in doing something with Mozilla internal tools&amp;rdquo; to the stage that they are confident and competent enough to work in a reasonably self-directed way. When I was on the A-Team, we classified this transition in terms of a &lt;a href="https://ateam-bootcamp.readthedocs.io/en/latest/guide/curve.html"&gt;commitment curve&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img srcset="/files/2018/11/commitment-curve-visualization.png 2x" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1080119"&gt;prototype commitment curve graphic by Steven Brown&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The hardest part, in my experience, is the initial part of that curve. At this point, people are just dipping their toe in the water. Some may not have a ton of experience with software development yet. In other cases, my projects may just not be the right fit for them. But of course, sometimes there is a fit, or at least one could be developed! What I&amp;rsquo;ve found most helpful is &amp;ldquo;clearing a viable path&amp;rdquo; forward for the right kind of contributor. That is, some kind of initial hypothesis of what a successful contribution experience would look like as a new person transitions from &amp;ldquo;explorer&amp;rdquo; stage in the chart above to &amp;ldquo;associate&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t exactly have a perfect template for what &amp;ldquo;clearing a path&amp;rdquo; looks like in every case. It depends quite a bit on the nature of the contributor. But there are some common themes that I&amp;rsquo;ve found effective:&lt;/p&gt;

&lt;p&gt;First, provide good, concise documentation both on the project&amp;rsquo;s purpose and vision and how to get started easily and keep it up to date. For projects with a front-end web component, I try to decouple the front end parts from the backend services so that people can &lt;code&gt;yarn install &amp;amp;&amp;amp; yarn start&lt;/code&gt; &lt;a href="/blog/2014/09/hacking-on-the-treeherder-front-end-refreshingly-easy/"&gt;their way to success&lt;/a&gt;. Being able to &lt;em&gt;see&lt;/em&gt; the project in action quickly (and not getting stuck on some mundane getting started step) is key in maintaining initial interest.&lt;/p&gt;

&lt;p&gt;Second, provide a set of good starter issues (sometimes called &amp;ldquo;good first bugs&amp;rdquo;) for people to work on. Generally these would be non-critical-path type issues that have straightforward instructions to resolve and fix. Again, the idea here is to give people a sense of quick progress and resolution, a &amp;ldquo;yes I can actually do this&amp;rdquo; sort of feeling. But be careful not to let a contributor get stuck here! These bugs take a disproportionate amount of effort to file and mentor compared to their actual value &amp;mdash; the key is to progress the contributor to the next level once it&amp;rsquo;s clear they can handle the basics involved in solving such an issue (checking out the source code, applying a fix, submitting a patch, etc). Otherwise you&amp;rsquo;re going to feel frustrated and wonder why you&amp;rsquo;re on an endless treadmill of writing up trivial bugs.&lt;/p&gt;

&lt;p&gt;Third, once a contributor has established themselves by fixing a few of these simple issues, I try to get to know them a little better. Send them an email, learn where they&amp;rsquo;re from, invite them to chat on the project channel if they can. At the same time, this is an opportunity to craft a somewhat larger piece of work (a sort of mini-project) that they can do, tailored to the interests. For example, a new contributor on the Mission Control has recently been working on adding &lt;a href="https://jestjs.io/"&gt;Jest&lt;/a&gt; tests to the project &amp;mdash; I provided some basic guidance of things to look at, but did not dictate exactly &lt;em&gt;how&lt;/em&gt; to perform the task. They figured that out for themselves.&lt;/p&gt;

&lt;p&gt;As time goes by, you just continue this process. Depending on the contributor, they may start coming up with their own ideas for how a project might be improved or they might still want to follow your lead (or that of the team), but at the least I generally see an improvement in their self-directedness and confidence after a period of sustained contribution. In either case, the key to success remains the same: sustained and positive communication and sharing of goals and aspirations, making sure that both parties are getting something positive out of the experience. Where possible, I try to include contributors in team meetings. Where there&amp;rsquo;s an especially close working relationship (e.g. &lt;a href="https://summerofcode.withgoogle.com/archive/"&gt;Google Summer of Code&lt;/a&gt;). I try to set up a weekly one on one. Regardless, I make reviewing code, answering questions, and providing suggestions on how to move forward a top priority (i.e. not something I&amp;rsquo;ll leave for a few days). It&amp;rsquo;s the least I can do if someone is willing to take time out to contribute to my project.&lt;/p&gt;

&lt;p&gt;If this seems similar to the best practices for how members of a team should onboard each other and work together, that&amp;rsquo;s not really a coincidence. Obviously the relationship is a little different because we&amp;rsquo;re not operating with a formal managerial structure and usually the work is unpaid: I try to bear that mind and make double sure that contributors are really getting some useful skills and habits that they can take with them to future jobs and other opportunities, while also emphasizing that their code contributions are their own, not Mozilla&amp;rsquo;s. So far it seems to have worked out pretty well for all concerned (me, Mozilla, and the contributors).&lt;/p&gt;</description></item>
  <item>
   <title>pydata nyc 2018</title>
   <link>https://wlach.github.io/blog/2018/10/pydata-nyc-2018/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2018-10-pydata-nyc-2018</guid>
   <pubDate>Mon, 29 Oct 2018 13:19:23 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;Went to &lt;a href="https://pydata.org/nyc2018/"&gt;PyData NYC&lt;/a&gt; a couple weeks ago, and figured I ought to write up my thoughts for the benefits of the others on my extended team. Why not publish as a blog post while I&amp;rsquo;m at it?&lt;/p&gt;

&lt;p&gt;This is actually the first conference I&amp;rsquo;d been to in my capacity as a &amp;ldquo;data engineer&amp;rdquo; at Mozilla, a team I joined about a year and a half ago after specializing in the same area on the (now-defunct) &lt;a href="https://wiki.mozilla.org/EngineeringProductivity"&gt;a-team&lt;/a&gt;. I&amp;rsquo;ve felt a special affinity for the Python community, particularly its data science offshoots (pandas, numpy, and jupyter notebooks) so it was great to finally go to a conference that specializes in these topics.&lt;/p&gt;

&lt;p&gt;Overall, the conference was a bit of a mix between people talking about the status of their projects, theoretical talks on specific statistical approaches to data, general talks on how people are doing &amp;ldquo;data science&amp;rdquo; (I would say the largest majority of attendees at the conference were users of python data science tools, rather than developers), and case studies of how people are using python data science tools in their research or work. This being New York, many (probably the majority) were using data science tools in fields like quantitative finance, sales, marketing, and health care.&lt;/p&gt;

&lt;p&gt;As a side note, it was really satisfying to be able to tell Mozilla&amp;rsquo;s story about how we collect and use data without violating the privacy of our users. This is becoming more and more of an issue (especailly in Europe with the GPDR) and it really makes me happy that we have a really positive story to tell, not a bunch of dirty secrets that we need to hide.&lt;/p&gt;

&lt;p&gt;In general I found the last two types of talks the most rewarding to go to: most of the work I do at Mozilla currently involves larger-scale data where, I&amp;rsquo;m sad to say, Python is usually not (currently) an applicable tool, at least not by itself (though maybe &lt;a href="https://iodide.io"&gt;iodide&lt;/a&gt; will help change that! see below). And I don&amp;rsquo;t usually find a 60 minute talk really enough time for me to be able to properly absorb new mathematical or statistical concepts, though I can sometimes get little tidbits of information from them that come in handy later.&lt;/p&gt;

&lt;p&gt;Some talks that made an impression on me:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="https://pydata.org/nyc2018/schedule/presentation/83/"&gt;Open source and quantitative finance&lt;/a&gt;: Keynote  talk, was a great introduction to the paranoia of the world of quantitative finance.  I think the main message was that things are gradually moving to a (slightly less)  paranoid model where generally-useful modifications done to numerical/ml software as  part of a trading platform may now be upstreamed&amp;hellip; but my main takeaway is that I&amp;rsquo;m  really glad I&amp;rsquo;m not working in that industry.&lt;/li&gt;
 &lt;li&gt;&lt;a href="https://pydata.org/nyc2018/schedule/presentation/9/"&gt;Words in Space&lt;/a&gt;: Introduced  an interesting-soundingl library called &lt;a href="http://www.scikit-yb.org/en/latest/"&gt;Yellow Brick&lt;/a&gt; for visualizing the results  of machine learning models.&lt;/li&gt;
 &lt;li&gt;&lt;a href="https://pydata.org/nyc2018/schedule/presentation/2/"&gt;Creating a data-driven product culture&lt;/a&gt;:  General talk on how to create a positive and useful data science culture at a company. I think  Mozilla already checks most of the boxes outlined in the talk.&lt;/li&gt;
 &lt;li&gt;&lt;a href="https://pydata.org/nyc2018/schedule/presentation/39/"&gt;What Data Scientists Really Do&lt;/a&gt;: Quite entertaining talk on the future of &amp;ldquo;data science&amp;rdquo;,  by Hugo Bowne-Anderson (who also has a &lt;a href="https://www.datacamp.com/community/podcast"&gt;podcast&lt;/a&gt; which sounds cool). The most  interesting takeaway from the talk was the speculation that within 10 years the term  &amp;ldquo;data scientist&amp;rdquo; might have the same meaning as the word &amp;ldquo;webmaster&amp;rdquo; now. It&amp;rsquo;s a  hyper-generalist job description which will almost inevitably be split into a number  of other more specialized roles.&lt;/li&gt;
 &lt;li&gt;&lt;a href="https://pydata.org/nyc2018/schedule/presentation/77/"&gt;Master Class: Bayesian Statistics&lt;/a&gt;: This falls under the &amp;ldquo;technical talk which I couldn&amp;rsquo;t  grasp in 60 minutes&amp;rdquo; category, but I think I finally do understand a little bit more  of what people mean when they say &amp;ldquo;Bayesian Statistics&amp;rdquo; now. It actually doesn&amp;rsquo;t have much  to do with &lt;a href="https://en.wikipedia.org/wiki/Bayes%27_theorem"&gt;Baye&amp;rsquo;s Theorem&lt;/a&gt;, rather it seems to be more of a philosophical approach to  data analysis which acknowledges the limitations of human capacity to understand the  world and asks us to more explicitly state our assumptions when developing models (probably  over-simplifying here). I think I can get behind that &amp;mdash; want to learn more. They  provided a &lt;a href="https://betanalpha.github.io/workshops/pydata/"&gt;bunch of material&lt;/a&gt; to work through,  which I&amp;rsquo;ve been meaning to take a look at.&lt;/li&gt;
 &lt;li&gt;&lt;a href="https://pydata.org/nyc2018/schedule/presentation/30/"&gt;Data Science in Health Care: Beyond the Hype&lt;/a&gt;:  Great presentations in how data science can be used to improve health care outcomes. Lots of relevant  insights that I think are also applicable to &amp;ldquo;product health&amp;rdquo; here at Mozilla. I  particularly liked the way the presenter framed requirements when deciding whether or not  to do a type of analysis: &amp;ldquo;if i knew [information], i would do [intervention], which would  have [measurable outcome]&amp;rdquo;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Of course, this post wouldn&amp;rsquo;t be complete without a mention of &lt;a href="http://droettboom.com/"&gt;Mike Droettboom&lt;/a&gt;&amp;rsquo;s &lt;a href="https://pydata.org/nyc2018/schedule/presentation/3/"&gt;talk&lt;/a&gt; on &lt;a href="https://iodide.io"&gt;iodide&lt;/a&gt;, a project I&amp;rsquo;ve been spending some considerable cycles helping with over the last couple of quarters. I need to write some longer thoughts on iodide at some point in the near future, but in a nutshell it&amp;rsquo;s a scientific notebook environment where the computational kernel lives entirely inside the browser. It was well received and we had a great followup session afterwards with people interested in using it for various things. Being able to show a python environment in the browser which &amp;ldquo;just works&amp;rdquo;, with no installation or other steps makes a &lt;em&gt;great&lt;/em&gt; tech demo. I&amp;rsquo;m really excited about the public launch of our server-based environment, which will hopefully be coming in the next couple of months.&lt;/p&gt;</description></item>
  <item>
   <title>Mission Control 1.0</title>
   <link>https://wlach.github.io/blog/2018/06/mission-control-1-0/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2018-06-mission-control-1-0</guid>
   <pubDate>Tue, 05 Jun 2018 21:50:32 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;Just a quick announcement that the first &amp;ldquo;production-ready&amp;rdquo; version of Mission Control just went live yesterday, at this easy-to-remember URL:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://missioncontrol.telemetry.mozilla.org"&gt;https://missioncontrol.telemetry.mozilla.org&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For those not yet familiar with the project, Mission Control aims to track release stability and quality across Firefox releases. It is similar in spirit to arewestableyet and other crash dashboards, with the following new and exciting properties:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Uses the full set of crash counts gathered via telemetry, rather than the arbitrary  sample that users decide to submit to crash-stats&lt;/li&gt;
 &lt;li&gt;Results are available within minutes of ingestion by telemetry (although be warned  &lt;a href="/blog/2017/10/better-or-worse-by-what-measure/"&gt;initial results for a release always look bad&lt;/a&gt;)&lt;/li&gt;
 &lt;li&gt;The denominator in our crash rate is usage hours, rather than the probably-incorrect  calculation of active-daily-installs used by  &lt;a href="https://arewestableyet.com"&gt;arewestableyet&lt;/a&gt; (not a knock on the people who  wrote that tool, there was nothing better available at the time)&lt;/li&gt;
 &lt;li&gt;We have a detailed breakdown of the results by platform (rather than letting Windows  results dominate the overall rates due to its high volume of usage)&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;In general, my hope is that this tool will provide a more scientific and accurate idea of release stability and quality over time. There&amp;rsquo;s lots more to do, but I think this is a promising start. Much gratitude to &lt;a href="https://home.kairo.at/blog/2014-04/how_effective_is_the_stability_program"&gt;kairo&lt;/a&gt;, calixte, &lt;a href="https://chuttenblog.wordpress.com/"&gt;chutten&lt;/a&gt; and others who helped build my understanding of this area.&lt;/p&gt;

&lt;p&gt;The dashboard itself an easier thing to show than talk about, so I recorded a quick demonstration of some of the dashboard&amp;rsquo;s capabilities and published it on air mozilla:&lt;/p&gt;

&lt;iframe src="https://air.mozilla.org/mission-control-dashboard-intro/video/" width="896" height="524" frameborder="0" allowfullscreen="allowfullscreen"&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;a href="https://air.mozilla.org/mission-control-dashboard-intro/"&gt;link&lt;/a&gt;&lt;/p&gt;</description></item>
  <item>
   <title>Some thoughts on opinion polling in the Ontario 2018 election</title>
   <link>https://wlach.github.io/blog/2018/05/some-thoughts-on-opinion-polling-in-the-ontario-2018-election/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2018-05-some-thoughts-on-opinion-polling-in-the-ontario-2018-election</guid>
   <pubDate>Sun, 27 May 2018 16:57:30 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;Like many, I&amp;rsquo;ve been a bit worried about the Ontario election, and have been rather obsessively checking a site called the &lt;a href="https://newsinteractives.cbc.ca/onvotes/poll-tracker/"&gt;Ontario Poll Tracker&lt;/a&gt;.&lt;/p&gt;

&lt;center&gt;&lt;img style="width:400px" srcset="/files/2018/05/CBC-poll-tracker-main.png 2x" /&gt;&lt;/center&gt;

&lt;p&gt;It has nice and shiny graphs and uses authoritative language and purports to provide a scientific analysis which predicts the election. Despite this, it&amp;rsquo;s my assertion that this kind of predictive modelling is nothing more than snake oil. I keep on reminding myself that I shouldn&amp;rsquo;t take it too seriously, but haven&amp;rsquo;t been too successful so far. This blog post is a reminder to myself on why I should stop reloading that site so much, but maybe it will be helpful to others as well. As a warning, it&amp;rsquo;s not going to say anything particularly novel. If you have any kind of background in statistics at all, this is probably going to be quite boring.&lt;/p&gt;

&lt;p&gt;First, a story. Way back when I had just graduated from university in 2003, I worked briefly at an &amp;ldquo;opinion research company&amp;rdquo;, telephoning people for various opinion surveys. It was easily the worst job I ever had, horrible for both the people doing the calling and those who were being called.&lt;/p&gt;

&lt;p&gt;The work was mind-numbingly repetitive. Get assigned a poll. Telephone people using an autodialer, work through the script using the DOS-based software the call center was using where they would answer multiple-choice questions. Repeat as many times as you can over the course of an hour. The topics were varied, but roughly 50/50 political parties doing private polling and businesses trying to get marketing data. In either case, the questions were definitely of the &amp;ldquo;lowest common denominator&amp;rdquo; type question (i.e. &amp;ldquo;Which products are you likely to buy in the next 12 months&amp;rdquo;, &amp;ldquo;If an election were held today, would you vote for party A, B, or C?&amp;rdquo;)&lt;/p&gt;

&lt;p&gt;One of the few benefits of tedious jobs is that they give you time to think about things. In this case, one of my distinct experiential take aways as that the results that we were getting were incredibly unrepresentative.&lt;/p&gt;

&lt;p&gt;For a poll to be valid it is supposed to be &amp;ldquo;reasonably&amp;rdquo; reflective of the general population. Over the quantities that we&amp;rsquo;re talking about, that means anywhere from hundreds of thousands to millions of people. If we were able to truly randomly sample a small number from this group, the results are likely to be &amp;ldquo;representative of the whole&amp;rdquo; (within some confidence interval). Let&amp;rsquo;s write up a small python script to confirm this intuition:&lt;/p&gt;

&lt;div class="brush: py"&gt;
 &lt;div class="colorful"&gt;
  &lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# 100,000 random numbers between 0 and 1&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;full_population_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100000&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;full_sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;full_population_size&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="c1"&gt;# average over entire result&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;full_sample&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;full_population_size&lt;/span&gt;
&lt;span class="mf"&gt;0.501036568906331&lt;/span&gt;

&lt;span class="c1"&gt;# pull out 100 randomly selected values from the full sample and&lt;/span&gt;
&lt;span class="c1"&gt;# get their average&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;random_subset_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;random_subset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;full_sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;100000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt;
                     &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random_subset_size&lt;/span&gt;&lt;span class="p"&gt;)]]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random_subset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;random_subset_size&lt;/span&gt;
&lt;span class="mf"&gt;0.4924555517866068&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;Only a small fraction of the total population, but a result within 1% of the true value. Expressing it this way makes random sampling almost like a tautology. You probably learned this in high school. Great right?&lt;/p&gt;

&lt;p&gt;Unfortunately, real life always comes in to disturb these assumptions, as it always does. You see, there were all sorts of factors that would affect who we would be talking to and thus get datapoints from. At that time, most of the population still had a land-line telephone but there were a wealth of other factors that meant that we weren&amp;rsquo;t getting a truly randoms sample of data. Men (at least men under 60 or so) were much less likely to answer a telephone survey than women. For general opinion surveys, we were calling at a specific time of day when &lt;em&gt;most&lt;/em&gt; people were likely to be available &amp;mdash; but that certainly wouldn&amp;rsquo;t apply to everyone. Some people would work night shifts, etc., etc. In our example above, this would be like taking out half the results over (say) 0.75 from our sample &amp;mdash; the end result would tend to skew much lower than the true value.&lt;/p&gt;

&lt;p&gt;Just for fun, let&amp;rsquo;s try doing that and see how it affects the results:&lt;/p&gt;

&lt;div class="brush: py"&gt;
 &lt;div class="colorful"&gt;
  &lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# if we take away approximately half the results with a value of&lt;/span&gt;
&lt;span class="c1"&gt;# &amp;gt;0.75, the population we are sampling from is reduced proportionally&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;full_sample_with_half75_removed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;full_sample&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mf"&gt;0.75&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;full_sample_with_half75_removed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;87607&lt;/span&gt;

&lt;span class="c1"&gt;# the sampled value is then proportionally skewed downwards (because&lt;/span&gt;
&lt;span class="c1"&gt;# a large percentage of the high values are no longer available)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;random_subset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;full_sample_with_half75_removed&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt;
                     &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;full_sample_with_half75_removed&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt;
                      &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random_subset_size&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
                    &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random_subset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;random_subset_size&lt;/span&gt;
&lt;span class="mf"&gt;0.4585241853943395&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;To try and get around this problem, the opinion polling company would try to demographically restrict who we were surveying past a certain point, so that the overall sample of the poll would reasonably reflect the characteristics of the population. This probably helped, but there&amp;rsquo;s only so much you can do here. For example, if you correct for the fact that men aged 20 to 60 are less likely to answer an opinion survey, your sample is going to now consist of those weird men who &lt;em&gt;do&lt;/em&gt; answer opinion surveys. Who knows what effect that&amp;rsquo;s going to have on your results?&lt;/p&gt;

&lt;p&gt;I want to be clear here: this is a methodological problem. Running more opinion polls doesn&amp;rsquo;t help. Probably some samples will be more affected by errors than others, but the problem remains regardless. Actually, let&amp;rsquo;s show this trivially for our small example:&lt;/p&gt;

&lt;div class="brush: py"&gt;
 &lt;div class="colorful"&gt;
  &lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;skewed_averages&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;   &lt;span class="n"&gt;full_sample_with_half75_removed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;full_sample&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mf"&gt;0.75&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt;
                                         &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;   &lt;span class="n"&gt;random_subset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;full_sample_with_half75_removed&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt;
          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;full_sample_with_half75_removed&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt;
           &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random_subset_size&lt;/span&gt;&lt;span class="p"&gt;)]]&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;   &lt;span class="n"&gt;skewed_averages&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random_subset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random_subset&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;skewed_averages&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.4585241853943395&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.4271412530288919&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.46414511969024697&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.4360740890986547&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="mf"&gt;0.4779021127791633&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.38419133106708714&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.48688298744651576&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.41076028280889915&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
 &lt;span class="mf"&gt;0.47975630795860363&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.4381467970818846&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;Each time we resampled from the main population and got a different result, but the end result was still one which was &lt;em&gt;far&lt;/em&gt; off from what we know in this case was the true value (0.5). Sampling from bad data doesn&amp;rsquo;t make up for these problems, it just gives you more bad results.&lt;/p&gt;

&lt;p&gt;Now, flash forward to 2018. Almost no one under 50 has a land-line anymore, people who have cell phones most often don&amp;rsquo;t answer to unknown callers. And don&amp;rsquo;t even get me started on how to find a representative set of people to participate in an &amp;ldquo;online panel&amp;rdquo;. What validity do polls have under these circumstances? I would say very little and probably more importantly we don&amp;rsquo;t even have a clear idea of &lt;em&gt;how&lt;/em&gt; our modern polls are skewed.&lt;/p&gt;

&lt;p&gt;There has been no shortage of thinking on how to correct for these problems but in my opinion it&amp;rsquo;s all just speculative and largely invalid. You can&amp;rsquo;t definitively solve the kind of uncertainty we&amp;rsquo;re talking about here by coming up with &amp;ldquo;just so&amp;rdquo; stories about how you&amp;rsquo;ve corrected for it. We might have some ideas about how our data is biased, but short of sampling the entire population and then seeing how our sampling method falls into that superset (which is impossible) there is no way of confirming that our efforts to correct for that bias were effective.&lt;/p&gt;

&lt;p&gt;With respect to the Ontario election which I alluded to above, the one thing that I am getting from the data is that support for the NDP (across the highly unrepresentative sample used in the polls) is increasing precipitously and that for the PC&amp;rsquo;s is decreasing almost as sharply. That seems to be a real phenomenon. We don&amp;rsquo;t know whether that crosses over to the general population but it doesn&amp;rsquo;t seem unreasonable to think it does. Exactly how is another question, and I make no assertions there.&lt;/p&gt;

&lt;center&gt;&lt;img style="width:400px" srcset="/files/2018/05/CBC-poll-tracker-trend.png 2x" /&gt;&lt;/center&gt;

&lt;p&gt;tl;dr If you don&amp;rsquo;t like the idea of &lt;a href="https://www.theguardian.com/world/2018/apr/30/doug-ford-ontario-conservative-trump-comparison-canada"&gt;Doug Ford&lt;/a&gt; in power, there is no reason to panic based on sites like the Ontario Poll Tracker. Spend your time doing something more productive, like convincing your friends and relatives to vote for someone who is not Conservative.&lt;/p&gt;</description></item>
  <item>
   <title>metricsgraphics movements</title>
   <link>https://wlach.github.io/blog/2018/04/metricsgraphics-movements/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2018-04-metricsgraphics-movements</guid>
   <pubDate>Mon, 30 Apr 2018 15:20:32 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;Just wanted to give a quick update on some things that have been happening with the &lt;a href="https://www.metricsgraphics.org"&gt;metrics graphics&lt;/a&gt; since I stepped up to help with maintainership a few months ago:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="https://github.com/hamilton"&gt;Hamilton&lt;/a&gt;&amp;rsquo;s back as co-maintainer! This has been especially helpful  as he understands much of the historical context of metricsgraphics better than I do.&lt;/li&gt;
 &lt;li&gt;We&amp;rsquo;ve merged in a large number of small fixes and improvements into the codebase,  thanks to myself and a number of other contributors. Special shout-out to Thomas  Champagne, who has contributed a &lt;a href="https://github.com/metricsgraphics/metrics-graphics/commits?author=thomaschampagne"&gt;large number of nifty new features&lt;/a&gt;.&lt;/li&gt;
 &lt;li&gt;We moved the project from Mozilla to its &lt;a href="https://github.com/metricsgraphics/"&gt;own organization&lt;/a&gt; on  github. This feels like a much better way forward for a project which is supposed to be useful  far outside the bounds of Mozilla, and hopeful makes contributors feel more like the first-class  citizens of the project that they actually are.&lt;/li&gt;
 &lt;li&gt;We have a GSOC intern! As part of the Mozilla GSOC, Yunhao Zheng is going to be working on  adding rich brushing/zooming support to Metrics Graphics, which should be quite useful for  visualizing complex data in projects like &lt;a href="https://wiki.mozilla.org/EngineeringProductivity/Projects/Perfherder"&gt;Perfherder&lt;/a&gt; (&lt;a href="https://docs.google.com/document/d/1_KIOJtemqlCBktDdfdjDuS4XhICeaKO3QhNRwbnnf-g/"&gt;Project outline&lt;/a&gt;, &lt;a href="https://summerofcode.withgoogle.com/projects/#5422620527296512"&gt;Yunhao&amp;rsquo;s proposal&lt;/a&gt;)&lt;/li&gt;&lt;/ul&gt;</description></item>
  <item>
   <title>Mission Control update</title>
   <link>https://wlach.github.io/blog/2018/04/mission-control-update/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2018-04-mission-control-update</guid>
   <pubDate>Fri, 06 Apr 2018 18:46:43 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;Yep, still working on this project. We&amp;rsquo;ve shifted gears somewhat from trying to identify problems in a time series of error aggregates to tracking somewhat longer term trends release over release, to fill the needs of the release management team at Mozilla. It&amp;rsquo;s been a good change, I think. A bit of a tighter focus.&lt;/p&gt;

&lt;p&gt;The main motivator for this work is that the ADI (active daily install) numbers that crash stats used to provide as input to a similar service, &lt;a href="https://arewestableyet.com"&gt;AreWeStableYet&lt;/a&gt; (link requires Mozilla credentials), are going away and we need some kind of replacement. I&amp;rsquo;ve been learning about this older system worked (this &lt;a href="https://home.kairo.at/blog/2014-04/how_effective_is_the_stability_program"&gt;blog post&lt;/a&gt; from KaiRo was helpful) and trying to develop a replacement which reproduces some of its useful characteristics while also taking advantage of some of the new features that are provided by the &lt;a href="https://docs.telemetry.mozilla.org/datasets/streaming/error_aggregates/reference.html"&gt;error_aggregates&lt;/a&gt; dataset and the mission control user interface.&lt;/p&gt;

&lt;p&gt;Some preliminary screenshots of what I&amp;rsquo;ve been able to come up with:&lt;/p&gt;

&lt;center&gt;&lt;img style="width:400px" srcset="/files/2018/04/missioncontrol-main-view.png 2x" /&gt; &lt;img style="width:400px" srcset="/files/2018/04/missioncontrol-windows-release.png 2x" /&gt;&lt;/center&gt;

&lt;p&gt;One of the key things to keep in mind with this dashboard is that by default it shows an &lt;em&gt;adjusted&lt;/em&gt; set of rates (defined as total number of events divided by total usage khours), which means we compare the latest release to the previous one within the same time interval.&lt;/p&gt;

&lt;p&gt;So if, say, the latest release is &amp;ldquo;59&amp;rdquo; and it&amp;rsquo;s been out for two weeks, we will compare it against the previous release (&amp;ldquo;58&amp;rdquo;) in its first two weeks. As I&amp;rsquo;ve said here before, things are &lt;a href="/blog/2017/10/better-or-worse-by-what-measure"&gt;always crashier when they first go out&lt;/a&gt;, and comparing a new release to one that has been out in the field for some time is not a fair comparison at all.&lt;/p&gt;

&lt;p&gt;This adjusted view of things is still not apples-to-apples: the causality of crashes and errors is so complex that there will always be differences between releases which are beyond our control or even understanding. Many crash reports, for example, have nothing to do with our product but with third party software and web sites beyond our control. That said, I feel like this adjusted rate is still good enough to tell us (broadly speaking) (1) whether our latest release / beta / nightly is ok (i.e. there is no major showstopper issue) and (2) whether our overall error rate is going up or down over several versions (if there is a continual increase in our crash rate, it might point to a problem in our release/qa process).&lt;/p&gt;

&lt;p&gt;Interestingly, the first things that we&amp;rsquo;ve found with this system are not real problems with the product but data collection issues:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1447161"&gt;we don&amp;rsquo;t seem to be collecting counts of gmplugin crashes on Windows anymore via telemetry&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1413172#c8"&gt;the number of content_shutdown_crashes is greater than the number of content_crashes, even though the former is a superset of the latter&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Data issues aside, the indications are that there&amp;rsquo;s been a steady increase in the quality of Firefox over the last few releases based on the main user facing error metric we&amp;rsquo;ve cared about in the past (main crashes), so that&amp;rsquo;s good. :)&lt;/p&gt;

&lt;p&gt;If you want to play with the system yourself, the &lt;a href="https://data-missioncontrol.dev.mozaws.net/"&gt;development instance&lt;/a&gt; is still up. We will probably look at making this thing &amp;ldquo;official&amp;rdquo; next quarter.&lt;/p&gt;</description></item>
  <item>
   <title>Derived versus direct</title>
   <link>https://wlach.github.io/blog/2018/02/derived-versus-direct/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2018-02-derived-versus-direct</guid>
   <pubDate>Mon, 12 Feb 2018 21:06:40 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;To attempt to make complex phenomena more understandable, we often use derived measures when representing Telemetry data at Mozilla. For error rates for example, we often measure things in terms of &amp;ldquo;X per khours of use&amp;rdquo; (where X might be &amp;ldquo;main crashes&amp;rdquo;, &amp;ldquo;appearance of the slow script dialogue&amp;rdquo;). I.e. instead of showing a raw &lt;em&gt;count&lt;/em&gt; of errors we show a rate. Normally this is a good thing: it allows the user to easily compare two things which might have different raw numbers for whatever reason but where you&amp;rsquo;d normally expect the ratio to be similar. For example, we see that although the &lt;em&gt;uptake&lt;/em&gt; of the newly-released Firefox 58.0.2 is a bit slower than 58.0.1, the overall crash rate (as sampled every 5 minutes) is more or less the same after about a day has rolled around:&lt;/p&gt;

&lt;p&gt;&lt;img srcset="/files/2018/02/main_crashes_normalized.png" /&gt;&lt;/p&gt;

&lt;p&gt;On the other hand, looking at raw counts doesn&amp;rsquo;t really give you much of a hint on how to interpret the results. Depending on the scale of the graph, the actual rates could actually resolve to being vastly different:&lt;/p&gt;

&lt;p&gt;&lt;img srcset="/files/2018/02/main_crashes_raw.png" /&gt;&lt;/p&gt;

&lt;p&gt;Ok, so this simple tool (using a ratio) is useful. Yay! Unfortunately, there is one case where using this technique can lead to a very deceptive visualization: when the number of samples is really small, a few outliers can give a really false impression of what&amp;rsquo;s really happening. Take this graph of what the crash rate looked like &lt;em&gt;just after&lt;/em&gt; Firefox 58.0 was released:&lt;/p&gt;

&lt;p&gt;&lt;img srcset="/files/2018/02/relative_small_crash_counts.png" /&gt;&lt;/p&gt;

&lt;p&gt;10 to 100 errors per 1000 hours, say it isn&amp;rsquo;t so? But wait, how many errors do we have absolutely? Hovering over a representative point in the graph with the normalization (use of a ratio) turned off:&lt;/p&gt;

&lt;p&gt;&lt;img srcset="/files/2018/02/absolute_small_crash_counts.png" /&gt;&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re really only talking about something between 1 to 40 crashes events over a relatively small number of usage hours. This is clearly so little data that we can&amp;rsquo;t (and shouldn&amp;rsquo;t) draw any kind of conclusion whatsoever.&lt;/p&gt;

&lt;p&gt;Ok, so that&amp;rsquo;s just science 101: don&amp;rsquo;t jump to conclusions based on small, vastly unrepresentative samples. Unfortunately due to human psychology people tend to assume that charts like this are authoritative and represent something real, absent an explanation otherwise &amp;mdash; and the use of a ratio obscured the one fact (extreme lack of data) that would have given the user a hint on how to correctly interpret the results. Something to keep in mind as we build our tools.&lt;/p&gt;</description></item>
  <item>
   <title>Giving and receiving help at Mozilla</title>
   <link>https://wlach.github.io/blog/2018/01/giving-and-receiving-help-at-mozilla/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2018-01-giving-and-receiving-help-at-mozilla</guid>
   <pubDate>Wed, 17 Jan 2018 18:49:34 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;This is going to sound corny, but helping people really is one of my favorite things at Mozilla, even with &lt;a href="https://mozilla.github.io/mozregression/"&gt;projects I have mostly moved on from&lt;/a&gt;. As someone who primarily works on internal tools, I love hearing about bugs in the software I maintain or questions on how to use it best.&lt;/p&gt;

&lt;p&gt;Given this, you might think that getting in touch with me via irc or slack is the fastest and best way to get your issue addressed. We certainly have a culture of using these instant-messaging applications at Mozilla for everything and anything. Unfortunately, I have found that being &amp;ldquo;always on&amp;rdquo; to respond to everything hasn&amp;rsquo;t been positive for either my productivity or mental health. My personal situation aside, getting pinged on irc while I&amp;rsquo;m out of the office often results in stuff getting lost &amp;mdash; the person who asked me the question is often gone by the time I return and am able to answer.&lt;/p&gt;

&lt;p&gt;With that in mind, here&amp;rsquo;s some notes on my preferred conversation style when making initial contact about an issue:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;Please don&amp;rsquo;t send context-free pings on irc. It has been &lt;a href="http://edunham.net/2017/10/05/saying_ping.html"&gt;explained elsewhere&lt;/a&gt;  why this doesn&amp;rsquo;t work that well, so I won&amp;rsquo;t repeat the argument here.&lt;/li&gt;
 &lt;li&gt;If you are at all suspicious that your issue might be a bug in some software  maintain, just &lt;a href="https://bugzilla.mozilla.org/enter_bug.cgi"&gt;file a bug&lt;/a&gt; and needinfo me. That puts us right on the path to documenting  the problem and getting to a resolution &amp;mdash; even if something turns out to not  be a bug, if you&amp;rsquo;re seeing an unexpected error it points to a usability issue.&lt;/li&gt;
 &lt;li&gt;For everything else, email is best. I do check it quite frequently between  bursts of work (i.e. many times a day). I promise I won&amp;rsquo;t leave you hanging  for days on end as long as I&amp;rsquo;m not on vacation.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;These aren&amp;rsquo;t ironclad rules. If your question pertains to a project I&amp;rsquo;m &lt;em&gt;actively&lt;/em&gt; working on, it might make sense to ping me on irc first (preferably on a channel where other people are around who might also be able to help). If it&amp;rsquo;s an actual &lt;em&gt;emergency&lt;/em&gt;, then of course talk to me on irc straight away (or even call me on my phone) &amp;mdash; if I don&amp;rsquo;t respond, then fall back to filing bug or sending email. Use common sense.&lt;/p&gt;

&lt;p&gt;One of my new years resolutions is to also apply these rules to my communications with others at Mozilla as well, so if you see my violating it feel free to point me back at this post. Or just use this handy meme I created:&lt;/p&gt;

&lt;center&gt;&lt;img src="/files/2018/01/scale-of-asking.jpg" /&gt;&lt;/center&gt;</description></item>
  <item>
   <title>Quitting Twitter</title>
   <link>https://wlach.github.io/blog/2018/01/quitting-twitter/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2018-01-quitting-twitter</guid>
   <pubDate>Sat, 13 Jan 2018 20:09:44 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;(this post probably won&amp;rsquo;t make much sense unless you have been a user of Twitter since/during 2016&amp;ndash;2018)&lt;/p&gt;

&lt;p&gt;Decided to kill my Twitter account today. Probably most people won&amp;rsquo;t miss my presence there as I haven&amp;rsquo;t actively posting on the site in quite some time, but I did check it quite frequently. I suspect it consumed (and I&amp;rsquo;m ashamed to admit this), at least an hour of my day on average. Twitter when I wake up, Twitter on the streetcar, Twitter every god damn empty moment.&lt;/p&gt;

&lt;p&gt;Of all the large social media propertes, I find Twitter by far leads in the amount of negative sentiment it displays, especially in the last year or so. Donald Trump, #GamerGate, Brexit, Nazis, the list goes on. That&amp;rsquo;s probably why it is so uniquely capable of grabbing my attention in particular, there&amp;rsquo;s some dark part of me which gravitates towards exactly this type of content.&lt;/p&gt;

&lt;p&gt;Now, due to the filter bubble effect, 99% of what &lt;em&gt;I&lt;/em&gt; see on that site is a negative &lt;em&gt;response&lt;/em&gt; to those things, but so what? The overwhelming of effect of my interaction with the site is simply to make me more more fearful and angry, nothing more. The very format of Twitter (brief bursts of performative text) doesn&amp;rsquo;t encourage any kind of intelligent or compassionate reaction to what you are seeing or reading. It would be one thing if the negative things I saw on Twitter motivated me to take a positive action of &lt;em&gt;any&lt;/em&gt; kind, but if the net effect of hearing bad news is just to make you feel bad, then what&amp;rsquo;s the point?&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m not much of a fan of Facebook either, but it has (less often than I would like, but sometimes) been a platform for facilitating worthwhile social and cultural interactions both on the site and off. I could probably count the number of similar exchanges I have had on Twitter on one hand &amp;mdash; overwhelmingly my experience with twitter is that engagement (posting) leads only to a regrettable salvo of petty bickering and &lt;a href="http://tirania.org/blog/archive/2011/Feb-17.html"&gt;well actuallys&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So, let&amp;rsquo;s see what life on the other side of this site is like. The demons which pulled me to Twitter every day are no doubt still there&amp;hellip; but hopefully they&amp;rsquo;ll be easier to handle without the influence of a technology which amplifies their effect.&lt;/p&gt;

&lt;p&gt;Further reading:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="http://www.macdrifter.com/2017/11/fuck-twitter.html"&gt;Fuck Twitter&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;&lt;a href="http://www.businessinsider.com/jaron-lanier-interview-on-silicon-valley-culture-metoo-backlash-ai-and-the-future-2017-12"&gt;Jaron Lanier&lt;/a&gt; on negative sentiment in social media (among other interesting topics)&lt;/li&gt;&lt;/ul&gt;</description></item>
  <item>
   <title>Maintaining metricsgraphics</title>
   <link>https://wlach.github.io/blog/2017/12/maintaining-metricsgraphics/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2017-12-maintaining-metricsgraphics</guid>
   <pubDate>Wed, 06 Dec 2017 22:16:23 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;Just a quick announcement that I&amp;rsquo;ve taken it upon myself to assume some maintership duties of the popular &lt;a href="https://github.com/mozilla/metrics-graphics"&gt;MetricsGraphics&lt;/a&gt; library and have released a &lt;a href="https://www.npmjs.com/package/metrics-graphics"&gt;new version&lt;/a&gt; with some bug fixes (2.12.0). We use this package pretty extensively at Mozilla for visualizing telemetry and other time series data, but its original authors (Hamilton Ulmer and Ali Almossawi) have mostly moved on to other things so there was a bit of a gap in getting fixes and improvements in that I hope to fill.&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t yet claim to be an expert in this library (which is quite rich and complex), but I&amp;rsquo;m sure I&amp;rsquo;ll learn more as I go along. At least initially, I expect that the changes I make will be small and primarily targetted to filling the needs of the &lt;a href="https://github.com/mozilla/missioncontrol"&gt;Mission Control&lt;/a&gt; project.&lt;/p&gt;

&lt;p&gt;Note that this emphatically does &lt;em&gt;not&lt;/em&gt; mean I am promising to respond to every issue/question/pull request made against the project. Like my work with mozregression and perfherder, my maintenance work is being done on a best-effort basis to support Mozilla and the larger open source community. I&amp;rsquo;ll help people out where I can, but there are only so many working hours in a day and I need to spend most of those pushing my team&amp;rsquo;s immediate projects and deliverables forward! In particular, when it comes to getting pull requests merged, small, self-contained and logical changes with good commit messages will take priority.&lt;/p&gt;</description></item>
  <item>
   <title>Better or worse: by what measure?</title>
   <link>https://wlach.github.io/blog/2017/10/better-or-worse-by-what-measure/?utm_source=all&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:https-wlach-github-io:-blog-2017-10-better-or-worse-by-what-measure</guid>
   <pubDate>Thu, 26 Oct 2017 20:58:20 UT</pubDate>
   <author>William Lachance</author>
   <description>
&lt;p&gt;Ok, after a series of posts extolling the virtues of my current project, it&amp;rsquo;s time to take a more critical look at some of its current limitations, and what we might do about them. In my &lt;a href="/blog/2017/10/mission-control/"&gt;introductory post&lt;/a&gt;, I talked about how Mission Control can let us know how &amp;ldquo;crashy&amp;rdquo; a new release is, within a short interval of it being released. I also alluded to the fact that things appear considerably worse when something first goes out, though I didn&amp;rsquo;t go into a lot of detail about how and why that happens.&lt;/p&gt;

&lt;p&gt;It just so happens that a new point release (56.0.2) just went out, so it&amp;rsquo;s a perfect opportunity to revisit this issue. Let&amp;rsquo;s take a look at what the graphs are saying (each of the images is also a link to the dashboard where they were generated):&lt;/p&gt;

&lt;p&gt;&lt;a href="https://data-missioncontrol.dev.mozaws.net/#/release/windows/content_crashes?timeInterval=172740&amp;amp;percentile=99&amp;amp;normalized=1&amp;amp;disabledVersions=&amp;amp;versionGrouping=version&amp;amp;startTime=1508990400"&gt;&lt;img srcset="/files/2017/10/missioncontrol_windows_content_crashes_56.0.2.png 2x" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ZOMG! It looks like 56.0.2 is off the charts relative to the two previous releases (56.0 and 56.0.1). Is it time to sound the alarm? Mission control abort? Well, let&amp;rsquo;s see what happens the last time we rolled something new out, say 56.0.1:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://data-missioncontrol.dev.mozaws.net/#/release/windows/content_crashes?timeInterval=345540&amp;amp;percentile=99&amp;amp;normalized=1&amp;amp;disabledVersions=&amp;amp;versionGrouping=version&amp;amp;startTime=1507435200"&gt;&lt;img srcset="/files/2017/10/missioncontrol_windows_content_crashes_56.0.1.png 2x" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We see the exact same pattern. Hmm. How about 56.0?&lt;/p&gt;

&lt;p&gt;&lt;a href="https://data-missioncontrol.dev.mozaws.net/#/release/windows/content_crashes?timeInterval=431940&amp;amp;percentile=99&amp;amp;normalized=1&amp;amp;disabledVersions=&amp;amp;versionGrouping=version&amp;amp;startTime=1506398400"&gt;&lt;img srcset="/files/2017/10/missioncontrol_windows_content_crashes_56.png 2x" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Yep, same pattern here too (actually slightly worse).&lt;/p&gt;

&lt;p&gt;What could be going on? Let&amp;rsquo;s start by reviewing what these time series graphs are based on. Each point on the graph represents the number of crashes reported by telemetry &amp;ldquo;main&amp;rdquo; pings corresponding to that channel/version/platform within a five minute interval, divided by the number of usage hours (how long users have had Firefox open) also reported in that interval. A main ping is submitted under &lt;a href="https://firefox-source-docs.mozilla.org/toolkit/components/telemetry/telemetry/data/main-ping.html"&gt;a few circumstances&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;The user shuts down Firefox&lt;/li&gt;
 &lt;li&gt;It’s been about 24 hours since the last time we sent a main ping.&lt;/li&gt;
 &lt;li&gt;The user starts Firefox after Firefox failed to start properly&lt;/li&gt;
 &lt;li&gt;The user changes something about Firefox’s environment (adds an addon, flips a user preference)&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;A high crash rate either means a larger number of crashes over the same number of usage hours, or a lower number of usage hours over the same number of crashes. There are several likely explanations for why we might see this type of crashy behaviour immediately after a new release:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;A Firefox update is applied after the user restarts their browser for any  reason, including their browser crash. Thus a user whose browser crashes a  lot (for any reason), is more prone to update to the latest version sooner  than a user that doesn’t crash as much.&lt;/li&gt;
 &lt;li&gt;Inherently, any crash data submitted to telemetry after a new version is  released will have a low number of usage hours attached, because the  client would not have had a chance to use it much (because it&amp;rsquo;s so new).&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Assuming that we&amp;rsquo;re reasonably satisfied with the above explanation, there&amp;rsquo;s a few things we could try to do to correct for this situation when implementing an &amp;ldquo;alerting&amp;rdquo; system for mission control (the next item on my todo list for this project):&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Set &amp;ldquo;error&amp;rdquo; thresholds for each crash measure sufficiently high that  we don&amp;rsquo;t consider these high initial values an error (i.e. only alert  if there is are 500 crashes per 1k hours).&lt;/li&gt;
 &lt;li&gt;Only trigger an error threshold when some kind of minimum quantity of  usage hours has been observed (this has the disadvantage of potentially  obscuring a serious problem until a large percentage of the user population  is affected by it).&lt;/li&gt;
 &lt;li&gt;Come up with some expected range of what we expect a value to be for  when a new version of firefox is first released and ratchet  that down as time goes on (according to some kind of model of our previous expectations).&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;The initial specification for this project called for just using raw thresholds for these measures (discounting usage hours), but I&amp;rsquo;m becoming increasingly convinced that won&amp;rsquo;t cut it. I&amp;rsquo;m not a quality control expert, but 500 crashes for 1k hours of use sounds completely unacceptable if we&amp;rsquo;re measuring things at all accurately (which I believe we are given a sufficient period of time). At the same time, generating 20&amp;ndash;30 “alerts” every time a new release went out wouldn’t particularly helpful either. Once again, we’re going to have to do this the hard way&amp;hellip;&lt;/p&gt;

&lt;p&gt;&amp;mdash;&lt;/p&gt;

&lt;p&gt;If this sounds interesting and you have some react/d3/data visualization skills (or would like to gain some), &lt;a href="/blog/2017/10/mission-control-ready-for-contributions/"&gt;learn about contributing to mission control&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Shout out to &lt;a href="https://chuttenblog.wordpress.com/"&gt;chutten&lt;/a&gt; for reviewing this post and providing feedback and additions.&lt;/p&gt;</description></item></channel></rss>