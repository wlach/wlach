<?xml version="1.0" encoding="utf-8"?> 
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
 <title type="text">Untitled Site: Posts tagged 'Mozilla'</title>
 <link rel="self" href="http://www.example.com/feeds/Mozilla.atom.xml" />
 <link href="http://www.example.com/tags/Mozilla.html" />
 <id>urn:http-www-example-com:-tags-Mozilla-html</id>
 <updated>2018-02-12T21:06:40Z</updated>
 <entry>
  <title type="text">Derived versus direct</title>
  <link rel="alternate" href="http://www.example.com/2018/02/derived-versus-direct.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2018-02-derived-versus-direct-html</id>
  <published>2018-02-12T21:06:40Z</published>
  <updated>2018-02-12T21:06:40Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;To attempt to make complex phenomena more understandable, we often use derived measures when representing Telemetry data at Mozilla. For error rates for example, we often measure things in terms of &amp;ldquo;X per khours of use&amp;rdquo; (where X might be &amp;ldquo;main crashes&amp;rdquo;, &amp;ldquo;appearance of the slow script dialogue&amp;rdquo;). I.e. instead of showing a raw &lt;em&gt;count&lt;/em&gt; of errors we show a rate. Normally this is a good thing: it allows the user to easily compare two things which might have different raw numbers for whatever reason but where you&amp;rsquo;d normally expect the ratio to be similar. For example, we see that although the &lt;em&gt;uptake&lt;/em&gt; of the newly-released Firefox 58.0.2 is a bit slower than 58.0.1, the overall crash rate (as sampled every 5 minutes) is more or less the same after about a day has rolled around:&lt;/p&gt;

&lt;p&gt;&lt;img srcset="/files/2018/02/main_crashes_normalized.png" /&gt;&lt;/p&gt;

&lt;p&gt;On the other hand, looking at raw counts doesn&amp;rsquo;t really give you much of a hint on how to interpret the results. Depending on the scale of the graph, the actual rates could actually resolve to being vastly different:&lt;/p&gt;

&lt;p&gt;&lt;img srcset="/files/2018/02/main_crashes_raw.png" /&gt;&lt;/p&gt;

&lt;p&gt;Ok, so this simple tool (using a ratio) is useful. Yay! Unfortunately, there is one case where using this technique can lead to a very deceptive visualization: when the number of samples is really small, a few outliers can give a really false impression of what&amp;rsquo;s really happening. Take this graph of what the crash rate looked like &lt;em&gt;just after&lt;/em&gt; Firefox 58.0 was released:&lt;/p&gt;

&lt;p&gt;&lt;img srcset="/files/2018/02/relative_small_crash_counts.png" /&gt;&lt;/p&gt;

&lt;p&gt;10 to 100 errors per 1000 hours, say it isn&amp;rsquo;t so? But wait, how many errors do we have absolutely? Hovering over a representative point in the graph with the normalization (use of a ratio) turned off:&lt;/p&gt;

&lt;p&gt;&lt;img srcset="/files/2018/02/absolute_small_crash_counts.png" /&gt;&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re really only talking about something between 1 to 40 crashes events over a relatively small number of usage hours. This is clearly so little data that we can&amp;rsquo;t (and shouldn&amp;rsquo;t) draw any kind of conclusion whatsoever.&lt;/p&gt;

&lt;p&gt;Ok, so that&amp;rsquo;s just science 101: don&amp;rsquo;t jump to conclusions based on small, vastly unrepresentative samples. Unfortunately due to human psychology people tend to assume that charts like this are authoritative and represent something real, absent an explanation otherwise &amp;mdash; and the use of a ratio obscured the one fact (extreme lack of data) that would have given the user a hint on how to correctly interpret the results. Something to keep in mind as we build our tools.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Giving and receiving help at Mozilla</title>
  <link rel="alternate" href="http://www.example.com/2018/01/giving-and-receiving-help-at-mozilla.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2018-01-giving-and-receiving-help-at-mozilla-html</id>
  <published>2018-01-17T18:49:34Z</published>
  <updated>2018-01-17T18:49:34Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;This is going to sound corny, but helping people really is one of my favorite things at Mozilla, even with &lt;a href="https://mozilla.github.io/mozregression/"&gt;projects I have mostly moved on from&lt;/a&gt;. As someone who primarily works on internal tools, I love hearing about bugs in the software I maintain or questions on how to use it best.&lt;/p&gt;

&lt;p&gt;Given this, you might think that getting in touch with me via irc or slack is the fastest and best way to get your issue addressed. We certainly have a culture of using these instant-messaging applications at Mozilla for everything and anything. Unfortunately, I have found that being &amp;ldquo;always on&amp;rdquo; to respond to everything hasn&amp;rsquo;t been positive for either my productivity or mental health. My personal situation aside, getting pinged on irc while I&amp;rsquo;m out of the office often results in stuff getting lost &amp;mdash; the person who asked me the question is often gone by the time I return and am able to answer.&lt;/p&gt;

&lt;p&gt;With that in mind, here&amp;rsquo;s some notes on my preferred conversation style when making initial contact about an issue:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;Please don&amp;rsquo;t send context-free pings on irc. It has been &lt;a href="http://edunham.net/2017/10/05/saying_ping.html"&gt;explained elsewhere&lt;/a&gt;  why this doesn&amp;rsquo;t work that well, so I won&amp;rsquo;t repeat the argument here.&lt;/li&gt;
 &lt;li&gt;If you are at all suspicious that your issue might be a bug in some software  maintain, just &lt;a href="https://bugzilla.mozilla.org/enter_bug.cgi"&gt;file a bug&lt;/a&gt; and needinfo me. That puts us right on the path to documenting  the problem and getting to a resolution &amp;mdash; even if something turns out to not  be a bug, if you&amp;rsquo;re seeing an unexpected error it points to a usability issue.&lt;/li&gt;
 &lt;li&gt;For everything else, email is best. I do check it quite frequently between  bursts of work (i.e. many times a day). I promise I won&amp;rsquo;t leave you hanging  for days on end as long as I&amp;rsquo;m not on vacation.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;These aren&amp;rsquo;t ironclad rules. If your question pertains to a project I&amp;rsquo;m &lt;em&gt;actively&lt;/em&gt; working on, it might make sense to ping me on irc first (preferably on a channel where other people are around who might also be able to help). If it&amp;rsquo;s an actual &lt;em&gt;emergency&lt;/em&gt;, then of course talk to me on irc straight away (or even call me on my phone) &amp;mdash; if I don&amp;rsquo;t respond, then fall back to filing bug or sending email. Use common sense.&lt;/p&gt;

&lt;p&gt;One of my new years resolutions is to also apply these rules to my communications with others at Mozilla as well, so if you see my violating it feel free to point me back at this post. Or just use this handy meme I created:&lt;/p&gt;

&lt;center&gt;&lt;img src="/files/2018/01/scale-of-asking.jpg" /&gt;&lt;/center&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Maintaining metricsgraphics</title>
  <link rel="alternate" href="http://www.example.com/2017/12/maintaining-metricsgraphics.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2017-12-maintaining-metricsgraphics-html</id>
  <published>2017-12-06T22:16:23Z</published>
  <updated>2017-12-06T22:16:23Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Just a quick announcement that I&amp;rsquo;ve taken it upon myself to assume some maintership duties of the popular &lt;a href="https://github.com/mozilla/metrics-graphics"&gt;MetricsGraphics&lt;/a&gt; library and have released a &lt;a href="https://www.npmjs.com/package/metrics-graphics"&gt;new version&lt;/a&gt; with some bug fixes (2.12.0). We use this package pretty extensively at Mozilla for visualizing telemetry and other time series data, but its original authors (Hamilton Ulmer and Ali Almossawi) have mostly moved on to other things so there was a bit of a gap in getting fixes and improvements in that I hope to fill.&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t yet claim to be an expert in this library (which is quite rich and complex), but I&amp;rsquo;m sure I&amp;rsquo;ll learn more as I go along. At least initially, I expect that the changes I make will be small and primarily targetted to filling the needs of the &lt;a href="https://github.com/mozilla/missioncontrol"&gt;Mission Control&lt;/a&gt; project.&lt;/p&gt;

&lt;p&gt;Note that this emphatically does &lt;em&gt;not&lt;/em&gt; mean I am promising to respond to every issue/question/pull request made against the project. Like my work with mozregression and perfherder, my maintenance work is being done on a best-effort basis to support Mozilla and the larger open source community. I&amp;rsquo;ll help people out where I can, but there are only so many working hours in a day and I need to spend most of those pushing my team&amp;rsquo;s immediate projects and deliverables forward! In particular, when it comes to getting pull requests merged, small, self-contained and logical changes with good commit messages will take priority.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Better or worse: by what measure?</title>
  <link rel="alternate" href="http://www.example.com/2017/10/better-or-worse-by-what-measure.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2017-10-better-or-worse-by-what-measure-html</id>
  <published>2017-10-26T20:58:20Z</published>
  <updated>2017-10-26T20:58:20Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Ok, after a series of posts extolling the virtues of my current project, it&amp;rsquo;s time to take a more critical look at some of its current limitations, and what we might do about them. In my &lt;a href="/blog/2017/10/mission-control/"&gt;introductory post&lt;/a&gt;, I talked about how Mission Control can let us know how &amp;ldquo;crashy&amp;rdquo; a new release is, within a short interval of it being released. I also alluded to the fact that things appear considerably worse when something first goes out, though I didn&amp;rsquo;t go into a lot of detail about how and why that happens.&lt;/p&gt;

&lt;p&gt;It just so happens that a new point release (56.0.2) just went out, so it&amp;rsquo;s a perfect opportunity to revisit this issue. Let&amp;rsquo;s take a look at what the graphs are saying (each of the images is also a link to the dashboard where they were generated):&lt;/p&gt;

&lt;p&gt;&lt;a href="https://data-missioncontrol.dev.mozaws.net/#/release/windows/content_crashes?timeInterval=172740&amp;amp;percentile=99&amp;amp;normalized=1&amp;amp;disabledVersions=&amp;amp;versionGrouping=version&amp;amp;startTime=1508990400"&gt;&lt;img srcset="/files/2017/10/missioncontrol_windows_content_crashes_56.0.2.png 2x" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ZOMG! It looks like 56.0.2 is off the charts relative to the two previous releases (56.0 and 56.0.1). Is it time to sound the alarm? Mission control abort? Well, let&amp;rsquo;s see what happens the last time we rolled something new out, say 56.0.1:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://data-missioncontrol.dev.mozaws.net/#/release/windows/content_crashes?timeInterval=345540&amp;amp;percentile=99&amp;amp;normalized=1&amp;amp;disabledVersions=&amp;amp;versionGrouping=version&amp;amp;startTime=1507435200"&gt;&lt;img srcset="/files/2017/10/missioncontrol_windows_content_crashes_56.0.1.png 2x" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We see the exact same pattern. Hmm. How about 56.0?&lt;/p&gt;

&lt;p&gt;&lt;a href="https://data-missioncontrol.dev.mozaws.net/#/release/windows/content_crashes?timeInterval=431940&amp;amp;percentile=99&amp;amp;normalized=1&amp;amp;disabledVersions=&amp;amp;versionGrouping=version&amp;amp;startTime=1506398400"&gt;&lt;img srcset="/files/2017/10/missioncontrol_windows_content_crashes_56.png 2x" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Yep, same pattern here too (actually slightly worse).&lt;/p&gt;

&lt;p&gt;What could be going on? Let&amp;rsquo;s start by reviewing what these time series graphs are based on. Each point on the graph represents the number of crashes reported by telemetry &amp;ldquo;main&amp;rdquo; pings corresponding to that channel/version/platform within a five minute interval, divided by the number of usage hours (how long users have had Firefox open) also reported in that interval. A main ping is submitted under &lt;a href="https://firefox-source-docs.mozilla.org/toolkit/components/telemetry/telemetry/data/main-ping.html"&gt;a few circumstances&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;The user shuts down Firefox&lt;/li&gt;
 &lt;li&gt;It’s been about 24 hours since the last time we sent a main ping.&lt;/li&gt;
 &lt;li&gt;The user starts Firefox after Firefox failed to start properly&lt;/li&gt;
 &lt;li&gt;The user changes something about Firefox’s environment (adds an addon, flips a user preference)&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;A high crash rate either means a larger number of crashes over the same number of usage hours, or a lower number of usage hours over the same number of crashes. There are several likely explanations for why we might see this type of crashy behaviour immediately after a new release:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;A Firefox update is applied after the user restarts their browser for any  reason, including their browser crash. Thus a user whose browser crashes a  lot (for any reason), is more prone to update to the latest version sooner  than a user that doesn’t crash as much.&lt;/li&gt;
 &lt;li&gt;Inherently, any crash data submitted to telemetry after a new version is  released will have a low number of usage hours attached, because the  client would not have had a chance to use it much (because it&amp;rsquo;s so new).&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Assuming that we&amp;rsquo;re reasonably satisfied with the above explanation, there&amp;rsquo;s a few things we could try to do to correct for this situation when implementing an &amp;ldquo;alerting&amp;rdquo; system for mission control (the next item on my todo list for this project):&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Set &amp;ldquo;error&amp;rdquo; thresholds for each crash measure sufficiently high that  we don&amp;rsquo;t consider these high initial values an error (i.e. only alert  if there is are 500 crashes per 1k hours).&lt;/li&gt;
 &lt;li&gt;Only trigger an error threshold when some kind of minimum quantity of  usage hours has been observed (this has the disadvantage of potentially  obscuring a serious problem until a large percentage of the user population  is affected by it).&lt;/li&gt;
 &lt;li&gt;Come up with some expected range of what we expect a value to be for  when a new version of firefox is first released and ratchet  that down as time goes on (according to some kind of model of our previous expectations).&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;The initial specification for this project called for just using raw thresholds for these measures (discounting usage hours), but I&amp;rsquo;m becoming increasingly convinced that won&amp;rsquo;t cut it. I&amp;rsquo;m not a quality control expert, but 500 crashes for 1k hours of use sounds completely unacceptable if we&amp;rsquo;re measuring things at all accurately (which I believe we are given a sufficient period of time). At the same time, generating 20&amp;ndash;30 “alerts” every time a new release went out wouldn’t particularly helpful either. Once again, we’re going to have to do this the hard way&amp;hellip;&lt;/p&gt;

&lt;p&gt;&amp;mdash;&lt;/p&gt;

&lt;p&gt;If this sounds interesting and you have some react/d3/data visualization skills (or would like to gain some), &lt;a href="/blog/2017/10/mission-control-ready-for-contributions/"&gt;learn about contributing to mission control&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Shout out to &lt;a href="https://chuttenblog.wordpress.com/"&gt;chutten&lt;/a&gt; for reviewing this post and providing feedback and additions.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Mission Control: Ready for contributions</title>
  <link rel="alternate" href="http://www.example.com/2017/10/mission-control-ready-for-contributions.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2017-10-mission-control-ready-for-contributions-html</id>
  <published>2017-10-20T18:33:19Z</published>
  <updated>2017-10-20T18:33:19Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;One of the great design decisions that was made for &lt;a href="https://treeherder.mozilla.org"&gt;Treeherder&lt;/a&gt; was a strict seperation of the client and server portions of the codebase. While its backend was moderately complicated to get up and running (especially into a state that looked at all like what we were running in production), you could get its web frontend running (pointed against the production data) just by starting up a simple node.js server. This dramatically lowered the barrier to entry, for Mozilla employees and casual contributors alike.&lt;/p&gt;

&lt;p&gt;I knew right from the beginning that I wanted to take the same approach with &lt;a href="https://wlach.github.io/blog/2017/10/mission-control/"&gt;Mission Control&lt;/a&gt;. While the full source of the project is available, unfortunately it isn&amp;rsquo;t presently possible to bring up the full stack with real data, as that requires privileged access to the athena/parquet error aggregates table. But since the UI is self-contained, it&amp;rsquo;s quite easy to bring up a development environment that allows you to freely browse the cached data which is stored server-side (essentially: &lt;code&gt;git clone https://github.com/mozilla/missioncontrol.git &amp;amp;&amp;amp; yarn install &amp;amp;&amp;amp; yarn start&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;In my experience, the most interesting problems when it comes to projects like these center around the question of how to present extremely complex data in a way that is intuitive but not misleading. Probably 90% of that work happens in the frontend. In the past, I&amp;rsquo;ve had pretty good luck finding contributors for my projects (especially &lt;a href="/tags/Perfherder.html"&gt;Perfherder&lt;/a&gt;) by doing call-outs on this blog. So let it be known: If Mission Control sounds like an interesting project and you know &lt;a href="https://reactjs.org/"&gt;React&lt;/a&gt;/&lt;a href="http://redux.js.org/"&gt;Redux&lt;/a&gt;/&lt;a href="https://d3js.org/"&gt;D3&lt;/a&gt;/&lt;a href="https://www.metricsgraphicsjs.org/"&gt;MetricsGraphics&lt;/a&gt; (or want to learn), let&amp;rsquo;s work together!&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve created some &lt;a href="https://github.com/mozilla/missioncontrol/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;good first bugs&lt;/a&gt; to tackle in the github issue tracker. From there, I have a galaxy of other work in mind to improve and enhance the usefulness of this project. Please get in touch with me (wlach) on &lt;a href="https://wiki.mozilla.org/IRC"&gt;irc.mozilla.org&lt;/a&gt; #missioncontrol if you want to discuss further.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Mission Control</title>
  <link rel="alternate" href="http://www.example.com/2017/10/mission-control.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2017-10-mission-control-html</id>
  <published>2017-10-06T19:05:37Z</published>
  <updated>2017-10-06T19:05:37Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Time for an overdue post on the mission control project that I&amp;rsquo;ve been working on for the past few quarters, since I transitioned to the data platform team.&lt;/p&gt;

&lt;p&gt;One of the gaps in our data story when it comes to Firefox is being able to see how a new release is doing in the immediate hours after release. Tools like &lt;a href="https://crash-stats.mozilla.com/home/product/Firefox"&gt;crashstats&lt;/a&gt; and the &lt;a href="https://telemetry.mozilla.org/new-pipeline/evo.html"&gt;telemetry evolution dashboard&lt;/a&gt; are great, but it can take many hours (if not days) before you can reliably see that there is an issue in a metric that we care about (number of crashes, say). This is just too long &amp;mdash; such delays unnecessarily retard rolling out a release when it is safe (because there is a paranoia that there might be some lingering problem which we we&amp;rsquo;re waiting to see reported). And if, somehow, despite our abundant caution a problem &lt;em&gt;did&lt;/em&gt; slip through it would take us some time to recognize it and roll out a fix.&lt;/p&gt;

&lt;p&gt;Enter mission control. By hooking up a high-performance spark streaming job directly to our ingestion pipeline, we can now be able to detect within moments whether firefox is performing unacceptably within the field according to a particular measure.&lt;/p&gt;

&lt;p&gt;To make the volume of data manageable, we create a grouped data set with the raw count of the various measures (e.g. main crashes, content crashes, slow script dialog counts) along with each unique combination of dimensions (e.g. platform, channel, release).&lt;/p&gt;

&lt;p&gt;Of course, all this data is not so useful without a tool to visualize it, which is what I&amp;rsquo;ve been spending the majority of my time on. The idea is to be able to go from a top level description of what&amp;rsquo;s going on a particular channel (release for example) all the way down to a detailed view of how a measure has been performing over a time interval:&lt;/p&gt;

&lt;p&gt;&lt;img srcset="/files/2017/10/missioncontrol-ui.png 2x" /&gt;&lt;/p&gt;

&lt;p&gt;This particular screenshot shows the volume of content crashes (sampled every 5 minutes) over the last 48 hours on windows release. You&amp;rsquo;ll note that the later version (56.0) seems to be much crashier than earlier versions (55.0.3) which would seem to be a problem except that the populations are not directly comparable (since the profile of a user still on an older version of Firefox is rather different from that of one who has already upgraded). This is one of the still unsolved problems of this project: finding a reliable, automatable baseline of what an &amp;ldquo;acceptable result&amp;rdquo; for any particular measure might be.&lt;/p&gt;

&lt;p&gt;Even still, the tool can still be useful for exploring a bunch of data quickly and it has been progressing rapidly over the last few weeks. And like almost everything Mozilla does, both the &lt;a href="https://github.com/mozilla/missioncontrol/"&gt;source&lt;/a&gt; and &lt;a href="https://data-missioncontrol.dev.mozaws.net/"&gt;dashboard&lt;/a&gt; are open to the public. I&amp;rsquo;m planning on flagging some easier bugs for newer contributors to work on in the next couple weeks, but in the meantime if you&amp;rsquo;re interested in this project and want to get involved, feel free to look us up on irc.mozilla.org #missioncontrol (I&amp;rsquo;m there as &amp;lsquo;wlach&amp;rsquo;).&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Functional is the future</title>
  <link rel="alternate" href="http://www.example.com/2017/08/functional-is-the-future.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2017-08-functional-is-the-future-html</id>
  <published>2017-08-28T21:02:21Z</published>
  <updated>2017-08-28T21:02:21Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Just spent well over an hour tracking down a silly bug in my code. For the &lt;a href="https://github.com/mozilla/missioncontrol/"&gt;mission control&lt;/a&gt; project, I wrote this very simple API method that returns a cached data structure to our front end:&lt;/p&gt;

&lt;div class="brush: py"&gt;
 &lt;table class="sourcetable"&gt;
  &lt;tbody&gt;
   &lt;tr&gt;
    &lt;td class="linenos"&gt;
     &lt;div class="linenodiv"&gt;
      &lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;
    &lt;td class="code"&gt;
     &lt;div class="source"&gt;
      &lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;measure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;channel_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GET&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;channel&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;platform_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GET&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;platform&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;measure_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GET&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;measure&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;interval&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GET&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;interval&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="nb"&gt;all&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;channel_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;platform_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;measure_name&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;HttpResponseBadRequest&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"All of channel, platform, measure required"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cache&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;get_measure_cache_key&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;platform_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;channel_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;measure_name&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;HttpResponseNotFound&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Data not available for this measure combination"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;interval&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;min_time&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;timedelta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seconds&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;interval&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;HttpResponseBadRequest&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Interval must be specified in seconds (as an integer)"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# Return any build data in the interval&lt;/span&gt;
        &lt;span class="n"&gt;empty_buildids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;build_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;build_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="n"&gt;build_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;build_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;min_time&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;build_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
                &lt;span class="n"&gt;empty_buildids&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;build_id&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# don&amp;#39;t bother returning empty indexed data&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;empty_buildid&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;empty_buildids&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;del&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;empty_buildid&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;JsonResponse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;measure_data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;As you can see, it takes 3 required parameters (channel, platform, and measure) and one optional one (interval), picks out the required data structure, filters it a bit, and returns it. This is &lt;em&gt;almost&lt;/em&gt; what we wanted for the frontend, unfortunately the time zone information isn&amp;rsquo;t quite what we want, since the strings that are returned don&amp;rsquo;t tell the frontend that they&amp;rsquo;re in UTC format &amp;mdash; they need a &amp;lsquo;Z&amp;rsquo; appended to them for that.&lt;/p&gt;

&lt;p&gt;After a bit of digging, I found out that Django&amp;rsquo;s &lt;a href="https://github.com/django/django/blob/afc06b56256f78ab832ff8066ac6f34b7443de22/django/core/serializers/json.py#L76"&gt;json serializer&lt;/a&gt; will only add the Z if the tzinfo structure is specified. So I figured out a simple pattern for adding that (using the dateutil library, which we are fortunately already using):&lt;/p&gt;

&lt;div class="brush: py"&gt;
 &lt;table class="sourcetable"&gt;
  &lt;tbody&gt;
   &lt;tr&gt;
    &lt;td class="linenos"&gt;
     &lt;div class="linenodiv"&gt;
      &lt;pre&gt;1
2&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;
    &lt;td class="code"&gt;
     &lt;div class="source"&gt;
      &lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;dateutil.tz&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;tzutc&lt;/span&gt;
&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fromtimestamp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mydatestamp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;timestamp&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;tz&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tzutc&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;I tested this quickly on the python console and it seemed to work great. But when I added the code to my function, the unit tests mysteriously failed. Can you see why?&lt;/p&gt;

&lt;div class="brush: py"&gt;
 &lt;table class="sourcetable"&gt;
  &lt;tbody&gt;
   &lt;tr&gt;
    &lt;td class="linenos"&gt;
     &lt;div class="linenodiv"&gt;
      &lt;pre&gt;1
2
3
4
5
6
7
8&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;
    &lt;td class="code"&gt;
     &lt;div class="source"&gt;
      &lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;build_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;build_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="c1"&gt;# add utc timezone info to each date, so django will serialize a&lt;/span&gt;
    &lt;span class="c1"&gt;# &amp;#39;Z&amp;#39; to the end of the string (and so javascript&amp;#39;s date constructor&lt;/span&gt;
    &lt;span class="c1"&gt;# will know it&amp;#39;s utc)&lt;/span&gt;
    &lt;span class="n"&gt;build_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fromtimestamp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;timestamp&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;tz&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tzutc&lt;/span&gt;&lt;span class="p"&gt;())]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt;
        &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;build_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;min_time&lt;/span&gt;
    &lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Trick question: there&amp;rsquo;s actually nothing wrong with this code. But if you look at the block in context (see the top of the post), you see that it&amp;rsquo;s only executed if &lt;em&gt;interval&lt;/em&gt; is specified, which it isn&amp;rsquo;t necessarily. The first case that my unit tests executed didn&amp;rsquo;t specify interval, so fail they did. It wasn&amp;rsquo;t immediately obvious to me why this was happening, so I went on a wild-goose chase of trying to figure out how the Django context might have been responsible for the unexpected output, before realizing my basic logic error.&lt;/p&gt;

&lt;p&gt;This was fairly easily corrected (my updated code applies the datetime-mapping unconditionally to set of optionally-filtered results) but perfectly illustrates my issue with idiomatic python: while the language itself has constructs like &lt;code&gt;map&lt;/code&gt; and &lt;code&gt;reduce&lt;/code&gt; that support the functional programming model, the language strongly steers you towards writing things in an imperative style that makes costly and annoying mistakes like this much easier to make. Yes, list and dictionary comprehensions are nice and compact but they start to break down in the more complex cases.&lt;/p&gt;

&lt;p&gt;As an experiment, I wrote up what this function might look like in a pure functional style with immutable data structures:&lt;/p&gt;

&lt;div class="brush: py"&gt;
 &lt;table class="sourcetable"&gt;
  &lt;tbody&gt;
   &lt;tr&gt;
    &lt;td class="linenos"&gt;
     &lt;div class="linenodiv"&gt;
      &lt;pre&gt;1
2
3
4
5
6
7
8&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;
    &lt;td class="code"&gt;
     &lt;div class="source"&gt;
      &lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;transform_and_filter_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;build_data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;new_build_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;build_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;new_build_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fromtimestamp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;timestamp&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;tz&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tzutc&lt;/span&gt;&lt;span class="p"&gt;())]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt;
        &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;build_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;min_time&lt;/span&gt;
    &lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;new_build_data&lt;/span&gt;
&lt;span class="n"&gt;transformed_build_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;transform_and_filter_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;A work of art it isn&amp;rsquo;t &amp;mdash; and definitely not &amp;ldquo;pythonic&amp;rdquo;. Compare this to a similar piece of code written in Javascript (ES6) with lodash (using a hypothetical &lt;code&gt;tzified&lt;/code&gt; function):&lt;/p&gt;

&lt;div class="brush: js"&gt;
 &lt;table class="sourcetable"&gt;
  &lt;tbody&gt;
   &lt;tr&gt;
    &lt;td class="linenos"&gt;
     &lt;div class="linenodiv"&gt;
      &lt;pre&gt;1
2
3
4
5
6
7&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;
    &lt;td class="code"&gt;
     &lt;div class="source"&gt;
      &lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt; &lt;span class="nx"&gt;transformedBuildData&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;mapValues&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;buildData&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;({&lt;/span&gt;
    &lt;span class="p"&gt;...&lt;/span&gt;&lt;span class="nx"&gt;buildData&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;buildData&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;data&lt;/span&gt;
      &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;datum&lt;/span&gt; &lt;span class="p"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;datum&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;minTimestamp&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;datum&lt;/span&gt; &lt;span class="p"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;tzcified&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;datum&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])].&lt;/span&gt;&lt;span class="nx"&gt;concat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;datum&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;slice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
  &lt;span class="p"&gt;})),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;buildId&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;length&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;A little bit easier to understand, but more importantly (to me anyway) it comes across as idiomatic and natural in a way that the python version just doesn&amp;rsquo;t. I&amp;rsquo;ve been happily programming Python for the last 10 years, but it&amp;rsquo;s increasingly feeling time to move on to greener pastures.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">mozregression's new mascot</title>
  <link rel="alternate" href="http://www.example.com/2017/07/mozregression-s-new-mascot.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2017-07-mozregression-s-new-mascot-html</id>
  <published>2017-07-31T15:32:02Z</published>
  <updated>2017-07-31T15:32:02Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Spent a few hours this morning on a few housekeeping issues with &lt;a href="https://mozilla.github.io/mozregression"&gt;mozregression&lt;/a&gt;. The web site was badly in need of an update (it was full of references to obsolete stuff like B2G and codefirefox.com) and the usual pile of fixes motivated a new release of the actual software. But most importantly, mozregression now has a proper application icon / logo, thanks to Victoria Wang!&lt;/p&gt;

&lt;p&gt;&lt;img src="/files/2017/07/mozregressionicon3.png" /&gt;&lt;/p&gt;

&lt;p&gt;One of the nice parts about working at Mozilla is the flexibility it offers to just hack on stuff that&amp;rsquo;s important, whether or not it&amp;rsquo;s part of your formal job description. Maintaining mozregression is pretty far outside my current set of responsibilities (or even interests), but I keep it going because it&amp;rsquo;s a key tool used by developers team here and no one else seems willing to take it over. Fortunately, tools like appveyor and pypi keep the time suckage to a mostly-reasonable level.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Taking over an npm package: sanity prevails</title>
  <link rel="alternate" href="http://www.example.com/2017/07/taking-over-an-npm-package-sanity-prevails.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2017-07-taking-over-an-npm-package-sanity-prevails-html</id>
  <published>2017-07-13T15:08:40Z</published>
  <updated>2017-07-13T15:08:40Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Sometimes problems are easier to solve than expected.&lt;/p&gt;

&lt;p&gt;For the last few months I&amp;rsquo;ve been working on the front end of a new project called &lt;a href="https://github.com/mozilla/missioncontrol"&gt;Mission Control&lt;/a&gt;, which aims to chart lots of interesting live information in something approximating realtime. Since this is a greenfield project, I thought it would make sense to use the currently-invogue framework at Mozilla (react) along with our standard visualization library, &lt;a href="http://metricsgraphicsjs.org/"&gt;metricsgraphics&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Metricsgraphics is great, but its jquery-esque api is somewhat at odds with the react way. The obvious solution to this problem is to wrap its functionality in a react component, and a quick google search determined that several people have tried to do exactly that, the most popular one being one called (obviously) react-metrics-graphics. Unfortunately, it hadn&amp;rsquo;t been updated in quite some time and some pull requests (including ones implementing features I needed for my project) weren&amp;rsquo;t being responded to.&lt;/p&gt;

&lt;p&gt;I expected this to be pretty difficult to resolve: I had no interaction with the author (Carter Feldman) before but based on my past experiences in free software, I was expecting stonewalling, leaving me no choice but to fork the package and give it a new name, a rather unsatisfying end result.&lt;/p&gt;

&lt;p&gt;But, hey, let&amp;rsquo;s keep an open mind on this. What does google say about unmaintained npm packages? Oh what&amp;rsquo;s this? They actually have a &lt;a href="https://docs.npmjs.com/misc/disputes"&gt;policy&lt;/a&gt;?&lt;/p&gt;

&lt;p&gt;tl;dr: You email the maintainer (politely) and CC support@npmjs.org about your interest in helping maintain the software. If you&amp;rsquo;re unable to come up with a resolution on your own, they will intervene.&lt;/p&gt;

&lt;p&gt;So I tried that. It turns out that Carter was really happy to hear that Mozilla was interested in taking over maintenance of this project, and not only gave me permission to start publishing newer versions to npm, but even transferred his repository over to Mozilla (so we could preserve issue and PR history). The project&amp;rsquo;s new location is here:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://github.com/mozilla/react-metrics-graphics"&gt;https://github.com/mozilla/react-metrics-graphics&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In hindsight, this is obviously the most reasonable outcome and I&amp;rsquo;m not sure why I was expecting anything else. Is the node community just friendlier than other areas I&amp;rsquo;ve worked in? Have community standards improved generally? In any case, thank you Carter for a great piece of software, hopefully it will thrive in its new home. :P&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Using Docker to run automated tests</title>
  <link rel="alternate" href="http://www.example.com/2017/06/using-docker-to-run-automated-tests.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2017-06-using-docker-to-run-automated-tests-html</id>
  <published>2017-06-02T20:04:38Z</published>
  <updated>2017-06-02T20:04:38Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;A couple months ago, I joined the Mozilla Data Platform team, to work on our &lt;a href="https://wiki.mozilla.org/Telemetry"&gt;Telemetry&lt;/a&gt; and automated data collection services. This has been an interesting transition for me, and a natural jumping off point from my work on &lt;a href="https://wiki.mozilla.org/EngineeringProductivity/Projects/Perfherder"&gt;Perfherder&lt;/a&gt;. Now, instead of manipulating mere 10s of gigabytes worth of fairly regular data, I&amp;rsquo;m working with 100s of terrabytes of noisy data with a much larger number of dimensions. :P It&amp;rsquo;s been interesting so far.&lt;/p&gt;

&lt;p&gt;One of the first things I decided to work on was improving our unit testing story around a few of our primary packages for data analysis/etl: &lt;a href="https://github.com/mozilla/python_moztelemetry/"&gt;python_moztelemetry&lt;/a&gt; (a library we use for running custom spark jobs against Telemetry data) and &lt;a href="https://github.com/mozilla/telemetry-batch-view/"&gt;telemetry-batch-view&lt;/a&gt; (a set of scala jobs we run against the main telemetry data store to create a useful set of aggregations that are easily queried with tools like &lt;a href="https://redash.io/"&gt;redash&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;It turns out that these tools interact with several larger / more involved pieces than I&amp;rsquo;m used to dealing with (such as hbase and thrift). For continuous integration/automation, we already had a set of travis scripts to install and reproduce the environment needed to test these parts, but there was no straightforward way to do this locally. My third time through creating an Ubuntu virtual machine environment to reproduce this environment locally (long story), I figured it was finally time for me to investigate using something to automate that setup procedure and make it easier for new developers to get into these projects.&lt;/p&gt;

&lt;p&gt;I hadn&amp;rsquo;t used it much before, but &lt;a href="https://docker.com"&gt;Docker&lt;/a&gt; seemed like a fairly obvious choice. Small, simple, and Linuxy? Sign me up.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m pretty happy with how things turned out, but there were a few caveats. Docker is more of a general purpose tool for building environments for running &lt;em&gt;things&lt;/em&gt;, whether that be an apache webserver or a jabber messaging doohickey (whereas e.g. something like travis is basically a domain-specific language for creating and running automated tests). There were a few tricks I needed to employ to make the whole testing process smooth in both cases, which I&amp;rsquo;ll document here for posterity:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;You can &lt;code&gt;ADD&lt;/code&gt; a set of files / directories to a docker environment inside your Dockerfile, but if you want your set of tests to pick up any changes made since the environment was created, you really should mount your testing directory inside the container using the &lt;code&gt;-v&lt;/code&gt; option.&lt;/li&gt;
 &lt;li&gt;If you need to download/install a piece of software when building the docker container, use the &lt;code&gt;RUN&lt;/code&gt; directive instead of &lt;code&gt;ADD&lt;/code&gt;. This will speed up rebuilding the container while you&amp;rsquo;re iterating on it (because you can take advantage of the Docker layers cache).&lt;/li&gt;
 &lt;li&gt;You almost certainly want to create a script (&lt;a href="https://github.com/mozilla/python_moztelemetry/blob/d2aa84bbac09465d38eeb0b5beb20edc7ddcc21b/runtests.sh"&gt;example&lt;/a&gt;) to streamline all the steps of running the tests: this will make running the tests easier for anyone wanting to contribute to your project and reduce the amount of documentation that you will have to write.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;The relevant files and documentation are in the repositories linked above.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Easier reproduction of intermittent test failures in automation</title>
  <link rel="alternate" href="http://www.example.com/2017/04/easier-reproduction-of-intermittent-test-failures-in-automation.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2017-04-easier-reproduction-of-intermittent-test-failures-in-automation-html</id>
  <published>2017-04-05T20:14:35Z</published>
  <updated>2017-04-05T20:14:35Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;As part of the &lt;a href="https://wiki.mozilla.org/Auto-tools/Projects/Stockwell"&gt;Stockwell project&lt;/a&gt;, I&amp;rsquo;ve been hacking on ways to make it easier for developers to diagnose failure of our tests in automation. It&amp;rsquo;s often very difficult to reproduce an intermittent failure we see in Treeherder locally since the environment is so different, but historically it has been a big hassle to get access to the machines we use in automation for various reasons.&lt;/p&gt;

&lt;p&gt;One option that rolled out last year was the so-called one-click loaner, which enabled developers to sign out an virtual machine instance identical to the ones used to run unit tests (at least if the tests are running on Taskcluster, which is increasingly often the case), then execute their particular case with whatever extra debugging options they would find useful. This is a big step forward, but it&amp;rsquo;s still quite a bit of hassle, since it requires a bunch of manual work on the part of the developer to interact with the instance.&lt;/p&gt;

&lt;p&gt;What if we could &lt;em&gt;just&lt;/em&gt; re-run the particular test an arbitrary number of times with whatever options we wanted, simply by clicking on a few buttons on Treeherder? I&amp;rsquo;ve been exploring this for the first few months of 2017 and I&amp;rsquo;ve come up with a prototype which I think is ready for people to start playing with.&lt;/p&gt;

&lt;p&gt;The user interface to this is pretty straightforward. Just find a job you want to retrigger in Treeherder:&lt;/p&gt;

&lt;p&gt;&lt;img src="/files/2017/04/treeherder-selected-mochitest.png" /&gt;&lt;/p&gt;

&lt;p&gt;Then select the &amp;rsquo;&amp;hellip;&amp;rsquo; option in the panel below and press &amp;ldquo;Custom Action&amp;hellip;&amp;rdquo;:&lt;/p&gt;

&lt;p&gt;&lt;img src="/files/2017/04/treeherder-taskcluster-options.png" /&gt;&lt;/p&gt;

&lt;p&gt;You should get a small piece of JSON to edit, which corresponds to the configuration for the retriggered job:&lt;/p&gt;

&lt;p&gt;&lt;img src="/files/2017/04/treeherder-custom-action.png" /&gt;&lt;/p&gt;

&lt;p&gt;The main field to edit is &amp;ldquo;path&amp;rdquo;. You should set this to the name of the test you want to try retriggering. For example &lt;code&gt;dom/animation/test/css-transitions/test_animation-ready.html&lt;/code&gt;. You can also set custom Firefox preferences and environment variables, to turn on different types of debugging.&lt;/p&gt;

&lt;p&gt;Unfortunately as usual with a new feature at Mozilla, there are a bunch of limitations and caveats:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;This depends on functionality that&amp;rsquo;s only in Taskcluster, so  buildbot jobs are exempt.&lt;/li&gt;
 &lt;li&gt;No support for Android yet. In combination with the above  limitation, this implies that this functionality only works  on Linux (at least until other platforms are moved to Taskcluster,  which hopefully isn&amp;rsquo;t that far off).&lt;/li&gt;
 &lt;li&gt;Browser chrome tests failing in mysterious ways if run repeatedly  (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1347654"&gt;bug 1347654&lt;/a&gt;)&lt;/li&gt;
 &lt;li&gt;Only reftest and mochitest are currently supported. XPCShell  support is blocked by the lack of support in its harness for  running a job repeatedly (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1347696"&gt;bug 1347696&lt;/a&gt;).  Web Platform Tests need the requisite support in mozharness for  just setting up the tests without running them &amp;mdash; the same issue  that prevents us from debugging such tests with a one-click loaner  (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1348833"&gt;bug 1348833&lt;/a&gt;).&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Aside from fixing the above limitations, the following features would also be really nifty to have:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Ability to trigger a custom job as part of a try push (i.e.  not needing to retrigger off an existing job)&lt;/li&gt;
 &lt;li&gt;Run these jobs under rr, and provide a way to login and  interactively debug when the problem is actually reproduced.&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;I am actually in the process of moving to another team @ Mozilla (more on that in another post), so I probably won&amp;rsquo;t have a ton of time to work on the above &amp;mdash; but I&amp;rsquo;d be happy to help anyone who&amp;rsquo;s interested in developing this idea further.&lt;/p&gt;

&lt;p&gt;A special shout out to the &lt;a href="https://wiki.mozilla.org/TaskCluster"&gt;Taskcluster&lt;/a&gt; team for helping me with the development of this feature: in particular the action task implementation from &lt;a href="https://jonasfj.dk/"&gt;Jonas Finnemann Jensen&lt;/a&gt; that made it possible to develop this feature in the first place.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Cancel all the things</title>
  <link rel="alternate" href="http://www.example.com/2017/02/cancel-all-the-things.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2017-02-cancel-all-the-things-html</id>
  <published>2017-02-07T18:36:09Z</published>
  <updated>2017-02-07T18:36:09Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;I just added a feature to Treeherder which lets you cancel a set of jobs (say, from a botched try push) much more easily. I&amp;rsquo;m hopeful that this will be helpful in keeping our resource usage on try more under control.&lt;/p&gt;

&lt;p&gt;It uses the &amp;ldquo;pinboard&amp;rdquo; feature of Treeherder which very few people are familiar with, so I made a very short video tutorial on how to make use of this feature and put it on the Joy of Automation channel:&lt;/p&gt;

&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/ryzsy38yw5A" frameborder="0" allowfullscreen="allowfullscreen"&gt;&lt;/iframe&gt;

&lt;p&gt;Happy cancelling!&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Training an autoclassifier</title>
  <link rel="alternate" href="http://www.example.com/2016/11/training-an-autoclassifier.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2016-11-training-an-autoclassifier-html</id>
  <published>2016-11-28T21:29:47Z</published>
  <updated>2016-11-28T21:29:47Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Here at Mozilla, we&amp;rsquo;ve accepted that a certain amount of intermittent failure in our automated testing of Firefox is to be expected. That is, for every push, a subset of the tests that we run will fail for reasons that have nothing to do with the quality (or lack thereof) of the push itself.&lt;/p&gt;

&lt;p&gt;On the main integration branches that developers commit code to, we have dedicated staff and volunteers called sheriffs who attempt to distinguish these expected failures from intermittents through a manual classification process using &lt;a href="https://treeherder.mozilla.org"&gt;Treeherder&lt;/a&gt;. On any given push, you can usually find some failed jobs that have stars beside them, this is the work of the sheriffs, indicating that a job&amp;rsquo;s failure is &amp;ldquo;nothing to worry about&amp;rdquo;:&lt;/p&gt;

&lt;p&gt;&lt;img src="/files/2016/11/treeherder-in-action.png" /&gt;&lt;/p&gt;

&lt;p&gt;This generally works pretty well, though unfortunately it doesn&amp;rsquo;t help developers who need to test their changes on Try, which have the same sorts of failures but no sheriffs to watch them or interpret the results. For this reason (and a few others which I won&amp;rsquo;t go into detail on here), there&amp;rsquo;s been much interest in having Treeherder autoclassify known failures.&lt;/p&gt;

&lt;p&gt;We have a partially implemented version that attempts to do this based on structured (failure line) information, but we&amp;rsquo;ve had some difficulty creating a reasonable user interface to train it. Sheriffs are used to being able to quickly tag many jobs with the same bug. Having to go through each job&amp;rsquo;s failure lines and manually annotate each of them is much more time consuming, at least with the approaches that have been tried so far.&lt;/p&gt;

&lt;p&gt;&lt;img src="/files/2016/11/treeherder-per-line-classification.png" /&gt;&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s quite possible that this is a solvable problem, but I thought it might be an interesting exercise to see how far we could get training an autoclassifier with only the existing per-job classifications as training data. With some recent work I&amp;rsquo;ve done on refactoring Treeherder&amp;rsquo;s database, getting a complete set of per-job failure line information is only a small SQL query away:&lt;/p&gt;

&lt;div class="brush: sql"&gt;
 &lt;table class="sourcetable"&gt;
  &lt;tbody&gt;
   &lt;tr&gt;
    &lt;td class="linenos"&gt;
     &lt;div class="linenodiv"&gt;
      &lt;pre&gt;1
2
3
4
5&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;
    &lt;td class="code"&gt;
     &lt;div class="source"&gt;
      &lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;select&lt;/span&gt; &lt;span class="n"&gt;bjm&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bjm&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bug_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tle&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="k"&gt;from&lt;/span&gt; &lt;span class="n"&gt;bug_job_map&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;bjm&lt;/span&gt;
  &lt;span class="k"&gt;left&lt;/span&gt; &lt;span class="k"&gt;join&lt;/span&gt; &lt;span class="n"&gt;text_log_step&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;tls&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="n"&gt;tls&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;job_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bjm&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;job_id&lt;/span&gt;
  &lt;span class="k"&gt;left&lt;/span&gt; &lt;span class="k"&gt;join&lt;/span&gt; &lt;span class="n"&gt;text_log_error&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;tle&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="n"&gt;tle&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tls&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;
  &lt;span class="k"&gt;where&lt;/span&gt; &lt;span class="n"&gt;bjm&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;created&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;2016-10-31&amp;#39;&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;bjm&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;created&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;2016-11-24&amp;#39;&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;bjm&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;user_id&lt;/span&gt; &lt;span class="k"&gt;is&lt;/span&gt; &lt;span class="k"&gt;not&lt;/span&gt; &lt;span class="k"&gt;NULL&lt;/span&gt; &lt;span class="k"&gt;and&lt;/span&gt; &lt;span class="n"&gt;bjm&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bug_id&lt;/span&gt; &lt;span class="k"&gt;is&lt;/span&gt; &lt;span class="k"&gt;not&lt;/span&gt; &lt;span class="k"&gt;NULL&lt;/span&gt;
  &lt;span class="k"&gt;order&lt;/span&gt; &lt;span class="k"&gt;by&lt;/span&gt; &lt;span class="n"&gt;bjm&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tle&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tle&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Just to give some explanation of this query, the &amp;ldquo;bug_job_map&amp;rdquo; provides a list of bugs that have been applied to jobs. The &amp;ldquo;text_log_step&amp;rdquo; and &amp;ldquo;text_log_error&amp;rdquo; tables contain the actual errors that Treeherder has extracted from the textual logs (to explain the failure). From this raw list of mappings and errors, we can construct a data structure incorporating the job, the assigned bug and the textual errors inside it. For example:&lt;/p&gt;

&lt;div class="brush: json"&gt;
 &lt;table class="sourcetable"&gt;
  &lt;tbody&gt;
   &lt;tr&gt;
    &lt;td class="linenos"&gt;
     &lt;div class="linenodiv"&gt;
      &lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;
    &lt;td class="code"&gt;
     &lt;div class="source"&gt;
      &lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="nt"&gt;"bug_number"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1202623&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="nt"&gt;"lines"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
  &lt;span class="s2"&gt;"browser_private_clicktoplay.js Test timed out -"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"browser_private_clicktoplay.js Found a tab after previous test timed out: http:/&amp;lt;number&amp;gt;&amp;lt;number&amp;gt;:&amp;lt;number&amp;gt;/browser/browser/base/content/test/plugins/plugin_test.html -"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"browser_private_clicktoplay.js Found a browser window after previous test timed out -"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"browser_private_clicktoplay.js A promise chain failed to handle a rejection:  - at chrome://mochikit/content/browser-test.js:&amp;lt;number&amp;gt; - TypeError: this.SimpleTest.isExpectingUncaughtException is not a function"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"browser_privatebrowsing_newtab_from_popup.js Test timed out -"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"browser_privatebrowsing_newtab_from_popup.js Found a browser window after previous test timed out -"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"browser_privatebrowsing_newtab_from_popup.js Found a browser window after previous test timed out -"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"browser_privatebrowsing_newtab_from_popup.js Found a browser window&lt;/span&gt;
&lt;span class="s2"&gt;  after previous test timed out -"&lt;/span&gt;
  &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Some quick google searching revealed that &lt;a href="http://scikit-learn.org/"&gt;scikit-learn&lt;/a&gt; is a popular tool for experimenting with text classifications. They even had a &lt;a href="http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"&gt;tutorial&lt;/a&gt; on classifying newsgroup posts which seemed tantalizingly close to what we needed to do here. In that example, they wanted to predict which newsgroup a post belonged to based on its content. In our case, we want to predict which existing bug a job failure should belong to based on its error lines.&lt;/p&gt;

&lt;p&gt;There are obviously some differences in our domain: test failures are much more regular and structured. There are lots of numbers in them which are mostly irrelevant to the classification (e.g. the &amp;ldquo;expected 12 pixels different, got 10!&amp;rdquo; type errors in reftests). Ordering of failures might matter. Still, some of the techniques used on corpora of normal text documents for training a classifier probably map nicely onto what we&amp;rsquo;re trying to do here: it seems plausible that weighting words which occur more frequently less strongly against ones that are less common would be helpful, for example, and that&amp;rsquo;s one thing their default transformers does.&lt;/p&gt;

&lt;p&gt;In any case, I built up a small little script to download a subset of the downloaded data (from November 1st to November 23rd), used it as training data for a classifier, then tested that against another subset of test failures between November 24th and 28th.&lt;/p&gt;

&lt;div class="brush: py"&gt;
 &lt;table class="sourcetable"&gt;
  &lt;tbody&gt;
   &lt;tr&gt;
    &lt;td class="linenos"&gt;
     &lt;div class="linenodiv"&gt;
      &lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;
    &lt;td class="code"&gt;
     &lt;div class="source"&gt;
      &lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_files&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.feature_extraction.text&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;CountVectorizer&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.feature_extraction.text&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;TfidfTransformer&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SGDClassifier&lt;/span&gt;


&lt;span class="n"&gt;training_set&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_files&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;training&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;count_vect&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;CountVectorizer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;X_train_counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;count_vect&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_set&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;tfidf_transformer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TfidfTransformer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;X_train_tfidf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tfidf_transformer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train_counts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SGDClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;hinge&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;penalty&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;l2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e-3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train_tfidf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;training_set&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;num_correct&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;num_missed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;subdir&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fnames&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;walk&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;testing/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;fnames&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;bugnum&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;basename&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;subdir&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;bugnum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fnames&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;fname&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;fnames&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;doc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;subdir&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fname&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;doc&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;"--&amp;gt; (skipping, empty)"&lt;/span&gt;
            &lt;span class="n"&gt;X_new_counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;count_vect&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;doc&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;X_new_tfidf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tfidf_transformer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_new_counts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;predicted_bugnum&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;training_set&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target_names&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_new_tfidf&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;bugnum&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;predicted_bugnum&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;num_correct&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
                &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;"--&amp;gt; correct"&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;num_missed&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
                &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;"--&amp;gt; missed (&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;)"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;predicted_bugnum&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;"Correct: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt; Missed: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt; Ratio: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_correct&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_missed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_correct&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_correct&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;num_missed&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;With absolutely no tweaking whatsoever, I got an accuracy rate of 75% on the test data. That is, the algorithm chose the correct classification given the failure text 1312 times out of 1959. Not bad for a first attempt!&lt;/p&gt;

&lt;p&gt;After getting that working, I did some initial testing to see if I could get better results by reusing some of the error ETL summary code in Treeherder we use for bug suggestions, but the results were pretty much the same.&lt;/p&gt;

&lt;p&gt;So what&amp;rsquo;s next? This seems like a wide open area to me, but some initial areas that seem worth exploring, if we wanted to take this idea further:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;Investigate cases where the autoclassification failed or had a near miss. Is there a pattern here? Is there something simple we could do, either by tweaking the input data or using a better vectorizer/tokenizer?&lt;/li&gt;
 &lt;li&gt;Have a confidence threshold for using the autoclassifier&amp;rsquo;s data. It seems likely to me that many of the cases above where we got the wrong were cases where the classifier itself wasn&amp;rsquo;t that confident in the result (vs. others). We can either present that in the user interface or avoid classifications for these cases altogether (and leave it up to a human being to make a decision on whether this is an intermittent).&lt;/li&gt;
 &lt;li&gt;Using the structured log data inside the database as input to a classifier. Structured log data here is much more regular and denser than the free text that we&amp;rsquo;re using. Even if it isn&amp;rsquo;t explicitly classified, we may well get better results by using it as our input data.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;If you&amp;rsquo;d like to experiment with the data and/or code, I&amp;rsquo;ve put it up on a &lt;a href="https://github.com/wlach/treeherder-classifier"&gt;github repository&lt;/a&gt;.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Slow Treeherder, Fast Treeherder</title>
  <link rel="alternate" href="http://www.example.com/2016/10/slow-treeherder-fast-treeherder.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2016-10-slow-treeherder-fast-treeherder-html</id>
  <published>2016-10-31T15:40:00Z</published>
  <updated>2016-10-31T15:40:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Just wanted to talk about some recent performance improvements we&amp;rsquo;ve made recently to &lt;a href="https://wiki.mozilla.org/EngineeringProductivity/Projects/Treeherder"&gt;Treeherder&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1311511"&gt;Bug 1311511&lt;/a&gt;: Changed the repository endpoint so we don&amp;rsquo;t do 40 redundant database  queries (this was generally innocuous, but could delay loading by  400ms if the database was under heavy load).&lt;/li&gt;
 &lt;li&gt;&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1310016"&gt;Bug 1310016&lt;/a&gt;: Persisted database connections across requests (this  can save ~40&amp;ndash;50ms per request, of which there can be 5&amp;ndash;10  when loading a Treeherder page).&lt;/li&gt;
 &lt;li&gt;&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1308782"&gt;Bug 1308782&lt;/a&gt;: &lt;em&gt;Don&amp;rsquo;t&lt;/em&gt; download job type and group information  from the server to get a &amp;ldquo;sorting order&amp;rdquo; for the job lists. This was  never necessary, but it&amp;rsquo;s gotten exponentially more painful as  people have added job types to Treeherder (job type information is  now around a megabyte of JSON these days). This saves 5&amp;ndash;10 seconds on a  typical page load.&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;There&amp;rsquo;s more to come, but with these changes Treeherder should be faster for everyone to load. It should be particularly noticeable on try pushes, where the last item was by far the largest bottleneck. Here&amp;rsquo;s a youtube video of the changes:&lt;/p&gt;

&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/xNJGoZhA4Vs" frameborder="0" allowfullscreen="allowfullscreen"&gt;&lt;/iframe&gt;

&lt;p&gt;The original is on the left. The newer, faster Treeherder is on the right. Pay particular attention to how much faster the job information populates.&lt;/p&gt;

&lt;p&gt;Moral of the story? Optimization can be helpful, but it&amp;rsquo;s better if you can avoid doing the work altogether.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Herding Automation Infrastructure</title>
  <link rel="alternate" href="http://www.example.com/2016/08/herding-automation-infrastructure.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2016-08-herding-automation-infrastructure-html</id>
  <published>2016-08-17T20:18:12Z</published>
  <updated>2016-08-17T20:18:12Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;For every commit to Firefox, we run a battery of builds and automated tests on the resulting source tree to make sure that the result still works and meets our correctness and performance quality criteria. This is expensive: every new push to our repository implies hundreds of hours of machine time. However, automated quality control is essential to ensure that the product that we&amp;rsquo;re shipping to users is something that we can be proud of.&lt;/p&gt;

&lt;p&gt;But what about evaluating the quality of the product which does the building and testing? Who does that? And by what criteria would we say that our automation system is good or bad? Up to now, our procedures for this have been rather embarassingly adhoc. With some exceptions (such as &lt;a href="https://brasstacks.mozilla.com/orangefactor/"&gt;OrangeFactor&lt;/a&gt;), our QA process amounts to motivated engineers doing a one-off analysis of a particular piece of the system, filing a few bugs, then forgetting about it. Occasionally someone will propose turning build and test automation for a specific platform on or off in mozilla.dev.planning.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;d like to suggest that the time has come to take a more systemic approach to this class of problem. We spend a lot of money on people and machines to maintain this infrastructure, and I think we need a more disciplined approach to make sure that we are getting good value for that investment.&lt;/p&gt;

&lt;p&gt;As a starting point, I feel like we need to pay closer attention to the following characteristics of our automation:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;End-to-end times from push submission to full completion of all  build and test jobs: if this gets too long, it makes the lives  of all sorts of people painful &amp;mdash; tree closures become longer when  they happen (because it takes longer to either notice bustage or  find out that it&amp;rsquo;s fixed), developers have to wait longer for  try pushes (making them more likely to just push directly to an  integration branch, causing the former problem&amp;hellip;)&lt;/li&gt;
 &lt;li&gt;Number of machine hours consumed by the different types of test  jobs: our resources are large (relatively speaking), but not  unlimited. We need proper accounting of where we&amp;rsquo;re spending money  and time. In some cases, resources used to perform a task that  we don&amp;rsquo;t care that much about could be redeployed towards an  underresourced task that we do care about. A good example of this  was linux32 talos (performance tests) last year: when the question  was raised of why we were doing performance testing on this specific  platform (in addition to Linux64), no one could come up with a great  justification. So we turned the tests off and reconfigured the machines  to do Windows performance tests (where we were suffering from a severe  lack of capacity).&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Over the past week, I&amp;rsquo;ve been prototyping a project I&amp;rsquo;ve been calling &amp;ldquo;Infraherder&amp;rdquo; which uses the data inside &lt;a href="https://treeherder.mozilla.org"&gt;Treeherder&lt;/a&gt;&amp;rsquo;s job database to try to answer these questions (and maybe some others that I haven&amp;rsquo;t thought of yet). You can see a hacky version of it on &lt;a href="http://wlach.github.io/treeherder/ui/infra.html#/last-finished"&gt;my github fork&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Why implement this in Treeherder you might ask? Two reasons. First, Treeherder already stores the job data in a historical archive that&amp;rsquo;s easy to query (using SQL). Using this directly makes sense over creating a new data store. Second, Treeherder provides a useful set of front-end components with which to build a UI with which to visualize this information. I actually did my initial prototyping inside an ipython notebook, but it quickly became obvious that for my results to be useful to others at Mozilla we needed some kind of real dashboard that people could dig into.&lt;/p&gt;

&lt;p&gt;On the Treeherder team at Mozilla, we&amp;rsquo;ve found the &lt;a href="https://newrelic.com"&gt;New Relic&lt;/a&gt; software to be invaluable for diagnosing and fixing quality and performance problems for Treeherder itself, so I took some inspiration from it (unfortunately the problem space of our automation is not quite the same as that of a web application, so we can&amp;rsquo;t just use New Relic directly).&lt;/p&gt;

&lt;p&gt;There are currently two views in the prototype, a &amp;ldquo;last finished&amp;rdquo; view and a &amp;ldquo;total&amp;rdquo; view. I&amp;rsquo;ll describe each of them in turn.&lt;/p&gt;

&lt;h3 id="last-finished"&gt;Last finished&lt;/h3&gt;

&lt;p&gt;This view shows the counts of which scheduled automation jobs were the &amp;ldquo;last&amp;rdquo; to finish. The hypothesis is that jobs that are frequently last indicate blockers to developer productivity, as they are the &amp;ldquo;long pole&amp;rdquo; in being able to determine if a push is good or bad.&lt;/p&gt;

&lt;p&gt;&lt;img src="/files/2016/08/infraherder-last-finished.png" width="500px" /&gt;&lt;/p&gt;

&lt;p&gt;Right away from this view, you can see the mochitest devtools 9 test is often the last to finish on try, with Windows 7 mochitest debug a close second. Assuming that the reasons for this are not resource starvation (they don&amp;rsquo;t appear to be), we could probably get results into the hands of developers and sheriffs faster if we split these jobs into two seperate ones. I filed bugs &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1294489"&gt;1294489&lt;/a&gt; and &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1294706"&gt;1294706&lt;/a&gt; to address these issues.&lt;/p&gt;

&lt;h3 id="total-time"&gt;Total Time&lt;/h3&gt;

&lt;p&gt;This view just shows which jobs are taking up the most machine hours.&lt;/p&gt;

&lt;p&gt;&lt;img src="/files/2016/08/infraherder-total.png" width="500px" /&gt;&lt;/p&gt;

&lt;p&gt;Probably unsurprisingly, it seems like it&amp;rsquo;s Android test jobs that are taking up most of the time here: these tests are running on multiple layers of emulation (AWS instances to emulate Linux hardware, then the already slow QEMU-based Android simulator) so are not expected to have fast runtime. I wonder if it might not be worth considering running these tests on faster instances and/or bare metal machines.&lt;/p&gt;

&lt;p&gt;Linux32 debug tests seem to be another large consumer of resources. Market conditions make turning these tests off altogether a non-starter (see bug &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1255890"&gt;1255890&lt;/a&gt;), but how much value do we really derive from running the debug version of linux32 through automation (given that we&amp;rsquo;re already doing the same for 64-bit Linux)?&lt;/p&gt;

&lt;h3 id="request-for-comments"&gt;Request for comments&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;ve created &lt;a href="https://docs.google.com/document/d/1SrlJQQ3qWuM0tvruG6Lr59t3hJ4XRUoMIrIRQYvwu9A/edit?usp=sharing"&gt;an RFC&lt;/a&gt; for this project on Google Docs, as a sort of test case for a new process we&amp;rsquo;re thinking of using in Engineering Productivity for these sorts of projects. If you have any questions or comments, I&amp;rsquo;d love to hear them! My perspective on this vast problem space is limited, so I&amp;rsquo;m sure there are things that I&amp;rsquo;m missing.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Perfherder Quarter of Contribution Summer 2016: Results</title>
  <link rel="alternate" href="http://www.example.com/2016/08/perfherder-quarter-of-contribution-summer-2016-results.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2016-08-perfherder-quarter-of-contribution-summer-2016-results-html</id>
  <published>2016-08-10T20:37:05Z</published>
  <updated>2016-08-10T20:37:05Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Following on the footsteps of Mike Ling&amp;rsquo;s &lt;a href="/blog/2015/09/perfherder-summer-of-contribution-thoughts/"&gt;amazing work&lt;/a&gt; on &lt;a href="https://wiki.mozilla.org/ngineeringProductivity‎/Projects/Perfherder"&gt;Perfherder&lt;/a&gt; in 2015 (he&amp;rsquo;s gone on to do a GSOC project), I got two amazing contributors to continue working on the project for a few weeks this summer as part of our &lt;a href="https://wiki.mozilla.org/Auto-tools/New_Contributor/Quarter_of_Contribution"&gt;quarter of contribution&lt;/a&gt; program: Shruti Jasoria and Roy Chiang.&lt;/p&gt;

&lt;p&gt;Shruti started by adding a feature to the treeherder/perfherder backend (ability to enable or disable a new performance framework on a tentative basis), then went on to make all sorts of improvements to the Treeherder / Perfherder frontend, fixing bugs in the performance sheriffing frontend, updating code to use more modern standards (including a gigantic patch to enable a bunch of eslint rules and fix the corresponding problems).&lt;/p&gt;

&lt;p&gt;Roy worked all over the codebase, starting with some simple frontend fixes to Treeherder, moving on to fix a large number of nits in Perfherder&amp;rsquo;s alerts view. My personal favorite is the fact that we now paginate the list of alerts inside this view, which makes navigation waaaaay back into history possible:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2016/08/perfherder-alert-pagination.png"&gt;&lt;img src="/files/2016/08/perfherder-alert-pagination.png" alt="alert pagination" width="300px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can see a summary of their work at these links:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="https://github.com/mozilla/treeherder/commits/master?author=SJasoria"&gt;Shruti Jasoria&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;&lt;a href="https://github.com/mozilla/treeherder/commits/master?author=crosscent"&gt;Roy Chiang&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Thank you Shruti and Roy! You&amp;rsquo;ve helped to make sure Firefox (and Servo!) performance remains top-notch.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Quarter of Contribution: June / July 2016 edition</title>
  <link rel="alternate" href="http://www.example.com/2016/05/quarter-of-contribution-june-july-2016-edition.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2016-05-quarter-of-contribution-june-july-2016-edition-html</id>
  <published>2016-05-27T14:51:54Z</published>
  <updated>2016-05-27T14:51:54Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Just wanted to announce that, once again, my team (&lt;a href="https://wiki.mozilla.org/EngineeringProductivity"&gt;Mozilla Engineering Productivity&lt;/a&gt;) is just about to start running another &lt;a href="https://wiki.mozilla.org/Auto-tools/New_Contributor/Quarter_of_Contribution"&gt;quarter of contribution&lt;/a&gt; &amp;mdash; a great opportunity for newer community members to dive deep on some of the projects we&amp;rsquo;re working on, brush up on their programming and problem solving skills, and work with experienced mentors. You can find more information on this program &lt;a href="https://wiki.mozilla.org/Auto-tools/New_Contributor/Quarter_of_Contribution/Summer_2016"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve found this program to be a really great experience on both sides &amp;mdash; it&amp;rsquo;s an opportunity for contributors to really go beyond the &amp;ldquo;good first bug&amp;rdquo; style of patches to having a really substantial impact on some of the projects that we&amp;rsquo;re working on while gaining lots of software development skills that are useful in the real world.&lt;/p&gt;

&lt;p&gt;Once again, I&amp;rsquo;m going to be mentoring one or two people on the Perfherder project, a tool we use to measure and sheriff Firefox performance. If you&amp;rsquo;re inclined to work on some really interesting data analysis and user interface problems in Python and JavaScript, please have a look at the &lt;a href="https://wiki.mozilla.org/Auto-tools/New_Contributor/Quarter_of_Contribution/Perfherder"&gt;project page&lt;/a&gt; and get in touch. :)&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Are We Fast Yet and Perfherder</title>
  <link rel="alternate" href="http://www.example.com/2016/03/are-we-fast-yet-and-perfherder.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2016-03-are-we-fast-yet-and-perfherder-html</id>
  <published>2016-03-30T15:42:39Z</published>
  <updated>2016-03-30T15:42:39Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Historically at Mozilla, we&amp;rsquo;ve had a bunch of different systems running to benchmark Firefox&amp;rsquo;s performance. The two most broadly-scoped are &lt;a href="https://wiki.mozilla.org/Buildbot/Talos"&gt;Talos&lt;/a&gt; (which runs as part of our build process, and emphasizes common real-world use cases, like page loading) and &lt;a href="https://arewefastyet.com/"&gt;Are We Fast Yet&lt;/a&gt; (which runs seperately, and emphasizes JavaScript performance and benchmarks).&lt;/p&gt;

&lt;p&gt;As many of you know, most of my focus over the last year-and-a-bit has been developing a system called Perfherder, which aims to make monitoring and acting on performance data easier. A great introduction to Perfherder is my &lt;a href="/blog/2016/03/platform-engineering-project-of-the-month-perfherder/"&gt;project of the month post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The initial focus of Perfherder has been Talos, which is deeply integrated into our automation and also maintained by Engineering Productivity (my group). However, the intention was always to allow anyone in the Mozilla community to submit performance data for Firefox and sheriff it, much like Treeherder has supported the submission of test result data from third parties (e.g. autophone, Firefox UI tests). There are more commonalities than differences in how we do performance sheriffing with Are We Fast Yet (which currently has its own web interface) and Perfherder, so it made sense to see if we could pool resources.&lt;/p&gt;

&lt;p&gt;So, over the last couple of months, &lt;a href="https://elvis314.wordpress.com/"&gt;Joel Maher&lt;/a&gt; and I have been in discussions with Hannes Verschore, current maintainer of Are We Fast Yet (AWFY) to see what could be done. It looks like it is possible for Perfherder to provide most of what AWFY needs, though there are a few exceptions. I thought for the benefit of others, it might be useful to outline what&amp;rsquo;s done, what&amp;rsquo;s coming next, and what might not be implemented (at least not any time soon).&lt;/p&gt;

&lt;h3 id="whats-done"&gt;What&amp;rsquo;s done&lt;/h3&gt;

&lt;ul&gt;
 &lt;li&gt;Get AWFY submitting data to Perfherder and allow it to be sheriffed  seperately from Talos. This is working on treeherder stage, and you  can already examine the &lt;a href="https://treeherder.allizom.org/perf.html#/alerts?status=0&amp;amp;framework=5"&gt;alert data&lt;/a&gt;.&lt;/li&gt;&lt;/ul&gt;

&lt;h3 id="whats-in-progress-or-in-the-near-term-pipeline"&gt;What&amp;rsquo;s in progress (or in the near-term pipeline)&lt;/h3&gt;

&lt;ul&gt;
 &lt;li&gt;Allow custom alerting behaviour (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1254595"&gt;bug 1254595&lt;/a&gt;). For example, we want  to alert on subtests for AWFY while still summarizing the results.  This is something we don&amp;rsquo;t currently support.&lt;/li&gt;
 &lt;li&gt;Allow creating an alert manually (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1260791"&gt;bug 1260791&lt;/a&gt;). Sadly, our regression detection  algorithm is not perfect. AWFY already supports this, we should too. This is something we also want for Talos.&lt;/li&gt;
 &lt;li&gt;Make regression-filing templates non-talos-specific (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1260805"&gt;bug 1260805&lt;/a&gt;). Currently we have a convenience template for filing bugs for performance  regressions, but this is currently specific to various things about Talos (job running instructions, links to documentation, etc.). We should  make it configurable so other projects like AWFY can take advantage of this functionality.&lt;/li&gt;&lt;/ul&gt;

&lt;h3 id="under-consideration"&gt;Under consideration&lt;/h3&gt;

&lt;ul&gt;
 &lt;li&gt;Some kind of support for bisecting a push to figure out which patch  caused a regression. AWFY currently supports this, but it&amp;rsquo;s a fairly  difficult thing to add to Perfherder (much of which is built upon  Treeherder&amp;rsquo;s per-push result model). Maybe this is something we should  do, but it would be a significant amount of effort.&lt;/li&gt;
 &lt;li&gt;Proprietary benchmarks: AWFY runs one benchmark the results for  which we can&amp;rsquo;t make public. Adding &amp;ldquo;private&amp;rdquo; jobs or results to  Treeherder is likely a big can of worms, but it might be something  we want to do eventually.&lt;/li&gt;&lt;/ul&gt;

&lt;h3 id="probably-wont-fix"&gt;Probably won&amp;rsquo;t fix&lt;/h3&gt;

&lt;ul&gt;
 &lt;li&gt;Supporting comparative measurements between Firefox and other  browsers. This is an important task, but doesn&amp;rsquo;t really fit into the  model of Perfherder, which is intimately tied to the revision data  associated with Firefox. To do this would require detailed tracking  of Chrome on the same basis, and I don&amp;rsquo;t think that&amp;rsquo;s really a place  where we want to go. We should definitely monitor for general  trends, but I think that is best done with a seperate system.&lt;/li&gt;&lt;/ul&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Platform engineering project of the month: Perfherder</title>
  <link rel="alternate" href="http://www.example.com/2016/03/platform-engineering-project-of-the-month-perfherder.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2016-03-platform-engineering-project-of-the-month-perfherder-html</id>
  <published>2016-03-15T00:10:57Z</published>
  <updated>2016-03-15T00:10:57Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;&lt;em&gt;[ originally posted on &lt;a href="https://groups.google.com/d/msg/mozilla.dev.platform/itdfru6csSk/vfVP_WDXBgAJ"&gt;mozilla.dev.platform&lt;/a&gt; ]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Hello from Platform Engineering Operations! Once a month we highlight one of our projects to help the Mozilla community discover a useful tool or an interesting contribution opportunity.&lt;/p&gt;

&lt;p&gt;This month’s project is Perfherder!&lt;/p&gt;

&lt;h3 id="what-is-perfherder"&gt;What is Perfherder?&lt;/h3&gt;

&lt;p&gt;Perfherder is a generic system for visualizing and analyzing performance data produced by the many automated tests we run here at Mozilla (such as Talos, &amp;ldquo;Are we fast yet?&amp;rdquo; or &amp;ldquo;Are we slim yet?&amp;rdquo;). The chief goal of the project is to make sure that performance of Firefox gets better, not worse over time. It does this by:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Tracking the performance generated by our automated tests, allowing  them to be visualized on a graph.&lt;/li&gt;
 &lt;li&gt;Providing a sheriffing dashboard which allows for incoming  alerts of performance regressions to be annotated and triaged - bugs  can be filed based on a template and their resolution status can be  tracked.&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;In addition to its own user interface, Perfherder also provides an API on the backend that other people can use to build custom performance visualizations and dashboards. For example, the metrics group has been working on a set of release quality indices for performance based on Perfherder data:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://metrics.mozilla.com/quality-indices/"&gt;https://metrics.mozilla.com/quality-indices/&lt;/a&gt;&lt;/p&gt;

&lt;h3 id="how-it-works"&gt;How it works&lt;/h3&gt;

&lt;p&gt;Perfherder is part of Treeherder, building on that project&amp;rsquo;s existing support for tracking revision and test job information. Like the rest of Treeherder, Perfherder&amp;rsquo;s backend is written in Python, using the Django web framework. The user interface is written as an AngularJS application.&lt;/p&gt;

&lt;h3 id="learning-more"&gt;Learning more&lt;/h3&gt;

&lt;p&gt;For more information on Perfherder than you ever wanted to know, please see the wiki page:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://wiki.mozilla.org/EngineeringProductivity/Projects/Perfherder"&gt;https://wiki.mozilla.org/EngineeringProductivity/Projects/Perfherder&lt;/a&gt;&lt;/p&gt;

&lt;h3 id="can-i-contribute"&gt;Can I contribute?&lt;/h3&gt;

&lt;p&gt;Yes! We have had some fantastic contributions from the community to Perfherder, and are always looking for more. This is a great way to help developers make Firefox faster (or use less memory). The core of Perfherder is relatively small, so this is a great chance to learn either Django or Angular if you have a small amount of Python and/or JavaScript experience.&lt;/p&gt;

&lt;p&gt;We have set aside a set of bugs that are suitable for getting started here:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://bugzilla.mozilla.org/buglist.cgi?list_id=12722722&amp;amp;resolution=---&amp;amp;status_whiteboard_type=allwordssubstr&amp;amp;query_format=advanced&amp;amp;status_whiteboard=perfherder-starter-bug"&gt;https://bugzilla.mozilla.org/buglist.cgi?list_id=12722722&amp;amp;resolution=---&amp;amp;status_whiteboard_type=allwordssubstr&amp;amp;query_format=advanced&amp;amp;status_whiteboard=perfherder-starter-bug&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information on contributing to Perfherder, please see the contribution section of the above wiki page:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://wiki.mozilla.org/EngineeringProductivity/Projects/Perfherder#Contribution"&gt;https://wiki.mozilla.org/EngineeringProductivity/Projects/Perfherder#Contribution&lt;/a&gt;&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Talos suites now visible from trychooser</title>
  <link rel="alternate" href="http://www.example.com/2016/02/talos-suites-now-visible-from-trychooser.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2016-02-talos-suites-now-visible-from-trychooser-html</id>
  <published>2016-02-13T19:28:47Z</published>
  <updated>2016-02-13T19:28:47Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;It&amp;rsquo;s a small thing, but I submitted a patch to &lt;a href="http://trychooser.pub.build.mozilla.org/"&gt;trychooser&lt;/a&gt; last week which adds a tooltip indicating the actual Talos tests that are run as part of the various jobs that you can schedule as part of a try push. It&amp;rsquo;s in production as of now:&lt;/p&gt;

&lt;video src="/files/2016/02/talos-trychooser.webm" controls="controls" autoplay="autoplay"&gt;&lt;/video&gt;

&lt;p&gt;Previously, the only way to do this was to dig into the actual buildbot code, which was more than a little annoying.&lt;/p&gt;

&lt;p&gt;If you think your patch might have a good chance of regressing performance, please do run the &lt;a href="https://wiki.mozilla.org/Buildbot/Talos/Tests"&gt;Talos tests&lt;/a&gt; before you check in. It&amp;rsquo;s much less work for all of us when these things are caught before integration and back outs are no fun for anyone. We really need better documentation for this stuff, but meanwhile if you need help with this, please ask in the #perf channel on irc.mozilla.org&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Perfherder: Onward!</title>
  <link rel="alternate" href="http://www.example.com/2015/11/perfherder-onward.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2015-11-perfherder-onward-html</id>
  <published>2015-11-04T05:00:00Z</published>
  <updated>2015-11-04T05:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;In addition to the &lt;a href="http://wrla.ch/blog/2015/10/the-new-old-perfherder-data-model/"&gt;database refactoring&lt;/a&gt; I mentioned a few weeks ago, some cool stuff has been going into &lt;a href="https://wiki.mozilla.org/Auto-tools/Projects/Perfherder"&gt;Perfherder&lt;/a&gt; lately.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tracking installer size&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Perfherder is now tracking the size of the Firefox installer for the various platforms we support (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1149164"&gt;bug 1149164&lt;/a&gt;). I originally only intended to track Android .APK size (on request from the mobile team), but installer sizes for other platforms came along for the ride. I don&amp;#8217;t think anyone will complain.&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2015/11/Screen-Shot-2015-11-03-at-5.28.48-PM.png"&gt;&lt;img src="/files/2015/11/Screen-Shot-2015-11-03-at-5.28.48-PM-300x181.png" alt="Screen Shot 2015-11-03 at 5.28.48 PM" width="300" height="181" class="alignnone size-medium wp-image-1274" srcset="/files/2015/11/Screen-Shot-2015-11-03-at-5.28.48-PM-300x181.png 300w, /files/2015/11/Screen-Shot-2015-11-03-at-5.28.48-PM-1024x618.png 1024w" sizes="(max-width: 300px) 100vw, 300px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://treeherder.mozilla.org/perf.html#/graphs?series=[mozilla-inbound,4eb0cde5431ee9aeb5eb14512ddb3da6d4702cf0,1]&amp;amp;#038;series=[mozilla-inbound,80cac7ef44b76864458627c574af1a18a425f338,1]&amp;amp;#038;series=[mozilla-inbound,0060252bdfb7632df5877b7594b4d16f1b5ca4c9,1]"&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Just as exciting to me as the feature itself is how it&amp;#8217;s implemented: I added a log parser to treeherder which just picks up a line called &amp;#8220;PERFHERDER_DATA&amp;#8221; in the logs with specially formatted JSON data, and then automatically stores whatever metrics are in there in the database (platform, options, etc. are automatically determined). For example, on Linux:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PERFHERDER_DATA: {"framework": {"name": "build_metrics"}, "suites": [{"subtests": [{"name": "libxul.so", "value": 99030741}], "name": "installer size", "value": 55555785}]}&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This should make it super easy for people to add their own metrics to Perfherder for build and test jobs. We&amp;#8217;ll have to be somewhat careful about how we do this (we don&amp;#8217;t want to add thousands of new series with irrelevant / inconsistent data) but I think there&amp;#8217;s lots of potential here to be able to track things we care about on a per-commit basis. Maybe build times (?).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;More compare view improvements&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I added filtering to the Perfherder compare view and added back links to the graphs view. Filtering should make it easier to highlight particular problematic tests in bug reports, etc. The graphs links shouldn&amp;#8217;t really be necessary, but unfortunately are due to the unreliability of our data &amp;#8212; sometimes you can only see if a particular difference between two revisions is worth paying attention to in the context of the numbers over the last several weeks.&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2015/11/Screen-Shot-2015-11-03-at-5.37.02-PM.png"&gt;&lt;img src="/files/2015/11/Screen-Shot-2015-11-03-at-5.37.02-PM-300x157.png" alt="Screen Shot 2015-11-03 at 5.37.02 PM" width="300" height="157" class="alignnone size-medium wp-image-1275" srcset="/files/2015/11/Screen-Shot-2015-11-03-at-5.37.02-PM-300x157.png 300w, /files/2015/11/Screen-Shot-2015-11-03-at-5.37.02-PM-1024x536.png 1024w" sizes="(max-width: 300px) 100vw, 300px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Miscellaneous&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Even after the &lt;a href="http://wrla.ch/blog/2015/09/perfherder-summer-of-contribution-thoughts/"&gt;summer of contribution&lt;/a&gt; has ended, Mike Ling continues to do great work. Looking at the commit log over the past few weeks, he&amp;#8217;s been responsible for the following fixes and improvements:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1218825"&gt;Bug 1218825&lt;/a&gt;: Can zoom in on perfherder graphs by selecting the main view&lt;/li&gt;
 &lt;li&gt;&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1207309"&gt;Bug 1207309&lt;/a&gt;: Disable &amp;#8216;&amp;lt;&amp;rsquo; button in test chooser if no test selected&lt;/li&gt;
 &lt;li&gt;&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1210503"&gt;Bug 1210503&lt;/a&gt; &amp;#8211; Include non-summary tests in main comparison view&lt;/li&gt;
 &lt;li&gt;&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1153956"&gt;Bug 1153956&lt;/a&gt; &amp;#8211; Persist the selected revision in the url on perfherder (based on earlier work by Akhilesh Pillai)&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Next up&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;My main goal for this quarter is to create a fully functional interface for actually sheriffing performance regressions, to replace &lt;a href="http://alertmanager.allizom.org:8080/alerts.html"&gt;alertmanager&lt;/a&gt;. Work on this has been going well. More soon.&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2015/11/Screen-Shot-2015-11-04-at-10.41.26-AM.png"&gt;&lt;img src="/files/2015/11/Screen-Shot-2015-11-04-at-10.41.26-AM-300x176.png" alt="Screen Shot 2015-11-04 at 10.41.26 AM" width="300" height="176" class="alignnone size-medium wp-image-1280" srcset="/files/2015/11/Screen-Shot-2015-11-04-at-10.41.26-AM-300x176.png 300w, /files/2015/11/Screen-Shot-2015-11-04-at-10.41.26-AM-1024x600.png 1024w, /files/2015/11/Screen-Shot-2015-11-04-at-10.41.26-AM.png 1126w" sizes="(max-width: 300px) 100vw, 300px" /&gt;&lt;/a&gt;&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">The new old Perfherder data model</title>
  <link rel="alternate" href="http://www.example.com/2015/10/the-new-old-perfherder-data-model.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2015-10-the-new-old-perfherder-data-model-html</id>
  <published>2015-10-23T04:00:00Z</published>
  <updated>2015-10-23T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;I spent a good chunk of time last quarter redesigning how &lt;a href="https://wiki.mozilla.org/Auto-tools/Projects/Perfherder"&gt;Perfherder&lt;/a&gt; stores its data internally. Here are some notes on this change, for posterity.&lt;/p&gt;

&lt;p&gt;Perfherder&amp;#8217;s data model is based around two concepts:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;Series signatures: A unique set of properties (platform, test name, suite name, options) that identifies a performance test.&lt;/li&gt;
 &lt;li&gt;Series data: A set of measurements for a series signature, indexed by treeherder push and job information.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;When it was first written, Perfherder stored the second type of data as a JSON-encoded series in a relational (MySQL) database. That is, instead of storing each datum as a row in the database, we would store sequences of them. The assumption was that for the common case (getting a bunch of data to plot on a graph), this would be faster than fetching a bunch of rows and then encoding them as JSON. Unfortunately this wasn&amp;#8217;t really true, and it had some serious drawbacks besides.&lt;/p&gt;

&lt;p&gt;First, the approach&amp;#8217;s performance was awful when it came time to add new data. To avoid needing to decode or download the full stored series when you wanted to render only a small subset of it, we stored the same series multiple times over various time intervals. For example, we stored the series data for one day, one week&amp;#8230; all the way up to one year. You can probably see the problem already: you have to decode and re-encode the same data structure many times for each time interval for every new performance datum you were inserting into the database. The pseudo code looked something like this for each push:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for each platform we're testing talos on:
  for each talos job for the platform:
    for each test suite in the talos job:
      for each subtest in the test suite:
        for each time interval in one year, 90 days, 60 days, ...:
           fetch and decode json series for that time interval from db
           add datapoint to end of series
           re-encode series as json and store in db&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Consider that we have some 6 platforms (android, linux64, osx, winxp, win7, win8), 20ish test suites with potentially dozens of subtests&amp;#8230; and you can see where the problems begin.&lt;/p&gt;

&lt;p&gt;In addition to being slow to write, this was also a pig in terms of disk space consumption. The overhead of JSON (&amp;#8220;{, }&amp;#8221; characters, object properties) really starts to add up when you&amp;#8217;re storing millions of performance measurements. We got around this (sort of) by gzipping the contents of these series, but that still left us with gigantic mysql replay logs as we stored the complete &amp;#8220;transaction&amp;#8221; of replacing each of these series rows thousands of times per day. At one point, we completely ran out of disk space on the treeherder staging instance due to this issue.&lt;/p&gt;

&lt;p&gt;Read performance was also often terrible for many common use cases. The original assumption I mentioned above was wrong: rendering points on a graph is only one use case a system like Perfherder has to handle. We also want to be able to get the set of series values associated with two result sets (to render comparison views) or to look up the data associated with a particular job. We were essentially indexing the performance data only on one single dimension (time) which made these other types of operations unnecessarily complex and slow &amp;#8212; especially as the data you want to look up ages. For example, to look up a two week old comparison between two pushes, you&amp;#8217;d also have to fetch the data for &lt;em&gt;every&lt;/em&gt; subsequent push. That&amp;#8217;s a lot of unnecessary overhead when you&amp;#8217;re rendering a comparison view with 100 or so different performance tests:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2015/08/Screen-Shot-2015-08-07-at-1.57.39-PM.png"&gt;&lt;img src="/files/2015/08/Screen-Shot-2015-08-07-at-1.57.39-PM-300x178.png" alt="Screen Shot 2015-08-07 at 1.57.39 PM" width="300" height="178" class="alignnone size-medium wp-image-1229" srcset="/files/2015/08/Screen-Shot-2015-08-07-at-1.57.39-PM-300x178.png 300w, /files/2015/08/Screen-Shot-2015-08-07-at-1.57.39-PM.png 1003w" sizes="(max-width: 300px) 100vw, 300px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So what&amp;#8217;s the alternative? It&amp;#8217;s actually the most obvious thing: just encode one database row per performance series value and create indexes on each of the properties that we might want to search on (repository, timestamp, job id, push id). Yes, this is a lot of rows (the new database stands at 48 million rows of performance data, and counting) but you know what? MySQL is &lt;em&gt;designed&lt;/em&gt; to handle that sort of load. The current performance data table looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;+----------------+------------------+
| Field          | Type             |
+----------------+------------------+
| id             | int(11)          |
| job_id         | int(10) unsigned |
| result_set_id  | int(10) unsigned |
| value          | double           |
| push_timestamp | datetime(6)      |
| repository_id  | int(11)          | 
| signature_id   | int(11)          | 
+----------------+------------------+&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;MySQL can store each of these structures very efficiently, I haven&amp;#8217;t done the exact calculations, but this is well under 50 bytes per row. Including indexes, the complete set of performance data going back to last year clocks in at 15 gigs. Not bad. And we can examine this data structure across any combination of dimensions we like (push, job, timestamp, repository) making common queries to perfherder very fast.&lt;/p&gt;

&lt;p&gt;What about the initial assumption, that it would be faster to get a series out of the database if it&amp;#8217;s already pre-encoded? Nope, not really. If you have a good index and you&amp;#8217;re only fetching the data you need, the overhead of encoding a bunch of database rows to JSON is pretty minor. From my (remote) location in Toronto, I can fetch 30 days of &lt;a href="https://treeherder.mozilla.org/perf.html#/graphs?timerange=2592000&amp;amp;#038;series=[mozilla-inbound,c233ba1133abbd544002dfbc29d9e63ced42a20e,1]"&gt;tcheck2 data&lt;/a&gt; in 250 ms. Almost certainly most of that is network latency. If the original implementation was faster, it&amp;#8217;s not by a significant amount.&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2015/10/Screen-Shot-2015-10-23-at-1.55.09-PM.png"&gt;&lt;img src="/files/2015/10/Screen-Shot-2015-10-23-at-1.55.09-PM-300x188.png" alt="Screen Shot 2015-10-23 at 1.55.09 PM" width="300" height="188" class="alignnone size-medium wp-image-1259" srcset="/files/2015/10/Screen-Shot-2015-10-23-at-1.55.09-PM-300x188.png 300w, /files/2015/10/Screen-Shot-2015-10-23-at-1.55.09-PM-1024x643.png 1024w" sizes="(max-width: 300px) 100vw, 300px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lesson&lt;/strong&gt;: Sometimes using ancient technologies (SQL) in the most obvious way is the right thing to do. &lt;a href="http://c2.com/xp/DoTheSimplestThingThatCouldPossiblyWork.html"&gt;DoTheSimplestThingThatCouldPossiblyWork&lt;/a&gt;&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Perfherder summer of contribution thoughts</title>
  <link rel="alternate" href="http://www.example.com/2015/09/perfherder-summer-of-contribution-thoughts.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2015-09-perfherder-summer-of-contribution-thoughts-html</id>
  <published>2015-09-29T04:00:00Z</published>
  <updated>2015-09-29T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;A few months ago, Joel Maher &lt;a href="https://elvis314.wordpress.com/2015/05/18/a-team-contribution-opportunity-dashboard-hacker/"&gt;announced the Perfherder summer of contribution&lt;/a&gt;. We wrapped things up there a few weeks ago, so I guess it&amp;#8217;s about time I wrote up a bit about how things went.&lt;/p&gt;

&lt;p&gt;As a reminder, the idea of summer of contribution was to give a set of contributors the opportunity to make a substantial contribution to a project we were working on (in this case, the &lt;a href="https://wiki.mozilla.org/Auto-tools/Projects/Perfherder"&gt;Perfherder&lt;/a&gt; performance sheriffing system). We would ask that they sign up to do 5&amp;ndash;10 hours of work a week for at least 8 weeks. In return, Joel and myself would make ourselves available as mentors to answer questions about the project whenever they ran into trouble.&lt;/p&gt;

&lt;p&gt;To get things rolling, I split off a bunch of work that we felt would be reasonable to do by a contributor into bugs of varying difficulty levels (assigning them the bugzilla whiteboard tag &lt;a href="https://bugzilla.mozilla.org/buglist.cgi?keywords=%20ateam-summer-of-contribution&amp;amp;#038;keywords_type=allwords&amp;amp;#038;list_id=12578272&amp;amp;#038;resolution=---&amp;amp;#038;resolution=FIXED&amp;amp;#038;resolution=INVALID&amp;amp;#038;resolution=WONTFIX&amp;amp;#038;resolution=DUPLICATE&amp;amp;#038;resolution=WORKSFORME&amp;amp;#038;resolution=INCOMPLETE&amp;amp;#038;resolution=SUPPORT&amp;amp;#038;resolution=EXPIRED&amp;amp;#038;resolution=MOVED&amp;amp;#038;query_format=advanced&amp;amp;#038;bug_status=UNCONFIRMED&amp;amp;#038;bug_status=NEW&amp;amp;#038;bug_status=ASSIGNED&amp;amp;#038;bug_status=REOPENED&amp;amp;#038;bug_status=RESOLVED&amp;amp;#038;bug_status=VERIFIED&amp;amp;#038;bug_status=CLOSED"&gt;ateam-summer-of-contribution&lt;/a&gt;). When someone first expressed interest in working on the project, I&amp;#8217;d assign them a relatively easy front end one, just to cover the basics of working with the project (checking out code, making a change, submitting a PR to github). If they made it through that, I&amp;#8217;d go on to assign them slightly harder or more complex tasks which dealt with other parts of the codebase, the nature of which depended on what they wanted to learn more about. Perfherder essentially has two components: a data storage and analysis backend written in Python and Django, and a web-based frontend written in JS and Angular. There was (still is) lots to do on both, which gave contributors lots of choice.&lt;/p&gt;

&lt;p&gt;This system worked pretty well for attracting people. I think we got at least 5 people interested and contributing useful patches within the first couple of weeks. In general I think onboarding went well. Having good documentation for Perfherder / Treeherder on the wiki certainly helped. We had lots of the usual problems getting people familiar with git and submitting proper pull requests: we use a somewhat clumsy combination of bugzilla and github to manage treeherder issues (we &amp;#8220;attach&amp;#8221; PRs to bugs as plaintext), which can be a bit offputting to newcomers. But once they got past these issues, things went relatively smoothly.&lt;/p&gt;

&lt;p&gt;A few weeks in, I set up a fortnightly skype call for people to join and update status and ask questions. This proved to be quite useful: it let me and Joel articulate the higher-level vision for the project to people (which can be difficult to summarize in text) but more importantly it was also a great opportunity for people to ask questions and raise concerns about the project in a free-form, high-bandwidth environment. In general I&amp;#8217;m not a big fan of meetings (especially status report meetings) but I think these were pretty useful. Being able to hear someone else&amp;#8217;s voice definitely goes a long way to establishing trust that you just can&amp;#8217;t get in the same way over email and irc.&lt;/p&gt;

&lt;p&gt;I think our biggest challenge was retention. Due to (understandable) time commitments and constraints only one person (Mike Ling) was really able to stick with it until the end. Still, I&amp;#8217;m pretty happy with that success rate: if you stop and think about it, even a 10-hour a week time investment is a fair bit to ask. Some of the people who didn&amp;#8217;t quite make it were quite awesome, I hope they come back some day.&lt;/p&gt;

&lt;p&gt;&amp;#8212;&lt;/p&gt;

&lt;p&gt;On that note, a special thanks to Mike Ling for sticking with us this long (he&amp;#8217;s still around and doing useful things long after the program ended). He&amp;#8217;s done &lt;a href="https://github.com/mozilla/treeherder/commits/master?author=MikeLing"&gt;some really fantastic work&lt;/a&gt; inside Perfherder and the project is much better for it. I think my two favorite features that he wrote up are the improved test chooser which I talked about a few months ago and a &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1134780"&gt;get related platform / branch&lt;/a&gt; feature which is a big time saver when trying to determine when a performance regression was first introduced.&lt;/p&gt;

&lt;p&gt;I took the time to do a short email interview with him last week. Here&amp;#8217;s what he had to say:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;Tell us a little bit about yourself. Where do you live? What is it you do when not contributing to Perfherder?&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;I&amp;#8217;m a postgraduate student of NanChang HangKong university in China whose major is Internet of things. Actually,there are a lot of things I would like to do when I am AFK, play basketball, video game, reading books and listening music, just name it ; )&lt;/em&gt;&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;How did you find out about the ateam summer of contribution program?&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;well, I remember when I still a new comer of treeherder, I totally don&amp;#8217;t know how to start my contribution. So, I just go to treeherder irc and ask for advice. As I recall, emorley and jfrench talk with me and give me a lot of hits. Then Will (wlach) send me an Email about ateam summer of contribution and perfherder. He told me it&amp;#8217;s a good opportunity to learn more about treeherder and how to work like a team! I almost jump out of bed (I receive that email just before get asleep) and reply with YES. Thank you Will!&lt;/em&gt;&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;What did you find most challenging in the summer of contribution?&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;I think the most challenging thing is I not only need to know how to code but also need to know how treeherder actually work. It&amp;#8217;s a awesome project and there are a ton of things I haven&amp;#8217;t heard before (i.e T-test, regression). So I still have a long way to go before I familiar with it.&lt;/em&gt;&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;What advice would give you to future ateam contributors?&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;The only thing you need to do is bring your question to irc and ask. Do not hesitate to ask for help if you need it! All the people in here are nice and willing to help. Enjoy it!&lt;/em&gt;&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">More Perfherder  updates</title>
  <link rel="alternate" href="http://www.example.com/2015/08/more-perfherder-updates.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2015-08-more-perfherder-updates-html</id>
  <published>2015-08-07T04:00:00Z</published>
  <updated>2015-08-07T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Since my last update, we&amp;#8217;ve been trucking along with improvements to &lt;a href="https://wiki.mozilla.org/Auto-tools/Projects/Perfherder"&gt;Perfherder&lt;/a&gt;, the project for making Firefox performance sheriffing and analysis easier.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Compare visualization improvements&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#8217;ve been spending quite a bit of time trying to fix up the display of information in the compare view, to address feedback from developers and hopefully generally streamline things. &lt;a href="https://blog.mozilla.org/vdjeric/"&gt;Vladan&lt;/a&gt; (from the perf team) referred me to &lt;a href="https://mozillians.org/en-US/u/bwinton/"&gt;Blake Winton&lt;/a&gt;, who provided tons of awesome suggestions on how to present things more concisely.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s an old versus new picture:&lt;/p&gt;

&lt;table&gt;
 &lt;tr&gt;
  &lt;td&gt;&lt;a href="/files/2015/07/Screen-Shot-2015-07-14-at-3.53.20-PM.png"&gt;&lt;img src="/files/2015/07/Screen-Shot-2015-07-14-at-3.53.20-PM-300x206.png" alt="Screen Shot 2015-07-14 at 3.53.20 PM" width="300" height="206" class="alignnone size-medium wp-image-1218" srcset="/files/2015/07/Screen-Shot-2015-07-14-at-3.53.20-PM-300x206.png 300w, /files/2015/07/Screen-Shot-2015-07-14-at-3.53.20-PM.png 980w" sizes="(max-width: 300px) 100vw, 300px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;td&gt;&lt;a href="/files/2015/08/Screen-Shot-2015-08-07-at-1.57.39-PM.png"&gt;&lt;img src="/files/2015/08/Screen-Shot-2015-08-07-at-1.57.39-PM-300x178.png" alt="Screen Shot 2015-08-07 at 1.57.39 PM" width="300" height="178" class="alignnone size-medium wp-image-1229" srcset="/files/2015/08/Screen-Shot-2015-08-07-at-1.57.39-PM-300x178.png 300w, /files/2015/08/Screen-Shot-2015-08-07-at-1.57.39-PM.png 1003w" sizes="(max-width: 300px) 100vw, 300px" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Summary of significant changes in this view:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Removed or consolidated several types of numerical information which were overwhelming or confusing (e.g. presenting both numerical and percentage standard deviation in their own columns).&lt;/li&gt;
 &lt;li&gt;Added tooltips all over the place to explain what&amp;#8217;s being displayed.&lt;/li&gt;
 &lt;li&gt;Highlight more strongly when it appears there aren&amp;#8217;t enough runs to make a definitive determination on whether there was a regression or improvement.&lt;/li&gt;
 &lt;li&gt;Improve display of visual indicator of magnitude of regression/improvement (providing a pseudo-scale showing where the change ranges from 0% &amp;#8211; 20%+).&lt;/li&gt;
 &lt;li&gt;Provide more detail on the two changesets being compared in the header and make it easier to retrigger them (thanks to Mike Ling).&lt;/li&gt;
 &lt;li&gt;Much better and more intuitive error handling when something goes wrong (also thanks to Mike Ling).&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;The point of these changes isn&amp;#8217;t necessarily to make everything &amp;#8220;immediately obvious&amp;#8221; to people. We&amp;#8217;re not building general purpose software here: Perfherder will always be a rather specialized tool which presumes significant domain knowledge on the part of the people using it. However, even for our audience, it turns out that there&amp;#8217;s a lot of room to improve how our presentation: reducing the amount of extraneous noise helps people zero in on the things they really need to care about.&lt;/p&gt;

&lt;p&gt;Special thanks to everyone who took time out of their schedules to provide so much good feedback, in particular &lt;a href="http://avih.github.io/"&gt;Avi Halmachi&lt;/a&gt;, &lt;a href="http://glandium.org/blog/"&gt;Glandium&lt;/a&gt;, and &lt;a href="http://elvis314.wordpress.com/"&gt;Joel Maher&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Of course more suggestions are always welcome. Please &lt;a href="https://treeherder.mozilla.org/perf.html#/comparechooser"&gt;give it a try&lt;/a&gt; and &lt;a href="https://bugzilla.mozilla.org/enter_bug.cgi?product=Tree%20Management&amp;amp;#038;component=Perfherder"&gt;file bugs against the perfherder component&lt;/a&gt; if you find anything you&amp;#8217;d like to see changed or improved.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Getting the word out&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Hammersmith:mozilla-central wlach$ hg push -f try
pushing to ssh://hg.mozilla.org/try
no revisions specified to push; using . to avoid pushing multiple heads
searching for changes
remote: waiting for lock on repository /repo/hg/mozilla/try held by 'hgssh1.dmz.scl3.mozilla.com:8270'
remote: got lock after 4 seconds
remote: adding changesets
remote: adding manifests
remote: adding file changes
remote: added 1 changesets with 1 changes to 1 files
remote: Trying to insert into pushlog.
remote: Inserted into the pushlog db successfully.
remote:
remote: View your change here:
remote:   https://hg.mozilla.org/try/rev/e0aa56fb4ace
remote:
remote: Follow the progress of your build on Treeherder:
remote:   https://treeherder.mozilla.org/#/jobs?repo=try&amp;amp;revision=e0aa56fb4ace
remote:
remote: It looks like this try push has talos jobs. Compare performance against a baseline revision:
remote:   https://treeherder.mozilla.org/perf.html#/comparechooser?newProject=try&amp;amp;newRevision=e0aa56fb4ace&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Try pushes incorporating Talos jobs now automatically link to perfherder&amp;#8217;s compare view, both in the output from mercurial and in the emails the system sends. One of the challenges we&amp;#8217;ve been facing up to this point is just letting developers know that Perfherder &lt;em&gt;exists&lt;/em&gt; and it can help them either avoid or resolve performance regressions. I believe this will help.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data quality and ingestion improvements&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Over the past couple weeks, we&amp;#8217;ve been comparing our regression detection code when run against Graphserver data to Perfherder data. In doing so, we discovered that we&amp;#8217;ve sometimes been using the wrong algorithm (geometric mean) to summarize some of our tests, leading to unexpected and less meaningful results. For example, the v8_7 benchmark uses a custom weighting algorithm for its score, to account for the fact that the things it tests have a particular range of expected values.&lt;/p&gt;

&lt;p&gt;To hopefully prevent this from happening again in the future, we&amp;#8217;ve decided to move the test summarization code out of Perfherder back into Talos (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1184966"&gt;bug 1184966&lt;/a&gt;). This has the additional benefit of creating a stronger connection between the content of the Talos logs and what Perfherder displays in its comparison and graph views, which has thrown people off in the past.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Continuing data challenges&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Having better tools for visualizing this stuff is great, but it also highlights some continuing problems we&amp;#8217;ve had with data quality. It turns out that our automation setup often produces &lt;em&gt;qualitatively different&lt;/em&gt; performance results for the exact same set of data, depending on when and how the tests are run.&lt;/p&gt;

&lt;p&gt;A certain amount of random noise is always expected when running performance tests. As much as we might try to make them uniform, our testing machines and environments are just not 100% identical. That we expect and can deal with: our standard approach is just to retrigger runs, to make sure we get a representative sample of data from our population of machines.&lt;/p&gt;

&lt;p&gt;The problem comes when there&amp;#8217;s a &lt;em&gt;pattern&lt;/em&gt; to the noise: we&amp;#8217;ve already noticed that tests run on the weekends produce different results (see Joel&amp;#8217;s post from a year ago, &lt;a href="https://elvis314.wordpress.com/2014/10/30/a-case-of-the-weekends/"&gt;&amp;#8220;A case of the weekends&amp;#8221;&lt;/a&gt;) but it seems as if there&amp;#8217;s other circumstances where one set of results will be different from another, depending on the time that each set of tests was run. Some tests and platforms (e.g. the a11yr suite, MacOS X 10.10) seem particularly susceptible to this issue.&lt;/p&gt;

&lt;p&gt;We need to find better ways of dealing with this problem, as it can result in a lot of wasted time and energy, for both sheriffs and developers. See for example &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1190877"&gt;bug 1190877&lt;/a&gt;, which concerned a completely spurious regression on the tresize benchmark that was initially blamed on some changes to the media code&amp;#8211; in this case, Joel speculates that the linux64 test machines we use might have changed from under us in some way, but we really don&amp;#8217;t know yet.&lt;/p&gt;

&lt;p&gt;I see two approaches possible here:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;Figure out what&amp;#8217;s causing the same machines to produce qualitatively different result distributions and address that. This is of course the ideal solution, but it requires coordination with other parts of the organization who are likely quite busy and might be hard.&lt;/li&gt;
 &lt;li&gt;Figure out better ways of detecting and managing these sorts of case. I have noticed that the standard deviation inside the results when we have spurious regressions/improvements tends to be higher (see for example &lt;a href="https://treeherder.mozilla.org/perf.html#/compare?originalProject=mozilla-inbound&amp;amp;#038;originalRevision=4d0818791d07&amp;amp;#038;newProject=mozilla-inbound&amp;amp;#038;newRevision=5e130ad70aa7"&gt;this compare view&lt;/a&gt; for the aforementioned &amp;#8220;regression&amp;#8221;). Knowing what we do, maybe there&amp;#8217;s some statistical methods we can use to detect bad data?&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;For now, I&amp;#8217;m leaning towards (2). I don&amp;#8217;t think we&amp;#8217;ll ever completely solve this problem and I think coming up with better approaches to understanding and managing it will pay the largest dividends. Open to other opinions of course!&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Perfherder update</title>
  <link rel="alternate" href="http://www.example.com/2015/07/perfherder-update.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2015-07-perfherder-update-html</id>
  <published>2015-07-14T04:00:00Z</published>
  <updated>2015-07-14T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Haven&amp;#8217;t been doing enough blogging about &lt;a href="https://wiki.mozilla.org/Auto-tools/Projects/Perfherder"&gt;Perfherder&lt;/a&gt; (our project to make Talos and other per-checkin performance data more useful) recently. Let&amp;#8217;s fix that. We&amp;#8217;ve been making some good progress, helped in part by a group of new contributors that joined us through an experimental &amp;#8220;&lt;a href="https://elvis314.wordpress.com/2015/06/09/please-welcome-the-dashboard-hacker-team/"&gt;summer of contribution&lt;/a&gt;&amp;#8221; program.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Comparison mode&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Inspired by Compare Talos, we&amp;#8217;ve designed something similar which hooks into the perfherder backend. This has already gotten some interest: see this post on &lt;a href="https://groups.google.com/d/msg/mozilla.dev.tree-management/IUmMuY8b52A/Asne1cW0I8EJ"&gt;dev.tree-management&lt;/a&gt; and this one on &lt;a href="https://groups.google.com/d/msg/mozilla.dev.platform/PaJFBtvc3Vg/BvX-pFlsAkoJ"&gt;dev.platform&lt;/a&gt;. We&amp;#8217;re working towards building something that will be really useful both for (1) illustrating that the performance regressions we detect are real and (2) helping developers figure out the impact of their changes before they land them.&lt;/p&gt;

&lt;table&gt;
 &lt;tr&gt;
  &lt;td&gt;&lt;a href="/files/2015/07/Screen-Shot-2015-07-14-at-3.54.57-PM.png"&gt;&lt;img src="/files/2015/07/Screen-Shot-2015-07-14-at-3.54.57-PM-300x207.png" alt="Screen Shot 2015-07-14 at 3.54.57 PM" width="300" height="207" class="alignnone size-medium wp-image-1219" srcset="/files/2015/07/Screen-Shot-2015-07-14-at-3.54.57-PM-300x207.png 300w, /files/2015/07/Screen-Shot-2015-07-14-at-3.54.57-PM.png 980w" sizes="(max-width: 300px) 100vw, 300px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;td&gt;&lt;a href="/files/2015/07/Screen-Shot-2015-07-14-at-3.53.20-PM.png"&gt;&lt;img src="/files/2015/07/Screen-Shot-2015-07-14-at-3.53.20-PM-300x206.png" alt="Screen Shot 2015-07-14 at 3.53.20 PM" width="300" height="206" class="alignnone size-medium wp-image-1218" srcset="/files/2015/07/Screen-Shot-2015-07-14-at-3.53.20-PM-300x206.png 300w, /files/2015/07/Screen-Shot-2015-07-14-at-3.53.20-PM.png 980w" sizes="(max-width: 300px) 100vw, 300px" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Most of the initial work was done by &lt;a href="https://elvis314.wordpress.com/"&gt;Joel Maher&lt;/a&gt; with lots of review for aesthetics and correctness by me. Avi Halmachi from the Performance Team also helped out with the &lt;a href="https://en.wikipedia.org/wiki/Student's_t-test"&gt;t-test&lt;/a&gt; model for detecting the confidence that we have that a difference in performance was real. Lately myself and &lt;a href="https://github.com/MikeLing"&gt;Mike Ling&lt;/a&gt; (one of our summer of contribution members) have been working on further improving the interface for usability &amp;#8212; I&amp;#8217;m hopeful that we&amp;#8217;ll soon have something implemented that&amp;#8217;s broadly usable and comprehensible to the Mozilla Firefox and Platform developer community.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Graphs improvements&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Although it&amp;#8217;s received slightly less attention lately than the comparison view above, we&amp;#8217;ve been making steady progress on the graphs view of performance series. Aside from demonstrations and presentations, the primary use case for this is being able to detect visually sustained changes in the result distribution for talos tests, which is often necessary to be able to confirm regressions. Notable recent changes include a much easier way of selecting tests to add to the graph from Mike Ling and more readable/parseable urls from &lt;a href="https://github.com/akhileshpillai"&gt;Akhilesh Pillai&lt;/a&gt; (another summer of contribution participant).&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2015/07/Screen-Shot-2015-07-14-at-4.09.45-PM.png"&gt;&lt;img src="/files/2015/07/Screen-Shot-2015-07-14-at-4.09.45-PM-300x174.png" alt="Screen Shot 2015-07-14 at 4.09.45 PM" width="300" height="174" class="alignnone size-medium wp-image-1221" srcset="/files/2015/07/Screen-Shot-2015-07-14-at-4.09.45-PM-300x174.png 300w, /files/2015/07/Screen-Shot-2015-07-14-at-4.09.45-PM-1024x595.png 1024w, /files/2015/07/Screen-Shot-2015-07-14-at-4.09.45-PM.png 1130w" sizes="(max-width: 300px) 100vw, 300px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Performance alerts&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#8217;ve also been steadily working on making Perfherder generate alerts when there is a significant discontinuity in the performance numbers, similar to what &lt;a href="http://graphs.mozilla.org"&gt;GraphServer&lt;/a&gt; does now. Currently we have an option to generate a static CSV file of these alerts, but the eventual plan is to insert these things into a peristent database. After that&amp;#8217;s done, we can actually work on creating a UI inside Perfherder to replace &lt;a href="http://alertmanager.allizom.org:8080/alerts.html#"&gt;alertmanager&lt;/a&gt; (which currently uses GraphServer data) and start using this thing to sheriff performance regressions &amp;#8212; putting the herder into perfherder.&lt;/p&gt;

&lt;p&gt;As part of this, I&amp;#8217;ve converted the graphserver alert generation code into a standalone python library, which has already proven useful as a component in the &lt;a href="https://hacks.mozilla.org/2015/06/performance-testing-firefox-os-with-raptor/"&gt;Raptor project for FirefoxOS&lt;/a&gt;. Yay modularity and reusability.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Python API&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#8217;ve also been working on creating and improving a &lt;a href="http://treeherder.readthedocs.org/retrieving_data.html#python-client"&gt;python API&lt;/a&gt; to access Treeherder data, which includes Perfherder. This lets you do interesting things, like dynamically run various types of statistical analysis on the data stored in the production instance of Perfherder (no need to ask me for a database dump or other credentials). I&amp;#8217;ve been using this to perform validation of the data we&amp;#8217;re storing and debug various tricky problems. For example, &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1182282"&gt;I found out last week that we were storing up to duplicate 200 entries in each performance series due to double data ingestion&lt;/a&gt; &amp;#8212; oops.&lt;/p&gt;

&lt;p&gt;You can also use this API to dynamically create interesting graphs and visualizations using &lt;a href="http://wrla.ch/blog/2014/04/pycon-2014-impressions-ipython-notebook-is-the-future-more/"&gt;ipython notebook&lt;/a&gt;, here&amp;#8217;s a simple example of me plotting the last 7 days of youtube.com pageload data inline in a notebook:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2015/07/Screen-Shot-2015-07-14-at-4.43.55-PM.png"&gt;&lt;img src="/files/2015/07/Screen-Shot-2015-07-14-at-4.43.55-PM-300x224.png" alt="Screen Shot 2015-07-14 at 4.43.55 PM" width="300" height="224" class="alignnone size-medium wp-image-1224" srcset="/files/2015/07/Screen-Shot-2015-07-14-at-4.43.55-PM-300x224.png 300w, /files/2015/07/Screen-Shot-2015-07-14-at-4.43.55-PM.png 842w" sizes="(max-width: 300px) 100vw, 300px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[&lt;a href="http://nbviewer.ipython.org/url/wrla.ch/blog/wp-content/uploads/2015/07/perfherder-api.ipynb"&gt;original&lt;/a&gt;]&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">PyCon 2015</title>
  <link rel="alternate" href="http://www.example.com/2015/04/pycon-2015.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2015-04-pycon-2015-html</id>
  <published>2015-04-23T04:00:00Z</published>
  <updated>2015-04-23T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;So I went to PyCon 2015. While I didn&amp;#8217;t leave quite as inspired as I did in 2014 (&lt;a href="http://wrla.ch/blog/2014/04/pycon-2014-impressions-ipython-notebook-is-the-future-more/"&gt;when I discovered iPython&lt;/a&gt;), it was a great experience and I learned a ton. Once again, I was incredibly impressed with the organization of the conference and the diversity and quality of the speakers.&lt;/p&gt;

&lt;p&gt;Since Mozilla was nice enough to sponsor my attendance, I figured I should do another round up of notable talks that I went to.&lt;/p&gt;

&lt;p&gt;Technical stuff that was directly relevant to what I work on:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;To ORM or not to ORM (Christine Spang): Useful talk on when using a database ORM (object relational manager) can be helpful and even faster than using a database directly. I feel like there&amp;#8217;s a lot of misinformation and FUD on this topic, so this was refreshing to see. &lt;a href="https://www.youtube.com/watch?v=Sadng6tR7Q4"&gt;video&lt;/a&gt; &lt;a href="https://speakerdeck.com/pycon2015/christine-spang-to-orm-or-not-to-orm"&gt;slides&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;Debugging hard problems (Alex Gaynor): Exactly what it says &amp;#8212; how to figure out what&amp;#8217;s going on when things aren&amp;#8217;t behaving as they should. Great advice and wisdom in this one (hint: take nothing for granted, dive into the source of everything you&amp;#8217;re using!). &lt;a href="https://www.youtube.com/watch?v=ij99SGGEX34"&gt;video&lt;/a&gt; &lt;a href="https://speakerdeck.com/alex/techniques-for-debugging-hard-problems"&gt;slides&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;Python Performance Profiling: The Guts And The Glory (Jesse Jiryu Davis): Quite an entertaining talk on how to properly profile python code. I really liked his systematic and realistic approach &amp;#8212; which also discussed the thought process behind how to do this (hint: again it comes down to understanding what&amp;#8217;s really going on). Unfortunately the video is truncated, but even the first few minutes are useful. &lt;a href="https://www.youtube.com/watch?v=4uJWWXYHxaM"&gt;video&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Non-technical stuff:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;The Ethical Consequences Of Our Collective Activities (Glyph): A talk on the ethical implications of how our software is used. I feel like this is an under-discussed topic &amp;#8212; how can we know that the results of our activity (programming) serves others and does not harm? &lt;a href="https://www.youtube.com/watch?v=uSbKjRRbjZs"&gt;video&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;How our engineering environments are killing diversity (and how we can fix it) (Kate Heddleston): This was a great talk on how to make the environments in which we develop more welcoming to under-represented groups (women, minorities, etc.). This is something I&amp;#8217;ve been thinking a bunch about lately, especially in the context of expanding the community of people working on our projects in Automation &amp;#38; Tools. The talk had some particularly useful advice (to me, anyway) on giving feedback. &lt;a href="https://www.youtube.com/watch?v=kNke_4WOWAU"&gt;video&lt;/a&gt; &lt;a href="https://speakerdeck.com/pycon2015/kate-heddleston-how-our-engineering-environments-are-killing-diversity-and-how-we-can-fix-it"&gt;slides&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;I probably missed out on a bunch of interesting things. If you also went to PyCon, please feel free to add links to your favorite talks in the comments!&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Perfherder update: Summary series drilldown</title>
  <link rel="alternate" href="http://www.example.com/2015/03/perfherder-update-summary-series-drilldown.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2015-03-perfherder-update-summary-series-drilldown-html</id>
  <published>2015-03-27T04:00:00Z</published>
  <updated>2015-03-27T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Just wanted to give another quick Perfherder update. Since the &lt;a href="http://wrla.ch/blog/2015/02/measuring-e10s-vs-non-e10s-performance-with-perfherder/"&gt;last time&lt;/a&gt;, I&amp;#8217;ve added summary series (which is what GraphServer shows you), so we now have (in theory) the best of both worlds when it comes to Talos data: aggregate summaries of the various suites we run (tp5, tart, etc), with the ability to dig into individual results as needed. This kind of analysis wasn&amp;#8217;t possible with Graphserver and I&amp;#8217;m hopeful this will be helpful in tracking down the root causes of Talos regressions more effectively.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s give an example of where this might be useful by showing how it can highlight problems. Recently we tracked a regression in the Customization Animation Tests (CART) suite from the commit in &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1128354"&gt;bug 1128354&lt;/a&gt;. Using &lt;a href="https://mozillians.org/en-US/u/mishravikas/"&gt;Mishra Vikas&lt;/a&gt;&amp;#8216;s new &amp;#8220;highlight revision mode&amp;#8221; in Perfherder (combined with the revision hash when the regression was pushed to inbound), we can quickly zero in on the location of it:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2015/03/Screen-Shot-2015-03-27-at-3.18.28-PM.png"&gt;&lt;img src="/files/2015/03/Screen-Shot-2015-03-27-at-3.18.28-PM-1024x498.png" alt="Screen Shot 2015-03-27 at 3.18.28 PM" width="474" height="230" class="alignnone size-large wp-image-1184" srcset="/files/2015/03/Screen-Shot-2015-03-27-at-3.18.28-PM-300x146.png 300w, /files/2015/03/Screen-Shot-2015-03-27-at-3.18.28-PM-1024x498.png 1024w, /files/2015/03/Screen-Shot-2015-03-27-at-3.18.28-PM.png 1167w" sizes="(max-width: 474px) 100vw, 474px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It does indeed look like things ticked up after this commit for the CART suite, but why? By clicking on the datapoint, you can open up a subtest summary view beneath the graph:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2015/03/Screen-Shot-2015-03-27-at-2.35.25-PM.png"&gt;&lt;img src="/files/2015/03/Screen-Shot-2015-03-27-at-2.35.25-PM.png" alt="Screen Shot 2015-03-27 at 2.35.25 PM" width="936" height="438" class="alignnone size-full wp-image-1175" srcset="/files/2015/03/Screen-Shot-2015-03-27-at-2.35.25-PM-300x140.png 300w, /files/2015/03/Screen-Shot-2015-03-27-at-2.35.25-PM.png 936w" sizes="(max-width: 936px) 100vw, 936px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We see here that it looks like the 3-customize-enter-css.all.TART entry ticked up a bunch. The related test 3-customize-enter-css.half.TART ticked up a bit too. The changes elsewhere look minimal. But is that a trend that holds across the data over time? We can add some of the relevant subtests to the overall graph view to get a closer look:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2015/03/Screen-Shot-2015-03-27-at-2.36.49-PM.png"&gt;&lt;img src="/files/2015/03/Screen-Shot-2015-03-27-at-2.36.49-PM-1024x503.png" alt="Screen Shot 2015-03-27 at 2.36.49 PM" width="474" height="232" class="alignnone size-large wp-image-1176" srcset="/files/2015/03/Screen-Shot-2015-03-27-at-2.36.49-PM-300x147.png 300w, /files/2015/03/Screen-Shot-2015-03-27-at-2.36.49-PM-1024x503.png 1024w, /files/2015/03/Screen-Shot-2015-03-27-at-2.36.49-PM.png 1155w" sizes="(max-width: 474px) 100vw, 474px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As is hopefully obvious, this confirms that the affected subtest continues to hold its higher value while another test just bounces around more or less in the range it was before.&lt;/p&gt;

&lt;p&gt;Hope people find this useful! If you want to play with this yourself, you can access the perfherder UI at &lt;a href="http://treeherder.mozilla.org/perf.html"&gt;http://treeherder.mozilla.org/perf.html&lt;/a&gt;.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Measuring e10s vs. non-e10s performance with Perfherder</title>
  <link rel="alternate" href="http://www.example.com/2015/02/measuring-e10s-vs-non-e10s-performance-with-perfherder.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2015-02-measuring-e10s-vs-non-e10s-performance-with-perfherder-html</id>
  <published>2015-02-18T05:00:00Z</published>
  <updated>2015-02-18T05:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;For the past few months I&amp;#8217;ve been working on a sub-project of Treeherder called Perfherder, which aims to provide a workflow that will let us more easily detect and manage performance regressions in our products (initially just those detected in &lt;a href="https://wiki.mozilla.org/Buildbot/Talos"&gt;Talos&lt;/a&gt;, but there&amp;#8217;s room to expand on that later). This is a long term project and we&amp;#8217;re still sorting out the details of exactly how it will work, but I thought I&amp;#8217;d quickly announce a milestone.&lt;/p&gt;

&lt;p&gt;As a first step, I&amp;#8217;ve been hacking on a graphical user interface to visualize the performance data we&amp;#8217;re now storing inside Treeherder. It&amp;#8217;s pretty bare bones so far, but already it has two features which &lt;a href="http://graphs.mozilla.org"&gt;graphserver&lt;/a&gt; doesn&amp;#8217;t: the ability to view sub-test results (i.e. the page load time for a specific page in the tp5 suite, as opposed to the geometric mean of all of them) and the ability to see results for e10s builds.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s an example, comparing the tp5o 163.com page load times on windows 7 with e10s enabled (and not):&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2015/02/e10s-vs-non-e10s.png"&gt;&lt;img src="/files/2015/02/e10s-vs-non-e10s.png" alt="e10s-vs-non-e10s" width="901" height="489" class="alignnone size-full wp-image-1157" srcset="/files/2015/02/e10s-vs-non-e10s-300x162.png 300w, /files/2015/02/e10s-vs-non-e10s.png 901w" sizes="(max-width: 901px) 100vw, 901px" /&gt;&lt;/a&gt;
 &lt;br /&gt;&lt;a href="https://treeherder.mozilla.org/perf.html#/graphs?timerange=7776000&amp;amp;#038;seriesList=[[%22mozilla-central%22,%22a78a233646c932ee1c56cf27da58b6aaa4eda2c3%22],[%22mozilla-central%22,%228e9323fd7fadb0623ec520a8ccaec2e733f3d501%22]]"&gt;[link]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Green is e10s, red is non-e10s (the legend picture doesn&amp;#8217;t reflect this because we have yet to deploy a fix to &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1130554"&gt;bug 1130554&lt;/a&gt;, but I promise I&amp;#8217;m not lying). As you can see, the gap has been closing (in particular, something landed in mid-January that improved the e10s numbers quite a bit), but page load times are still measurably slower with this feature enabled.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">mozregression updates</title>
  <link rel="alternate" href="http://www.example.com/2015/01/mozregression-updates.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2015-01-mozregression-updates-html</id>
  <published>2015-01-27T05:00:00Z</published>
  <updated>2015-01-27T05:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Lots of movement in &lt;a href="http://mozilla.github.io/mozregression"&gt;mozregression&lt;/a&gt; (a tool for automatically determining when a regression was introduced in Firefox by bisecting builds on ftp.mozilla.org) in the last few months. Here&amp;#8217;s some highlights:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Support for win64 nightly and inbound builds (Kapil Singh, Vaibhav Agarwal)&lt;/li&gt;
 &lt;li&gt;Support for using an http cache to reduce time spent downloading builds (Sam Garrett)&lt;/li&gt;
 &lt;li&gt;Way better logging and printing of remaining time to finish bisection (Julien Pagès)&lt;/li&gt;
 &lt;li&gt;Much improved performance when bisecting inbound (Julien)&lt;/li&gt;
 &lt;li&gt;Support for automatic determination on whether a build is good/bad via a custom script (Julien)&lt;/li&gt;
 &lt;li&gt;Tons of bug fixes and other robustness improvements (me, Sam, Julien, others&amp;#8230;)&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Also thanks to Julien, we have a &lt;a href="http://mozilla.github.io/mozregression/"&gt;spiffy new website&lt;/a&gt; which documents many of these features. If it&amp;#8217;s been a while, be sure to &lt;a href="http://mozilla.github.io/mozregression/install.html"&gt;update your copy of mozregression to the latest version&lt;/a&gt; and check out the site for documentation on how to use the new features described above!&lt;/p&gt;

&lt;p&gt;Thanks to everyone involved (especially Julien) for all the hard work. Hopefully the payoff will be a tool that&amp;#8217;s just that much more useful to Firefox contributors everywhere.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Using Flexbox in web applications</title>
  <link rel="alternate" href="http://www.example.com/2014/09/using-flexbox-in-web-applications.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2014-09-using-flexbox-in-web-applications-html</id>
  <published>2014-09-29T04:00:00Z</published>
  <updated>2014-09-29T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Over last few months, I discovered the joy that is CSS Flexbox, which solves the &amp;#8220;how do I lay out this set of div&amp;#8217;s in horizontally or vertically&amp;#8221;. I&amp;#8217;ve used it in three projects so far:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Centering the timer interface in my &lt;a href="http://wrla.ch/blog/2014/08/a-new-meditation-app/"&gt;meditation app&lt;/a&gt;, so that it scales nicely from a 320&amp;#215;480 FirefoxOS device all the way up to a high definition monitor&lt;/li&gt;
 &lt;li&gt;Laying out the chart / sidebar elements in the &lt;a href="http://eideticker.mozilla.org"&gt;Eideticker dashboard&lt;/a&gt; so that maximum horizontal space is used&lt;/li&gt;
 &lt;li&gt;Fixing various problems in the &lt;a href="http://treeherder.mozilla.org"&gt;Treeherder UI&lt;/a&gt; on smaller screens (see &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1043474"&gt;bug 1043474&lt;/a&gt; and its dependent bugs)&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;When I talk to people about their troubles with CSS, layout comes up really high on the list. Historically, basic layout problems like a panel of vertical buttons have been ridiculously difficult, involving hacks involving floating divs and absolute positioning or &lt;a href="http://layout.jquery-dev.com/"&gt;JavaScript layout libraries&lt;/a&gt;. This is why people write articles entitled &lt;a href="http://uxmag.com/articles/give-and-use-tables"&gt;&amp;#8220;Give up and use tables&amp;#8221;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Flexbox has pretty much put an end to these problems for me. There&amp;#8217;s no longer any need to &amp;#8220;give up and use tables&amp;#8221; because using flexbox is pretty much just *like* using tables for layout, just with more uniform and predictable behaviour. They&amp;#8217;re so great. I think we&amp;#8217;re pretty close to Flexbox being supported across all the major browsers, so it&amp;#8217;s fair to start using them for custom web applications where compatibility with (e.g.) IE8 is not an issue.&lt;/p&gt;

&lt;p&gt;To try and spread the word, I wrote up &lt;a href="https://developer.mozilla.org/en-US/docs/Web/Guide/CSS/Using_flexbox_to_lay_out_web_applications"&gt;a howto article on using flexbox for web applications on MDN&lt;/a&gt;, covering some of the common use cases I mention above. If you&amp;#8217;ve been curious about flexbox but unsure how to use it, please have a look.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">mozregression 0.24</title>
  <link rel="alternate" href="http://www.example.com/2014/09/mozregression-0-24.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2014-09-mozregression-0-24-html</id>
  <published>2014-09-15T04:00:00Z</published>
  <updated>2014-09-15T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;I just released &lt;a href="http://mozilla.github.io/mozregression/"&gt;mozregression&lt;/a&gt; 0.24. This would be a good time to note some of the user-visible fixes / additions that have gone in recently:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;
  &lt;p&gt;Thanks to Sam Garrett, you can now specify a different branch other than inbound to get finer grained regression ranges from. E.g. if you&amp;#8217;re pretty sure a regression occurred on fx-team, you can do something like:   &lt;code&gt;mozregression --inbound-branch fx-team -g 2014-09-13 -b 2014-09-14&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;Fixed a bug where we could get an incorrect regression range (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1059856"&gt;bug 1059856&lt;/a&gt;). Unfortunately the root cause of the bug is still open (it&amp;#8217;s a bit tricky to match mozilla-central commits to that of other branches) but I think this most recent fix should make things work in 99.9% of cases. Let me know if I&amp;#8217;m wrong.&lt;/li&gt;
 &lt;li&gt;Thanks to Julien Pagès, we now download the inbound build metadata in parallel, which speeds up inbound bisection quite significantly&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;If you know a bit of python, contributing to mozregression is a great way to have a high impact on Mozilla. Many platform developers use this project in their day-to-day work, but there&amp;#8217;s still &lt;a href="https://bugzilla.mozilla.org/buglist.cgi?component=mozregression&amp;amp;#038;product=Testing"&gt;lots of room for improvement&lt;/a&gt;.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Hacking on the Treeherder front end: refreshingly easy</title>
  <link rel="alternate" href="http://www.example.com/2014/09/hacking-on-the-treeherder-front-end-refreshingly-easy.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2014-09-hacking-on-the-treeherder-front-end-refreshingly-easy-html</id>
  <published>2014-09-11T04:00:00Z</published>
  <updated>2014-09-11T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Over the past two weeks, I&amp;#8217;ve been working a bit on the &lt;a href="http://treeherder.mozilla.org"&gt;Treeherder&lt;/a&gt; front end (our interface to managing build and test jobs from mercurial changesets), trying to help get things in shape so that the sheriffs can feel comfortable transitioning to it from &lt;a href="https://tbpl.mozilla.org/"&gt;tbpl&lt;/a&gt; by the end of the quarter.&lt;/p&gt;

&lt;p&gt;One thing that has pleasantly surprised me is just how easy it&amp;#8217;s been to get going and be productive. The process looks like this on Linux or Mac:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;&amp;lt;br /&amp;gt;
git clone https://github.com/mozilla/treeherder-ui.git&amp;lt;br /&amp;gt;
cd treeherder-ui/webapp&amp;lt;br /&amp;gt;
./scripts/web-server.js&amp;lt;br /&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Then just load http://localhost:8000 in your favorite web browser (Firefox) and you should be good to go (it will load data from the actually treeherder site). If you want to make modifications to the HTML, Javascript, or CSS just go ahead and do so with your favorite editor and the changes will be immediately reflected.&lt;/p&gt;

&lt;p&gt;We have a fair backlog of issues to get through, many of them related to the front end. If you&amp;#8217;re interested in helping out, please have a look:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://wiki.mozilla.org/Auto-tools/Projects/Treeherder#Bugs_.26_Project_Tracking"&gt;https://wiki.mozilla.org/Auto-tools/Projects/Treeherder#Bugs_.26_Project_Tracking&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If nothing jumps out at you, please drop by irc.mozilla.org #treeherder and we can probably find something for you to work on. We&amp;#8217;re most active during Pacific Time working hours.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">A new meditation app</title>
  <link rel="alternate" href="http://www.example.com/2014/08/a-new-meditation-app.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2014-08-a-new-meditation-app-html</id>
  <published>2014-08-14T04:00:00Z</published>
  <updated>2014-08-14T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;I had some time on my hands two weekends ago and was feeling a bit of an itch to build something, so I decided to do a project I&amp;#8217;ve had in the back of my head for a while: a meditation timer.&lt;/p&gt;

&lt;p&gt;If you&amp;#8217;ve been following this log, you&amp;#8217;d know that &lt;a href="http://wrla.ch/blog/category/meditation/"&gt;meditation&lt;/a&gt; has been a pretty major interest of mine for the past year. The foundation of my practice is a daily round of seated meditation at home, where I have been attempting to follow the breath and generally try to connect with the world for a set period every day (usually varying between 10 and 30 minutes, depending on how much of a rush I&amp;#8217;m in).&lt;/p&gt;

&lt;p&gt;Clock watching is rather distracting while sitting so having a tool to notify you when a certain amount of time has elapsed is quite useful. Writing a smartphone app to do this is an obvious idea, and indeed approximately a zillion of these things have been written for Android and iOS. Unfortunately, most are not very good. Really, I just want something that does this:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;Select a meditation length (somewhere between 10 and 40 minutes).&lt;/li&gt;
 &lt;li&gt;Sound a bell after a short preparation to demarcate the beginning of meditation.&lt;/li&gt;
 &lt;li&gt;While the meditation period is ongoing, do a countdown of the time remaining (not strictly required, but useful for peace of mind in case you&amp;#8217;re wondering whether you&amp;#8217;ve really only sat for 25 minutes).&lt;/li&gt;
 &lt;li&gt;Sound a bell when the meditation ends.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;Yes, meditation can get more complex than that. In Zen practice, for example, sometimes you have several periods of varying length, broken up with kinhin (walking meditation). However, that mostly happens in the context of a formal setting (e.g. &lt;a href="http://en.wikipedia.org/wiki/Zendo"&gt;a Zendo&lt;/a&gt;) where you leave your smartphone at the door. Trying to shoehorn all that into an app needlessly complicates what should be simple.&lt;/p&gt;

&lt;p&gt;Even worse are the apps which &amp;#8220;chart&amp;#8221; your progress or have other gimmicks to connect you to a virtual &amp;#8220;community&amp;#8221; of meditators. I have to say I find that kind of stuff really turns me off. Meditation should be about connecting with reality in a more fundamental way, not charting gamified statistics or interacting online. We already have way too much of that going on elsewhere in our lives without adding even more to it.&lt;/p&gt;

&lt;p&gt;So, you might ask why the alarm feature of most clock apps isn&amp;#8217;t sufficient? Really, it is most of the time. A specialized app can make selecting the interval slightly more convenient and we can preselect an appropriate bell sound up front. It&amp;#8217;s also nice to hear something to demarcate the start of a meditation session. But honestly I didn&amp;#8217;t have much of a reason to write this other than the fact than I could. Outside of work, I&amp;#8217;ve been in a bit of a creative rut lately and felt like I needed to build something, anything and put it out into the world (even if it&amp;#8217;s tiny and only a very incremental improvement over what&amp;#8217;s out there already). So here it is:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2014/08/meditation-timer-screen.png"&gt;&lt;img src="/files/2014/08/meditation-timer-screen.png" alt="meditation-timer-screen" width="320" height="483" class="alignnone size-full wp-image-1089" srcset="/files/2014/08/meditation-timer-screen-198x300.png 198w, /files/2014/08/meditation-timer-screen.png 320w" sizes="(max-width: 320px) 100vw, 320px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The app was written entirely in HTML5 so it should work fine on pretty much any reasonably modern device, desktop or mobile. I tested it on my Nexus 5 (Chrome, Firefox for Android)&lt;sup&gt;&lt;a href="http://wrla.ch/blog/category/meditation/"&gt;1&lt;/a&gt;&lt;/sup&gt;, FirefoxOS Flame, and on my laptop (Chrome, Firefox, Safari). It lives on a &lt;a href="http://meditation.wrla.ch"&gt;subdomain of this site&lt;/a&gt; or you can &lt;a href="https://marketplace.firefox.com/app/meditation/"&gt;grab it from the Firefox Marketplace&lt;/a&gt; if you&amp;#8217;re using some variant of Firefox (OS). The source, such as it is, &lt;a href="http://github.com/wlach/meditation"&gt;can be found&lt;/a&gt; on github.&lt;/p&gt;

&lt;p&gt;I should acknowledge taking some design inspiration from the &lt;a href="http://helloform.com/projects/mind/"&gt;Mind application&lt;/a&gt; for iOS, which has a similarly minimalistic take on things. Check that out too if you have an iPhone or iPad!&lt;/p&gt;

&lt;p&gt;Happy meditating!&lt;/p&gt;

&lt;p&gt;&lt;sup&gt;&lt;a href="http://wrla.ch/blog/category/meditation/"&gt;1&lt;/a&gt;&lt;/sup&gt; Note that there isn&amp;#8217;t a way to inhibit the screen/device from going to sleep with these browsers, which means that you might miss the ending bell. On FirefoxOS, I used the &lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/Navigator.requestWakeLock"&gt;requestWakeLock&lt;/a&gt; API to make sure that doesn&amp;#8217;t happen. I filed &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1054113"&gt;a bug&lt;/a&gt; to get this implemented on Firefox for Android.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Measuring frames per second and animation smoothness with Eideticker</title>
  <link rel="alternate" href="http://www.example.com/2014/07/measuring-frames-per-second-and-animation-smoothness-with-eideticker.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2014-07-measuring-frames-per-second-and-animation-smoothness-with-eideticker-html</id>
  <published>2014-07-07T04:00:00Z</published>
  <updated>2014-07-07T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;&lt;em&gt;[ For more information on the Eideticker software I&amp;#8217;m referring to, see &lt;a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/"&gt;this entry&lt;/a&gt; ]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Just wanted to write up a few notes on using Eideticker to measure animation smoothness, since this is a topic that comes up pretty often and I wind up explaining these things repeatedly. 😉&lt;/p&gt;

&lt;p&gt;When rendering web content, we want the screen to update something like 60 times per second (typical refresh rate of an LCD screen) when an animation or other change is occurring. When this isn&amp;#8217;t happening, there is often a user perception of jank (a.k.a. things not working as they should). Generally we express how well we measure up to this ideal by counting the number of &amp;#8220;frames per second&amp;#8221; that we&amp;#8217;re producing. If you&amp;#8217;re reading this, you&amp;#8217;re probably already familiar with the concept in outline. If you want to know more, you can check out the &lt;a href="http://en.wikipedia.org/wiki/Frame_rate"&gt;wikipedia article&lt;/a&gt; which goes into more detail.&lt;/p&gt;

&lt;p&gt;At an internal level, this concept matches up conceptually with what Gecko is doing. The graphics pipeline produces frames inside graphics memory, which is then sent to the LCD display (whether it be connected to a laptop or a mobile phone) to be viewed. By instrumenting the code, we can see how often this is happening, and whether it is occurring at the right frequency to reach 60 fps. My understanding is that we have at least some code which does exactly this, though I&amp;#8217;m not 100% up to date on how accurate it is.&lt;/p&gt;

&lt;p&gt;But even assuming the best internal system monitoring, Eideticker might still be useful because:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;It is more &amp;#8220;objective&amp;#8221;. This is valuable not only for our internal purposes to validate other automation (sometimes internal instrumentation can be off due to a bug or whatever), but also to &amp;#8220;prove&amp;#8221; to partners that our software has the performance characteristics that we claim.&lt;/li&gt;
 &lt;li&gt;The visual artifacts it leaves behind can be valuable for inspection and debugging. i.e. &lt;a href="http://wrla.ch/blog/2012/09/more-eideticker-happenings-profiling-and-startup-testing/"&gt;you can correlate videos with profiling information&lt;/a&gt;.&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Unfortunately, deriving this sort of information from a video capture is more complicated than you&amp;#8217;d expect.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What does frames per second even mean?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Given a set of N frames captured from the device, the immediate solution when it comes to &amp;#8220;frames per second&amp;#8221; is to just compare frames against each other (e.g. by comparing the value of individual pixels) and then counting the ones that are different as &amp;#8220;unique frames&amp;#8221;. Divide the total number of unique frames by the length of the
 &lt;br /&gt;capture and&amp;#8230; voila? Frames per second? Not quite.&lt;/p&gt;

&lt;p&gt;First off, there&amp;#8217;s the inherent problem that sometimes the expected behaviour of a test is for the screen to be unchanging for a period of time. For example, at the very beginning of a capture (when we are waiting for the input event to be acknowledged) and at the end (when we are waiting for things to settle). Second, it&amp;#8217;s also easy to imagine the display remaining static for a period of time in the middle of a capture (say in between gestures in a multi-part capture). In these cases, there will likely be no observable change on the screen and thus the number of frames counted will be artificially low, skewing the frames per second number down.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Measurement problems&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Ok, so you might not consider that class of problem that big a deal. Maybe we could just not consider the frames at the beginning or end of the capture. And for pauses in the middle&amp;#8230; as long as we get an absolute number at the end, we&amp;#8217;re fine right? That&amp;#8217;s at least enough to let us know that we&amp;#8217;re getting better or worse, assuming that whatever we&amp;#8217;re testing is behaving the same way between runs and we&amp;#8217;re just trying to measure how many frames hit the screen.&lt;/p&gt;

&lt;p&gt;I might agree with you there, but there&amp;#8217;s a further problems that are specific to measuring on-screen performance using a high-speed camera as we are currently with FirefoxOS.&lt;/p&gt;

&lt;p&gt;An LCD updates gradually, and not all at once. Remnants of previous frames will remain on screen long past their interval. Take for example these five frames (sampled at 120fps) from a capture of a pan down in the FirefoxOS Contacts application (&lt;a href="/files/2014/07/movie.webm"&gt;movie&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2014/07/sidebyside.png"&gt;&lt;img src="/files/2014/07/sidebyside-1024x263.png" alt="sidebyside" width="474" height="121" class="alignnone size-large wp-image-1074" srcset="/files/2014/07/sidebyside-300x77.png 300w, /files/2014/07/sidebyside-1024x263.png 1024w" sizes="(max-width: 474px) 100vw, 474px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Note how if you look closely these 5 frames are actually the intersection of *three* seperate frames. One with &amp;#8220;Adam Card&amp;#8221; at the top, another with &amp;#8220;Barbara Bloomquist&amp;#8221; at the top, then another with &amp;#8220;Barbara Bloomquist&amp;#8221; even further up. Between each frame, artifacts of the previous one are clearly visible.&lt;/p&gt;

&lt;p&gt;Plausible sounding solutions:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Try to resolve the original images by distinguishing &amp;#8220;new&amp;#8221; content from ghosting artifacts. Sounds possible, but probably hard? I&amp;#8217;ve tried a number of simplistic techniques (i.e. trying to find times when change is &amp;#8220;peaking&amp;#8221;), but nothing has really worked out very well.&lt;/li&gt;
 &lt;li&gt;Somehow reverse engineering the interface between the graphics chipset and the LCD panel, and writing some kind of custom hardware to &amp;#8220;capture&amp;#8221; the framebuffer as it is being sent from one to the other. Also sounds difficult.&lt;/li&gt;
 &lt;li&gt;Just forget about this problem altogether and only try to capture periods of time in the capture where the image has stayed static for a sustained period of time (i.e. for say 4&amp;ndash;5 frames and up) and we&amp;#8217;re pretty sure it&amp;#8217;s jank.&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Personally the last solution appeals to me the most, although it has the obvious disadvantage of being a &amp;#8220;homebrew&amp;#8221; metric that no one has ever heard of before, which might make it difficult to use to prove that performance is adequate &amp;#8212; the numbers come with a long-winded explanation instead of being something that people immediately understand.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">End of Q2 Eideticker update: Flame tests, future plans</title>
  <link rel="alternate" href="http://www.example.com/2014/06/end-of-q2-eideticker-update-flame-tests-future-plans.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2014-06-end-of-q2-eideticker-update-flame-tests-future-plans-html</id>
  <published>2014-06-27T04:00:00Z</published>
  <updated>2014-06-27T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;&lt;em&gt;[ For more information on the Eideticker software I&amp;#8217;m referring to, see &lt;a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/"&gt;this entry&lt;/a&gt; ]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Just wanted to give an update on where Eideticker is at the end of Q2 2014. The big news is that we&amp;#8217;ve started to run startup tests against the Flame, the results of which are starting to appear on the dashboard:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2014/06/eideticker-contacts-flame.png"&gt;&lt;img src="/files/2014/06/eideticker-contacts-flame.png" alt="eideticker-contacts-flame" width="1002" height="664" class="alignnone size-full wp-image-1062" srcset="/files/2014/06/eideticker-contacts-flame-300x198.png 300w, /files/2014/06/eideticker-contacts-flame.png 1002w" sizes="(max-width: 1002px) 100vw, 1002px" /&gt;&lt;/a&gt; &lt;a href="http://eideticker.mozilla.org/b2g/#/flame/b2g-contacts-startup/timetostableframe"&gt;[link]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It is expected that these tests will provide a useful complement to the &lt;a href="https://datazilla.mozilla.org/b2g/?branch=master&amp;amp;#038;device=flame&amp;amp;#038;range=7&amp;amp;#038;test=cold_load_time&amp;amp;#038;app_list=browser,calendar,camera,clock,contacts,email%20FTU,fm_radio,gallery,marketplace,messages,music,phone,settings,template,usage,video&amp;amp;#038;app=phone&amp;amp;#038;gaia_rev=b8f36518696f3191&amp;amp;#038;gecko_rev=c90b38c47a1d&amp;amp;#038;plot=avg"&gt;existing startup tests&lt;/a&gt; we&amp;#8217;re running with b2gperf, in particular answering the &amp;#8220;is this regression real?&amp;#8221; question.&lt;/p&gt;

&lt;p&gt;Pending work for Q3:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Enable scrolling tests on the Flame. I got these working against the Hamachi &lt;a href="http://wrla.ch/blog/2014/03/its-all-about-the-entropy/"&gt;a few months ago&lt;/a&gt; but because of some weird input issue we&amp;#8217;re seeing we can&amp;#8217;t yet enable them on the Flame. This is being tracked in &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1028824"&gt;bug 1028824&lt;/a&gt;. If anyone has background on the behaviour of the touch screen driver for this device I would appreciate some help.&lt;/li&gt;
 &lt;li&gt;Enable tests for multiple branches on the Flame (currently we&amp;#8217;re only doing master). This is pretty much ready to go (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1017834"&gt;bug 1017834&lt;/a&gt;), just need to land it.&lt;/li&gt;
 &lt;li&gt;Annotate eideticker graphs with internal benchmark information. Eli Perelman of the FirefoxOS performance team has come up with a standard set of on-load events for the Gaia apps (app chrome loaded, app content loaded, &amp;#8230;) that each app will generate, feeding into tools like b2gperf and test-perf. We want to show this information in Eideticker&amp;#8217;s frame-by-frame analysis (&lt;a href="http://eideticker.mozilla.org/b2g/detail.html?id=2b007f8cfd8b11e3923c10ddb19eacac#/framecannyentropies"&gt;example&lt;/a&gt;) so we can verify that the app&amp;#8217;s behaviour is consistent with what it is claimed. This is being tracked in &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1018334"&gt;bug 1018334&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;Re-enable Eideticker for Android and run tests more frequently. Sadly we haven&amp;#8217;t been consistently generating new Eideticker results for Android for the last quarter because of networking issues in the new Mountain View office, where the test rig for those live. One way or another, we want to fix this next quarter and hopefully run tests more frequently against mozilla-inbound (instead of just nightly builds)&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;The above isn&amp;#8217;t an exhaustive list: there&amp;#8217;s much more that we have in mind for the future that&amp;#8217;s not yet scheduled or defined well (e.g. get Eideticker reporting to Treeherder&amp;#8217;s new performance module). If you have any questions or feedback on anything outlined above I&amp;#8217;d love to hear it!&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Managing test manifests: ManifestDestiny -&gt; manifestparser</title>
  <link rel="alternate" href="http://www.example.com/2014/06/managing-test-manifests-manifestdestiny-manifestparser.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2014-06-managing-test-manifests-manifestdestiny-manifestparser-html</id>
  <published>2014-06-11T04:00:00Z</published>
  <updated>2014-06-11T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Just wanted to make a quick announcement that &lt;a href="https://pypi.python.org/pypi/ManifestDestiny"&gt;ManifestDestiny&lt;/a&gt;, the python package we use internally here at Mozilla for declaratively managing lists of tests in &lt;a href="https://developer.mozilla.org/en/docs/Mochitest"&gt;Mochitest&lt;/a&gt; and other places, has been renamed to &lt;a href="https://pypi.python.org/pypi/manifestparser"&gt;manifestparser&lt;/a&gt;. We kept the versioning the same (0.6), so the only thing you should need to change in your python package dependencies is a quick substitution of &amp;#8220;ManifestDestiny&amp;#8221; with &amp;#8220;manifestparser&amp;#8221;. We will keep ManifestDestiny around indefinitely on pypi, but only to make sure old stuff doesn&amp;#8217;t break. New versions of the software will only be released under the name &amp;#8220;manifestparser&amp;#8221;.&lt;/p&gt;

&lt;p&gt;Quick history lesson: &amp;#8220;Manifest destiny&amp;#8221; refers to a philosophy of exceptionalism and expansionism that was widely held by American settlers in the 19th century. The concept is considered offensive by some, as it was used to justify displacing and dispossessing Native Americans. Wikipedia&amp;#8217;s &lt;a href="http://en.wikipedia.org/wiki/Manifest_destiny"&gt;article&lt;/a&gt; on the subject has a good summary if you want to learn more.&lt;/p&gt;

&lt;p&gt;Here at &lt;a href="https://wiki.mozilla.org/Auto-tools"&gt;Mozilla Tools &amp;#38; Automation&lt;/a&gt;, we&amp;#8217;re most interested in creating software that everyone can feel good about depending on, so we agreed to rename it. When I raised this with my peers, there were no objections. I know these things are often the source of much drama in the free software world, but there&amp;#8217;s really none to see here.&lt;/p&gt;

&lt;p&gt;Happy manifest parsing!&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">mozregression: New maintainer, issues tracked in bugzilla</title>
  <link rel="alternate" href="http://www.example.com/2014/05/mozregression-new-maintainer-issues-tracked-in-bugzilla.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2014-05-mozregression-new-maintainer-issues-tracked-in-bugzilla-html</id>
  <published>2014-05-08T04:00:00Z</published>
  <updated>2014-05-08T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Just wanted to give some quick updates on &lt;a href="http://mozilla.github.io/mozregression/"&gt;mozregression&lt;/a&gt;, your favorite regression-finding tool for Firefox:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;I moved all issue tracking in mozregression to bugzilla from github issues. Github unfortunately doesn&amp;#8217;t really scale to handle notifications sensibly when you&amp;#8217;re part of a large organization like Mozilla, which meant many problems were flying past me unseen. &lt;a href="https://bugzilla.mozilla.org/enter_bug.cgi?product=Testing&amp;amp;#038;component=mozregression"&gt;File your new bugs&lt;/a&gt; in bugzilla, they&amp;#8217;re now much more likely to be acted upon.&lt;/li&gt;
 &lt;li&gt;&lt;a href="http://blackrhino.io/"&gt;Sam Garrett&lt;/a&gt; has stepped up to be co-maintainer of the project with me. He&amp;#8217;s been doing a great job whacking out a bunch of bugs and keeping things running reliably, and it was time to give him some recognition and power to keep things moving forward.&lt;/li&gt;
 &lt;li&gt;On that note, I just released mozregression 0.17, which now shows the revision number when running a build (a request from the graphics team, bug &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1007238"&gt;1007238&lt;/a&gt;) and handles respins of nightly builds correctly (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1000422"&gt;bug 1000422&lt;/a&gt;). Both of these were fixed by Sam.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;If you&amp;#8217;re interested in contributing to Mozilla and are somewhat familiar with python, mozregression is a great place to start. The codebase is quite approachable and the impact will be high &amp;#8212; as I&amp;#8217;ve found out over the last few months, people &lt;em&gt;all over&lt;/em&gt; the Mozilla organization (managers, developers, QA &amp;#8230;) use it in the course of their work and it saves tons of their time. A list of currently open bugs is &lt;a href="https://bugzilla.mozilla.org/buglist.cgi?component=mozregression&amp;amp;#038;product=Testing"&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">PyCon 2014 impressions: ipython notebook is the future &amp;#038; more</title>
  <link rel="alternate" href="http://www.example.com/2014/04/pycon-2014-impressions-ipython-notebook-is-the-future-038-more.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2014-04-pycon-2014-impressions-ipython-notebook-is-the-future-038-more-html</id>
  <published>2014-04-22T04:00:00Z</published>
  <updated>2014-04-22T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;This year&amp;#8217;s PyCon US (&lt;a href="http://python.org"&gt;Python&lt;/a&gt; Conference) was in my city of residence (Montr&amp;eacute;al) so I took the opportunity to go and see what was up in the world of the language I use the most at Mozilla. It was pretty great!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ipython&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The highlight for me was learning about the possibilities of &lt;a href="http://ipython.org/"&gt;ipython notebooks&lt;/a&gt;, an absolutely fantastic interactive tool for debugging python in a live browser-based environment. I&amp;#8217;d heard about it before, but it wasn&amp;#8217;t immediately apparent how it would really improve things &amp;#8212; it seemed to be just a less convenient interface to the python console that required me to futz around with my web browser. Watching a few presentations on the topic made me realize how wrong I was. It&amp;#8217;s already changed the way I do work with Eideticker data, for the better.&lt;/p&gt;

&lt;figure id="attachment_1042" style="width: 848px" class="wp-caption alignnone"&gt;[&lt;img src="/files/2014/04/eideticker-ipython.png" alt="Using ipython to analyze some eideticker data" width="848" height="842" class="size-full wp-image-1042" srcset="/files/2014/04/eideticker-ipython-150x150.png 150w, /files/2014/04/eideticker-ipython-300x297.png 300w, /files/2014/04/eideticker-ipython.png 848w" sizes="(max-width: 848px) 100vw, 848px" /&gt;][3]
 &lt;figcaption class="wp-caption-text"&gt;Using ipython to analyze some eideticker data&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;I think the basic premise is really quite simple: a better interface for typing in, experimenting with, and running python code. If you stop and think about it, the modern web interface supports a much richer vocabulary of interactive concepts that the console (or even text editors like emacs): there&amp;#8217;s no reason we shouldn&amp;#8217;t take advantage of it.&lt;/p&gt;

&lt;p&gt;Here are the (IMO) killer features that make it worth using:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;The ability to immediately re-execute a block of code after editing and seeing an error (essentially merging the immediacy of the python console with the permanency / cut &amp;#38; pastability of an actual script)&lt;/li&gt;
 &lt;li&gt;Live-printing out graphs of numerical results using &lt;a href="http://matplotlib.org/"&gt;matplotlib&lt;/a&gt;. ZOMG this is so handy. Especially in conjunction with the live-editing outlined above, there&amp;#8217;s no better tool for fine-tuning mathematical/statistical analysis.&lt;/li&gt;
 &lt;li&gt;The shareability of the results. Any ipython notebook can be saved and then saved to a public website. Many presentations at PyCon 2014, in fact, were done entirely with ipython notebooks. So handy for answering questions like &amp;#8220;how did you get that&amp;#8221;?&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;To learn more about how to use ipython notebooks for data analysis, I highly recommend Julie Evan&amp;#8217;s talk &lt;a href="http://pyvideo.org/video/2657/diving-into-open-data-with-ipython-notebook-pan-0"&gt;Diving into Open Data with IPython Notebook &amp;#38; Pandas&lt;/a&gt;, which you can find on pyvideo.org.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Other Good Talks&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I saw some other good talks at the conference, here are some of them:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="http://pyvideo.org/video/2571/all-your-ducks-in-a-row-data-structures-in-the-s"&gt;All Your Ducks In A Row: Data Structures in the Standard Library and Beyond&lt;/a&gt; &amp;#8211; A useful talk by Brandon Rhoades on the implementation of basic data structures in Python, and how to select the ones to use for optimal performance. It turns out that lists aren&amp;#8217;t the best thing to use for long sequences of numerical data (who knew?)&lt;/li&gt;
 &lt;li&gt;&lt;a href="http://pyvideo.org/video/2627/fast-python-slow-python"&gt;Fast Python, Slow Python&lt;/a&gt; &amp;#8211; An interesting talk by Alex Gaynor about how to write decent performing pure-python code in a single-threaded context. Lots of intelligent stuff about producing robust code that matches your intention and data structures, and caution against doing fancy things in the name of being &amp;#8220;pythonic&amp;#8221; or &amp;#8220;general&amp;#8221;.&lt;/li&gt;
 &lt;li&gt;&lt;a href="http://pyvideo.org/video/2658/analyzing-rap-lyrics-with-python"&gt;Analyzing Rap Lyrics with Python&lt;/a&gt; &amp;#8211; Another data analysis talk, this one about a subject I knew almost nothing about. The best part of it (for me anyway) was learning how the speaker (Julie Lavoie) narrowed her focus in her research to the exact aspects of the problem that would let her answer the question she was interested in (&amp;#8220;Can we automatically find out which rap lyrics are the most sexist?&amp;#8221;) as opposed to interesting problems (&amp;#8220;how can I design the most general scraping library possible?&amp;#8221;) that don&amp;#8217;t answer the question. In my opinion, this ability to focus is one of the key things that seperates successful projects from unsuccessful ones.&lt;/li&gt;&lt;/ul&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Upcoming travels to Japan and Taiwan</title>
  <link rel="alternate" href="http://www.example.com/2014/03/upcoming-travels-to-japan-and-taiwan.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2014-03-upcoming-travels-to-japan-and-taiwan-html</id>
  <published>2014-03-16T04:00:00Z</published>
  <updated>2014-03-16T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Just a quick note that I&amp;#8217;ll shortly be travelling from the frozen land of Montreal, Canada to Japan and Taiwan over the next week, with no particular agenda other than to explore and meet people. If any Mozillians are interested in meeting up for food or drink, and discussion of &lt;a href="https://wiki.mozilla.org/B2G/Performance"&gt;FirefoxOS performance&lt;/a&gt;, &lt;a href="https://wiki.mozilla.org/Project_Eideticker"&gt;Eideticker&lt;/a&gt;, &lt;a href="http://wrla.ch/blog/2014/03/its-all-about-the-entropy/"&gt;entropy&lt;/a&gt; or anything else&amp;#8230; feel free to contact me at &lt;a href="mailto:wrlach@gmail.com"&gt;wrlach@gmail.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Exact itinerary:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Thu Mar 20 &amp;#8211; Sat Mar 22: Tokyo, Japan&lt;/li&gt;
 &lt;li&gt;Sat Mar 22 &amp;#8211; Tue Mar 25: Kyoto, Japan&lt;/li&gt;
 &lt;li&gt;Tue Mar 25 &amp;#8211; Thu Mar 27: Tokyo, Japan&lt;/li&gt;
 &lt;li&gt;Thu Mar 27 &amp;#8211; Sun Mar 30: Taipei, Taiwan&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;I will also be in Taipei the week of the March 31st, though I expect most of my time to be occupied with discussions/activities inside the Taipei office about FirefoxOS performance matters (the Firefox performance team is having a work week there, and I&amp;#8217;m tagging along to talk about / hack on Eideticker and other automation stuff).&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">It&amp;#8217;s all about the entropy</title>
  <link rel="alternate" href="http://www.example.com/2014/03/it-8217-s-all-about-the-entropy.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2014-03-it-8217-s-all-about-the-entropy-html</id>
  <published>2014-03-14T04:00:00Z</published>
  <updated>2014-03-14T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;&lt;em&gt;[ For more information on the Eideticker software I&amp;#8217;m referring to, see &lt;a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/"&gt;this entry&lt;/a&gt; ]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;So recently I&amp;#8217;ve been exploring new and different methods of measuring things that we care about on FirefoxOS &amp;#8212; like startup time or amount of &lt;a href="http://www.masonchang.com/blog/2014/3/2/wow-such-checkerboard"&gt;checkerboarding&lt;/a&gt;. With Android, where we have a mostly clean signal, these measurements were pretty straightforward. Want to measure startup times? Just capture a video of Firefox starting, then compare the frames pixel by pixel to see how much they differ. When the pixels aren&amp;#8217;t that different anymore, we&amp;#8217;re &amp;#8220;done&amp;#8221;. Likewise, to measure checkerboarding we just calculated the areas of the screen where things were not completely drawn yet, frame-by-frame.&lt;/p&gt;

&lt;p&gt;On FirefoxOS, where we&amp;#8217;re using a camera to measure these things, it has not been so simple. I&amp;#8217;ve already discussed this with respect to startup time in a &lt;a href="http://wrla.ch/blog/2013/10/automatically-measuring-startup-load-time-with-eideticker/"&gt;previous post&lt;/a&gt;. One of the ideas I talk about there is &amp;#8220;entropy&amp;#8221; (or the amount of unique information in the frame). It turns out that this is a pretty deep concept, and is useful for even more things than I thought of at the time. Since this is probably a concept that people are going to be thinking/talking about for a while, it&amp;#8217;s worth going into a little more detail about the math behind it.&lt;/p&gt;

&lt;p&gt;The &lt;a href="http://en.wikipedia.org/wiki/Shannon_entropy"&gt;wikipedia article&lt;/a&gt; on information theoretic entropy is a pretty good introduction. You should read it. It all boils down to this formula:&lt;/p&gt;

&lt;p&gt;&lt;img src="/files/2014/03/wikipedia-entropy-formula.png" alt="wikipedia-entropy-formula" width="401" height="37" class="alignnone size-full wp-image-1014" srcset="/files/2014/03/wikipedia-entropy-formula-300x27.png 300w, /files/2014/03/wikipedia-entropy-formula.png 401w" sizes="(max-width: 401px) 100vw, 401px" /&gt;&lt;/p&gt;

&lt;p&gt;You can see this section of the wikipedia article (and the various articles that it links to) if you want to break down where that comes from, but the short answer is that given a set of random samples, the more different values there are, the higher the entropy will be. Look at it from a probabilistic point of view: if you take a random set of data and want to make predictions on what future data will look like. If it is highly random, it will be harder to predict what comes next. Conversely, if it is more uniform it is easier to predict what form it will take.&lt;/p&gt;

&lt;p&gt;Another, possibly more accessible way of thinking about the entropy of a given set of data would be &amp;#8220;how well would it compress?&amp;#8221;. For example, a bitmap image with nothing but black in it could compress very well as there&amp;#8217;s essentially only 1 piece of unique information in it repeated many times &amp;#8212; the black pixel. On the other hand, a bitmap image of completely randomly generated pixels would probably compress very badly, as almost every pixel represents several dimensions of unique information. For all the statistics terminology, etc. that&amp;#8217;s all the above formula is trying to say.&lt;/p&gt;

&lt;p&gt;So we have a model of entropy, now what? For Eideticker, the question is &amp;#8212; how can we break the frame data we&amp;#8217;re gathering down into a form that&amp;#8217;s amenable to this kind of analysis? The approach I took (on the recommendation of &lt;a href="http://brainacle.com/calculating-image-entropy-with-python-how-and-why.html"&gt;this article&lt;/a&gt;) was to create a histogram with 256 bins (representing the number of distinct possibilities in a black &amp;#38; white capture) out of all the pixels in the frame, then run the formula over that. The exact function I wound up using looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def _get_frame_entropy((i, capture, sobelized)):
    frame = capture.get_frame(i, True).astype('float')
    if sobelized:
        frame = ndimage.median_filter(frame, 3)

        dx = ndimage.sobel(frame, 0)  # horizontal derivative
        dy = ndimage.sobel(frame, 1)  # vertical derivative
        frame = numpy.hypot(dx, dy)  # magnitude
        frame *= 255.0 / numpy.max(frame)  # normalize (Q&amp;amp;D)

    histogram = numpy.histogram(frame, bins=256)[0]
    histogram_length = sum(histogram)
    samples_probability = [float(h) / histogram_length for h in histogram]
    entropy = -sum([p * math.log(p, 2) for p in samples_probability if p != 0])

    return entropy

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href="https://github.com/mozilla/eideticker/blob/master/src/videocapture/videocapture/entropy.py#L10"&gt;[Context]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The &amp;#8220;sobelized&amp;#8221; bit allows us to optionally convolve the frame with a sobel filter before running the entropy calculation, which removes most of the data in the capture except for the edges. This is especially useful for FirefoxOS, where the signal has quite a bit of random noise from ambient lighting that artificially inflate the entropy values even in places where there is little actual &amp;#8220;information&amp;#8221;.&lt;/p&gt;

&lt;p&gt;This type of transformation often reveals very interesting information about what&amp;#8217;s going on in an eideticker test. For example, take this video of the user panning down in the contacts app:&lt;/p&gt;

&lt;div style="width: 640px; " class="wp-video"&gt;
 &lt;video class="wp-video-shortcode" id="video-1012-2" width="640" height="917" preload="metadata" controls="controls"&gt;
  &lt;source type="video/webm" src="/files/2014/03/contacts-scrolling-movie.webm?_=2" /&gt;&lt;a href="/files/2014/03/contacts-scrolling-movie.webm"&gt;/files/2014/03/contacts-scrolling-movie.webm&lt;/a&gt;&lt;/video&gt;&lt;/div&gt;

&lt;p&gt;If you graph the entropies of the frame of the capture using the formula above you, you get a graph like this:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2014/03/contacts-scrolling-entropy-graph.png"&gt;&lt;img src="/files/2014/03/contacts-scrolling-entropy-graph.png" alt="contacts scrolling entropy graph" width="933" height="482" class="alignnone size-full wp-image-1022" srcset="/files/2014/03/contacts-scrolling-entropy-graph-300x154.png 300w, /files/2014/03/contacts-scrolling-entropy-graph.png 933w" sizes="(max-width: 933px) 100vw, 933px" /&gt;&lt;/a&gt;
 &lt;br /&gt;&lt;a href="http://eideticker.wrla.ch/b2g/detail.html?id=3f7b7c88a9ed11e380c5f0def1767b24#/framesobelentropies"&gt;[Link to original]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The Y axis represents entropy, as calculated by the code above. There is no inherently &amp;#8220;right&amp;#8221; value for this &amp;#8212; it all depends on the application you&amp;#8217;re testing and what you expect to see displayed on the screen. In general though, higher values are better as it indicates more frames of the capture are &amp;#8220;complete&amp;#8221;.&lt;/p&gt;

&lt;p&gt;The region at the beginning where it is at about 5.0 represents the contacts app with a set of contacts fully displayed (at startup). The &amp;#8220;flat&amp;#8221; regions where the entropy is at roughly 4.25? Those are the areas where the app is &amp;#8220;checkerboarding&amp;#8221; (blanking out waiting for graphics or layout engine to draw contact information). Click through to the original and swipe over the graph to see what I mean.&lt;/p&gt;

&lt;p&gt;It&amp;#8217;s easy to see what a hypothetical ideal end state would be for this capture: a graph with a smooth entropy of about 5.0 (similar to the start state, where all contacts are fully drawn in). We can track our progress towards this goal (or our deviation from it), by watching the eideticker b2g dashboard and seeing if the summation of the entropy values for frames over the entire test increases or decreases over time. If we see it generally increase, that probably means we&amp;#8217;re seeing less checkerboarding in the capture. If we see it decrease, that might mean we&amp;#8217;re now seeing checkerboarding where we weren&amp;#8217;t before.&lt;/p&gt;

&lt;p&gt;It&amp;#8217;s too early to say for sure, but over the past few days the trend has been positive:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2014/03/entropy-levels-climbing.png"&gt;&lt;img src="/files/2014/03/entropy-levels-climbing.png" alt="entropy-levels-climbing" width="822" height="529" class="alignnone size-full wp-image-1025" srcset="/files/2014/03/entropy-levels-climbing-300x193.png 300w, /files/2014/03/entropy-levels-climbing.png 822w" sizes="(max-width: 822px) 100vw, 822px" /&gt;&lt;/a&gt;
 &lt;br /&gt;&lt;a href="http://eideticker.wrla.ch/b2g/#/inari/b2g-contacts-scrolling/overallentropy"&gt;[Link to original]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(note that there were some problems in the way the tests were being run before, so results before the 12th should not be considered valid)&lt;/p&gt;

&lt;p&gt;So one concept, at least two relevant metrics we can measure with it (startup time and checkerboarding). Are there any more? Almost certainly, let&amp;#8217;s find them!&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Eideticker for FirefoxOS: Becoming more useful</title>
  <link rel="alternate" href="http://www.example.com/2014/03/eideticker-for-firefoxos-becoming-more-useful.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2014-03-eideticker-for-firefoxos-becoming-more-useful-html</id>
  <published>2014-03-09T05:00:00Z</published>
  <updated>2014-03-09T05:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;&lt;em&gt;[ For more information on the Eideticker software I&amp;#8217;m referring to, see &lt;a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/"&gt;this entry&lt;/a&gt; ]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Time for a long overdue eideticker-for-firefoxos update. &lt;a href="http://wrla.ch/blog/2013/10/automatically-measuring-startup-load-time-with-eideticker/"&gt;Last time we were here&lt;/a&gt; (almost 5 months ago! man time flies), I was discussing methodologies for measuring startup performance. Since then, &lt;a href="http://blargon7.com/"&gt;Dave Hunt&lt;/a&gt; and myself have been doing lots of work to make Eideticker more robust and useful. Notably, we now have a setup in London running a suite of Eideticker tests on the latest version of FirefoxOS on the Inari on a daily basis, reporting to &lt;a href="http://eideticker.mozilla.org/b2g"&gt;http://eideticker.mozilla.org/b2g&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2014/03/b2g-contacts-startup-dashboard.png"&gt;&lt;img src="/files/2014/03/b2g-contacts-startup-dashboard.png" alt="b2g-contacts-startup-dashboard" width="840" height="601" class="alignnone size-full wp-image-1005" srcset="/files/2014/03/b2g-contacts-startup-dashboard-300x214.png 300w, /files/2014/03/b2g-contacts-startup-dashboard.png 840w" sizes="(max-width: 840px) 100vw, 840px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;There were more than a few false starts with and some of the earlier data is not to be entirely trusted&amp;#8230; but it now seems to be chugging along nicely, hopefully providing startup numbers that provide a useful counterpoint to the &lt;a href="https://datazilla.mozilla.org/b2g"&gt;datazilla startup numbers&lt;/a&gt; we&amp;#8217;ve already been collecting for some time. There still seem to be some minor problems, but in general I am becoming more and more confident in it as time goes on.&lt;/p&gt;

&lt;p&gt;One feature that I am particularly proud of is the detail view, which enables you to see frame-by-frame what&amp;#8217;s going on. Click on any datapoint on the graph, then open up the view that gives an account of what eideticker is measuring. Hover over the graph and you can see what the video looks like at any point in the capture. This not only lets you know that something regressed, but how. For example, in the messages app, you can scan through this view to see exactly when the first message shows up, and what exact state the application is in when Eideticker says it&amp;#8217;s &amp;#8220;done loading&amp;#8221;.&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2014/03/capture-detail-view.png"&gt;&lt;img src="/files/2014/03/capture-detail-view.png" alt="Capture Detail View" width="964" height="843" class="alignnone size-full wp-image-1008" srcset="/files/2014/03/capture-detail-view-300x262.png 300w, /files/2014/03/capture-detail-view.png 964w" sizes="(max-width: 964px) 100vw, 964px" /&gt;&lt;/a&gt;
 &lt;br /&gt;&lt;a href="http://eideticker.wrla.ch/b2g/framediff.html?id=3819a484a6d611e3ab89f0def1767b24"&gt;[link to original]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(apologies for the low quality of the video &amp;#8212; should be fixed with &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=980479"&gt;this bug&lt;/a&gt; next week)&lt;/p&gt;

&lt;p&gt;As it turns out, this view has also proven to be particularly useful when working with the new entropy measurements in Eideticker which I&amp;#8217;ve been using to measure checkerboarding (redraw delay) on FirefoxOS. More on that next week.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">mozregression now supports inbound builds</title>
  <link rel="alternate" href="http://www.example.com/2013/11/mozregression-now-supports-inbound-builds.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2013-11-mozregression-now-supports-inbound-builds-html</id>
  <published>2013-11-28T05:00:00Z</published>
  <updated>2013-11-28T05:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Just wanted to send out a quick note that I recently added inbound support to &lt;a href="http://mozilla.github.io/mozregression/"&gt;mozregression&lt;/a&gt; for desktop builds of Firefox on Windows, Mac, and Linux.&lt;/p&gt;

&lt;p&gt;For the uninitiated, mozregression is an automated tool that lets you bisect through builds of Firefox to find out when a problem was introduced. You give it the last known good date, the last known bad date and off it will go, automatically pulling down builds to test. After each iteration, it will ask you whether this build was good or bad, update the regression range accordingly, and then the cycle repeats until there are no more intermediate builds.&lt;/p&gt;

&lt;p&gt;Previously, it would only use nightlies which meant a one day granularity &amp;#8212; this meant pretty wide regression ranges, made wider in the last year by the fact that so much more is now going into the tree over the course of the day. However, with inbound support (using the new inbound archive) we now have the potential to get a much tighter range, which should be super helpful for developers. Best of all, mozregression doesn&amp;#8217;t require any particularly advanced skills to use which means everyone in the Mozilla community can help out.&lt;/p&gt;

&lt;p&gt;For anyone interested, there&amp;#8217;s quite a bit of scope to improve mozregression to make it do more things (FirefoxOS support, easier installation&amp;#8230;). Feel free to check out &lt;a href="http://github.com/mozilla/mozregression"&gt;the repository&lt;/a&gt;, the &lt;a href="https://github.com/mozilla/mozregression/issues?state=open"&gt;issues list&lt;/a&gt; (I just added &lt;a href="https://github.com/mozilla/mozregression/issues/76"&gt;an easy one&lt;/a&gt; which would make a great first bug) and ask questions on irc.mozilla.org#ateam!&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Automatically measuring startup / load time with Eideticker</title>
  <link rel="alternate" href="http://www.example.com/2013/10/automatically-measuring-startup-load-time-with-eideticker.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2013-10-automatically-measuring-startup-load-time-with-eideticker-html</id>
  <published>2013-10-17T04:00:00Z</published>
  <updated>2013-10-17T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;So we&amp;#8217;ve been using Eideticker to automatically measure startup/pageload times for about a year now on Android, and more recently on FirefoxOS as well (albeit not automatically). This gives us nice and pretty graphs like this:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2013/10/flot-startup-times-gn.png"&gt;&lt;img src="/files/2013/10/flot-startup-times-gn.png" alt="flot-startup-times-gn" width="620" height="568" class="alignnone size-full wp-image-986" srcset="/files/2013/10/flot-startup-times-gn-300x274.png 300w, /files/2013/10/flot-startup-times-gn.png 620w" sizes="(max-width: 620px) 100vw, 620px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Ok, so we&amp;#8217;re generating numbers and graphing them. That&amp;#8217;s great. But what&amp;#8217;s really going on behind the scenes? I&amp;#8217;m glad you asked. The story is a bit different depending on which platform you&amp;#8217;re talking about.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Android&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;On Android we connect Eideticker to the device&amp;#8217;s HDMI out, so we count on a nearly pixel-perfect signal. In practice, it isn&amp;#8217;t quite, but it is within a few RGB values that we can easily filter for. This lets us come up with a pretty good mechanism for determining when a page load or app startup is finished: just compare frames, and say we&amp;#8217;ve &amp;#8220;stopped&amp;#8221; when the pixel differences between frames are negligible (previously defined at 2048 pixels, now 4096 &amp;#8212; see below). Eideticker&amp;#8217;s new frame difference view lets us see how this works. Look at this graph of application startup:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2013/10/frame-difference-android-startup.png"&gt;&lt;img src="/files/2013/10/frame-difference-android-startup.png" alt="frame-difference-android-startup" width="803" height="514" class="alignnone size-full wp-image-973" srcset="/files/2013/10/frame-difference-android-startup-300x192.png 300w, /files/2013/10/frame-difference-android-startup.png 803w" sizes="(max-width: 803px) 100vw, 803px" /&gt;&lt;/a&gt;
 &lt;br /&gt;&lt;a href="http://eideticker.wrla.ch/#/samsung-gn/startup-abouthome-dirty/timetostableframe"&gt;[Link to original]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;What&amp;#8217;s going on here? Well, we see some huge jumps in the beginning. This represents the animated transitions that Android makes as we transition from the SUTAgent application (don&amp;#8217;t ask) to the beginnings of the FirefoxOS browser chrome. You&amp;#8217;ll notice though that there&amp;#8217;s some more changes that come in around the 3 second mark. This is when the site bookmarks are fully loaded. If you load the original page (link above) and swipe your mouse over the graph, you can see what&amp;#8217;s going on for yourself.&lt;/p&gt;

&lt;p&gt;This approach is not completely without problems. It turns out that there is sometimes some minor churn in the display even when the app is for all intents and purposes started. For example, &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=922770"&gt;sometimes the scrollbar fading out of view can result in a significantish pixel value change&lt;/a&gt;, so I recently upped the threshold of pixels that are different from 2048 to 4096. We also recently encountered a &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=926997"&gt;silly problem&lt;/a&gt; with a random automation app displaying &amp;#8220;toasts&amp;#8221; which caused results to artificially spike. More tweaking may still be required. However, on the whole I&amp;#8217;m pretty happy with this solution. It gives useful, undeniably objective results whose meaning is easy to understand.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;FirefoxOS&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;So as mentioned previously, we use a camera on FirefoxOS to record output instead of HDMI output. Pretty unsurprisingly, this is much noisier. See this movie of the contacts app starting and note all the random lighting changes, for example:&lt;/p&gt;

&lt;div style="width: 409px; " class="wp-video"&gt;&lt;!--[if lt IE 9]&gt;&lt;![endif]--&gt;
 &lt;video class="wp-video-shortcode" id="video-972-1" width="409" height="580" preload="metadata" controls="controls"&gt;
  &lt;source type="video/webm" src="/files/2013/10/contacts-b2g-aug30-load-taphomescreen1.webm?_=1" /&gt; &lt;a href="/files/2013/10/contacts-b2g-aug30-load-taphomescreen1.webm"&gt;/files/2013/10/contacts-b2g-aug30-load-taphomescreen1.webm&lt;/a&gt;&lt;/video&gt;&lt;/div&gt;

&lt;p&gt;My experience has been that pixel differences can be so great between visually identical frames on an eideticker capture on these devices that it&amp;#8217;s pretty much impossible to settle on when startup is done using the frame difference method. It&amp;#8217;s of course possible to detect very large scale changes, but the small scale ones (like the contacts actually appearing in the example above) are very hard to distinguish from random differences in the amount of light absorbed by the camera sensor. Tricks like using median filtering (a.k.a. &amp;#8220;blurring&amp;#8221;) help a bit, but not much. Take a look at this graph, for example:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2013/10/plotly-contacts-load-pixeldiff.png"&gt;&lt;img src="/files/2013/10/plotly-contacts-load-pixeldiff.png" alt="plotly-contacts-load-pixeldiff" width="531" height="679" class="alignnone size-full wp-image-980" srcset="/files/2013/10/plotly-contacts-load-pixeldiff-234x300.png 234w, /files/2013/10/plotly-contacts-load-pixeldiff.png 531w" sizes="(max-width: 531px) 100vw, 531px" /&gt;&lt;/a&gt;
 &lt;br /&gt;&lt;a href="https://plot.ly/~WilliamLachance/3"&gt;[Link to original]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You&amp;#8217;ll note that the pixel differences during &amp;#8220;static&amp;#8221; parts of the capture are highly variable. This is because the pixel difference depends heavily on how &amp;#8220;bright&amp;#8221; each frame is: parts of the capture which are black (e.g. a contacts icon with a black background) have a much lower difference between them than parts that are bright (e.g. the contacts screen fully loaded).&lt;/p&gt;

&lt;p&gt;After a day or so of experimenting and research, I settled on an approach which seems to work pretty reliably. Instead of comparing the frames directly, I measure the &lt;a href="http://en.wikipedia.org/wiki/Entropy"&gt;entropy&lt;/a&gt; of the &lt;a href="http://en.wikipedia.org/wiki/Image_histogram"&gt;histogram&lt;/a&gt; of colours used in each frame (essentially just an indication of brightness in this case, see &lt;a href="http://brainacle.com/calculating-image-entropy-with-python-how-and-why.html"&gt;this article&lt;/a&gt; for more on calculating it), then compare that of each frame with the average of the same measure over 5 previous frames (to account for the fact that two frames may be arbitrarily different, but that is unlikely that a sequence of frames will be). This seems to work much better than frame difference in this environment: although there are plenty of minute differences in light absorption in a capture from this camera, the overall color composition stays mostly the same. See this graph:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2013/10/plotly-contacts-load-entropy.png"&gt;&lt;img src="/files/2013/10/plotly-contacts-load-entropy.png" alt="plotly-contacts-load-entropy" width="546" height="674" class="alignnone size-full wp-image-979" srcset="/files/2013/10/plotly-contacts-load-entropy-243x300.png 243w, /files/2013/10/plotly-contacts-load-entropy.png 546w" sizes="(max-width: 546px) 100vw, 546px" /&gt;&lt;/a&gt;
 &lt;br /&gt;&lt;a href="https://plot.ly/~WilliamLachance/5"&gt;[Link to original]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you look closely, you can see some minor variance in the entropy differences depending on the state of the screen, but it&amp;#8217;s not nearly as pronounced as before. In practice, I&amp;#8217;ve been able to get extremely consistent numbers with a reasonable &amp;#8220;threshold&amp;#8221; of &amp;#8220;0.05&amp;#8221;.&lt;/p&gt;

&lt;p&gt;In Eideticker I&amp;#8217;ve tried to steer away from using really complicated math or algorithms to measure things, unless all the alternatives fail. In that sense, I really liked the simplicity of &amp;#8220;pixel differences&amp;#8221; and am not thrilled about having to resort to this: hopefully the concepts in this case (histograms and entropy) are simple enough that most people will be able to understand my methodology, if they care to. Likely I will need to come up with something else for measuring responsiveness and animation smoothness (frames per second), as likely we can&amp;#8217;t count on light composition changing the same way for those cases. My initial thought was to use &lt;a href="http://en.wikipedia.org/wiki/Edge_detection"&gt;edge detection&lt;/a&gt; (which, while somewhat complex to calculate, is at least easy to understand conceptually) but am open to other ideas.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">First Eideticker Responsiveness Tests</title>
  <link rel="alternate" href="http://www.example.com/2013/10/first-eideticker-responsiveness-tests.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2013-10-first-eideticker-responsiveness-tests-html</id>
  <published>2013-10-07T04:00:00Z</published>
  <updated>2013-10-07T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;&lt;em&gt;[ For more information on the Eideticker software I&amp;#8217;m referring to, see &lt;a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/"&gt;this entry&lt;/a&gt; ]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Time for another update on Eideticker. In the last quarter, I&amp;#8217;ve been working on two main items:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;Responsiveness tests (Android / FirefoxOS)&lt;/li&gt;
 &lt;li&gt;Eideticker for FirefoxOS&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;The focus of this post is the responsiveness work. I&amp;#8217;ll talk about Eideticker for FirefoxOS soon.&lt;/p&gt;

&lt;p&gt;So what do I mean by responsiveness? At a high-level, I mean how quickly one sees a response after performing an action on the device. For example, if I perform a swipe gesture to scroll the content down while browsing CNN.com, how long does it take after
 &lt;br /&gt;I start the gesture for the content to &lt;em&gt;visibly&lt;/em&gt; scroll down? If you break it down, there&amp;#8217;s a multi-step process that happens behind the scenes after a user action like this:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2013/10/input-events.png"&gt;&lt;img src="/files/2013/10/input-events.png" alt="input-events" width="880" height="752" class="alignnone size-full wp-image-957" srcset="/files/2013/10/input-events-300x256.png 300w, /files/2013/10/input-events.png 880w" sizes="(max-width: 880px) 100vw, 880px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If anywhere in the steps above, there is a significant delay, the user experience is likely to be bad. Usability research
 &lt;br /&gt;suggests that any lag that is consistently above 100 milliseconds will lead the user to &lt;a href="http://stackoverflow.com/questions/536300/what-is-the-shortest-perceivable-application-response-delay"&gt;perceive things as being laggy&lt;/a&gt;. To keep our users happy, we need to do our bit to make sure that we respond quickly at all levels that we control (just the application layer on Android, but pretty much everything on FirefoxOS). Even if we can&amp;#8217;t complete the work required on our end to completely respond to the user&amp;#8217;s desire, we should at least display something to acknowledge that things have changed.&lt;/p&gt;

&lt;p&gt;But you can&amp;#8217;t improve what you can&amp;#8217;t measure. Fortunately, we have the means to do calculate of the time delta between &lt;em&gt;most&lt;/em&gt; of the steps above. I learned from &lt;a href="http://taras.glek.net/"&gt;Taras Glek&lt;/a&gt; this weekend that it should be &lt;a href="http://hackaday.com/2012/05/04/reaching-out-to-a-touch-screen-with-a-microcontroller/"&gt;possible to simulate&lt;/a&gt; the actual capacitative touch event on a modern touch screen. We can recognize when the hardware event is available to be consumed by userspace by monitoring the `/dev/input` subsystem. And once the event reaches the application (the Android or FirefoxOS application) there&amp;#8217;s no reason we can&amp;#8217;t add instrumentation in all sorts of places to track the processing of both the event and the rendering of the response.&lt;/p&gt;

&lt;p&gt;My working hypothesis is that it&amp;#8217;s application-level latency (i.e. the time between the application receiving the event and being able to act on it) that dominates, so that&amp;#8217;s what I decided to measure. This is purely based on intuition and by no means proven, so we should test this (it would certainly be an interesting exercise!). However, even if it turns out that there are significant problems here, we still care about the other bits of the stack &amp;#8212; there&amp;#8217;s lots of potentially-latency-introducing churn there and the risk of regression in our own code is probably higher than it is elsewhere since it changes so much.&lt;/p&gt;

&lt;p&gt;Last year, I wrote up a tool called &lt;a href="http://wrla.ch/blog/2012/07/the-evolution-of-simulating-events-in-eideticker-from-monkeys-to-orangutns/?utm_source=rss&amp;amp;#038;utm_medium=rss&amp;amp;#038;utm_campaign=the-evolution-of-simulating-events-in-eideticker-from-monkeys-to-orangutns"&gt;Orangutan&lt;/a&gt; that can directly inject input events into an input device on Android or FirefoxOS. It seemed like a fairly straightforward extension of the tool to output timestamps when these events were registered. It was. Then, by &lt;a href="http://wrla.ch/blog/2013/07/simple-command-line-ntp-client-for-android-and-firefoxos/"&gt;synchronizing the time&lt;/a&gt; between the device and the machine doing the capturing, we can then correlate the input timestamps to events. To help visualize what&amp;#8217;s going on, I generated this view:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2013/10/taskjs-framediff-view.png"&gt;&lt;img src="/files/2013/10/taskjs-framediff-view.png" alt="taskjs-framediff-view" width="583" height="418" class="alignnone size-full wp-image-962" srcset="/files/2013/10/taskjs-framediff-view-300x215.png 300w, /files/2013/10/taskjs-framediff-view.png 583w" sizes="(max-width: 583px) 100vw, 583px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="http://eideticker.wrla.ch/framediff-view.html?title=Frame%20Difference%20Scrolling%20on%20taskjs.org%20%282013-10-06%29&amp;amp;#038;video=videos/video-1381129971.63.webm&amp;amp;#038;framediff=framediffs/framediff-1381129990.79.json&amp;amp;#038;actionlog=actionlogs/action-log-1381129990.79.json"&gt;[Link to original]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The X axis in that graph represents time. The Y-axis represents the difference between the frame at that time with the previous one in number of pixels. The &amp;#8220;red&amp;#8221; represents periods in capture when events are ongoing (we use different colours only to
 &lt;br /&gt;distinguish distinct events). &lt;sup&gt;&lt;a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;For a first pass at measuring responsiveness, I decided to measure the time between the first event being initiated and there being a significant frame difference (i.e. an observable response to the action). You can see some preliminary results on the eideticker dashboard:&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2013/10/taskjs-responsiveness.png"&gt;&lt;img src="/files/2013/10/taskjs-responsiveness.png" alt="taskjs-responsiveness" width="637" height="540" class="alignnone size-full wp-image-956" srcset="/files/2013/10/taskjs-responsiveness-300x254.png 300w, /files/2013/10/taskjs-responsiveness.png 637w" sizes="(max-width: 637px) 100vw, 637px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="http://eideticker.mozilla.org/#/samsung-gn/taskjs/timetoresponse"&gt;[Link to original]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The results seem pretty highly variable at first because I was synchronizing time between the device and an external ntp server, rather than the host machine. I believe this is now fixed, hopefully giving us results that will indicate when regressions occur. As time goes by, we may want to craft some special eideticker tests for responsiveness specifically (e.g. a site where there is heavy javascript background processing).&lt;/p&gt;

&lt;p&gt;&lt;sup&gt;&lt;a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/"&gt;1&lt;/a&gt;&lt;/sup&gt; &lt;em&gt;Incidentally, these &amp;#8220;frame difference&amp;#8221; graphs are also quite useful for understanding where and how application startup has regressed in Fennec &amp;#8212; try opening these two startup views side-by-side (before/after a large regression) and spot the difference: &lt;a href="http://eideticker.wrla.ch/framediff-view.html?title=Frame%20Difference%20Startup%20to%20about:home%20%28dirty%20profile%29%20%282013-08-20%29&amp;amp;#038;video=videos/video-1377070981.95.webm&amp;amp;#038;framediff=framediffs/framediff-1377070991.95.json"&gt;[1]&lt;/a&gt; and &lt;a href="http://eideticker.wrla.ch/framediff-view.html?title=Frame%20Difference%20Startup%20to%20about:home%20%28dirty%20profile%29%20%282013-08-23%29&amp;amp;#038;video=videos/video-1377330042.28.webm&amp;amp;#038;framediff=framediffs/framediff-1377330051.67.json"&gt;[2]&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Simple command-line ntp client for Android and FirefoxOS</title>
  <link rel="alternate" href="http://www.example.com/2013/07/simple-command-line-ntp-client-for-android-and-firefoxos.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2013-07-simple-command-line-ntp-client-for-android-and-firefoxos-html</id>
  <published>2013-07-08T04:00:00Z</published>
  <updated>2013-07-08T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Today I did a quick port of Larry Doolittle&amp;#8217;s &lt;a href="http://doolittle.icarus.com/ntpclient/"&gt;ntpclient program&lt;/a&gt; to Android and FirefoxOS. Basically this lets you easily synchronize your device&amp;#8217;s time to that of a central server. Yes, there&amp;#8217;s lots and lots of Android &amp;#8220;applications&amp;#8221; which let you do this, but I wanted to be able to do this from the command line because that&amp;#8217;s how I roll. If you&amp;#8217;re interested, source and instructions are here:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://github.com/wlach/ntpclient-android"&gt;https://github.com/wlach/ntpclient-android&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For those curious, no, I didn&amp;#8217;t just do this for fun. For next quarter, we want to write some Eideticker-based responsiveness tests for FirefoxOS and Android. For example, how long does it take from the time you tap on an icon in the homescreen on FirefoxOS to when the application is fully loaded? Or on Android, how long does it take to see a full list of sites in the awesomebar from the time you tap on the URL field and enter your search term?&lt;/p&gt;

&lt;p&gt;Because an Eideticker test run involves two different machines (a host machine which controls the device and captures video of it in action, as well as the device itself), we need to use timestamps to really understand when and how events are being sent to the device. To do that reliably, we really need some easy way of synchronizing time between two machines (or at least accounting for the difference in their clocks, which amounts to about the same thing). NTP struck me as being the easiest, most standard way of doing this.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Proof of concept Eideticker dashboard for FirefoxOS</title>
  <link rel="alternate" href="http://www.example.com/2013/05/proof-of-concept-eideticker-dashboard-for-firefoxos.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2013-05-proof-of-concept-eideticker-dashboard-for-firefoxos-html</id>
  <published>2013-05-06T04:00:00Z</published>
  <updated>2013-05-06T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;&lt;em&gt;[ For more information on the Eideticker software I&amp;#8217;m referring to, see &lt;a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/"&gt;this entry&lt;/a&gt; ]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I just put up a proof of concept Eideticker dashboard for FirefoxOS &lt;a href="http://eideticker.wrla.ch/b2g"&gt;here&lt;/a&gt;. Right now it has two days worth of data, manually sampled from an Unagi device running b2g18. Right now there are two tests: one the measures the &amp;#8220;speed&amp;#8221; of the contacts application scrolling, another that measures the amount of time it takes for the contacts application to be fully loaded.&lt;/p&gt;

&lt;p&gt;For those not already familiar with it, Eideticker is a benchmarking suite which captures live video data coming from a device and analyzes it to determine performance. This lets us get data which is more representative of actual user experience (as opposed to an oft artificial benchmark). For example, Eideticker measures contacts startup as taking anywhere between 3.5 seconds and 4.5 seconds, versus than the 0.5 to 1 seconds that the &lt;a href="https://datazilla.mozilla.org/b2g/?branch=master&amp;amp;#038;range=7&amp;amp;#038;test=cold_load_time&amp;amp;#038;app_list=contacts&amp;amp;#038;app=contacts&amp;amp;#038;gaia_rev=114bf216de0a19f7&amp;amp;#038;gecko_rev=9c0de2afd22a8476"&gt;existing datazilla benchmarks&lt;/a&gt; show. What accounts for the difference? If you step through an eideticker-captured video, you can see that even though &lt;em&gt;something&lt;/em&gt; appears very quickly, not all the contacts are displayed until the 3.5 second mark. There is a gap between an app being reported as &amp;#8220;loaded&amp;#8221; and it being fully available for use, which we had not been measuring until now.&lt;/p&gt;

&lt;p&gt;At this point, I am most interested in hearing from FirefoxOS developers on new tests that would be interesting and useful to track performance of the system on an ongoing basis. I&amp;#8217;d obviously prefer to focus on things which have been difficult to measure accurately through other means. My setup is rather fiddly right now, but hopefully soon we can get some useful numbers going on an ongoing basis, as we &lt;a href="http://eideticker.wrla.ch"&gt;do already&lt;/a&gt; for Firefox for Android.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Actual useful FirefoxOS Eideticker results at last</title>
  <link rel="alternate" href="http://www.example.com/2013/04/actual-useful-firefoxos-eideticker-results-at-last.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2013-04-actual-useful-firefoxos-eideticker-results-at-last-html</id>
  <published>2013-04-22T04:00:00Z</published>
  <updated>2013-04-22T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Another update on getting &lt;a href="http://wrla.ch/blog/2013/02/eideticker-for-firefoxos/"&gt;Eideticker working with FirefoxOS&lt;/a&gt;. Once again this is sort of high-level, looking forward to writing something more in-depth soon now that we have the basics working.&lt;/p&gt;

&lt;p&gt;I finally got the last kinks out of the rig I was using to capture live video from FirefoxOS phones using the Point Grey devices last week. In order to make things reasonable I had to write some custom code to isolate the actual device screen from the rest of capture and a few other things. The setup looks interesting (reminds me a bit of something out of the War of the Worlds):&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/2013/04/eideticker-pointgrey-mounted.jpg"&gt;&lt;img src="/files/2013/04/eideticker-pointgrey-mounted.jpg" alt="eideticker-pointgrey-mounted" width="512" height="683" class="alignnone size-full wp-image-894" srcset="/files/2013/04/eideticker-pointgrey-mounted-224x300.jpg 224w, /files/2013/04/eideticker-pointgrey-mounted.jpg 512w" sizes="(max-width: 512px) 100vw, 512px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s some example video of a test I wrote up to measure the performance of contacts scrolling performance (measured at a very respectable 44 frames per second, in case you wondering):&lt;/p&gt;

&lt;video src="/files/eideticker/contacts-scrolling-pointgrey.webm" controls="controls"&gt;&lt;/video&gt;

&lt;p&gt;Surprisingly enough, I didn&amp;#8217;t wind up having to write up any code to compensate for a noisy image. Of course there&amp;#8217;s a certain amount of variance in every frame depending on how much light is hitting the camera sensor at any particular moment, but apparently not enough to interfere with getting useful results in the tests I&amp;#8217;ve been running.&lt;/p&gt;

&lt;p&gt;Likely next step: Create some kind of chassis for mounting both the camera and device on a permanent basis (instead of an adhoc one on my desk) so we can start running these sorts of tests on a daily basis, much like we currently do with Android on the &lt;a href="http://eideticker.wrla.ch"&gt;Eideticker Dashboard&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As an aside, I&amp;#8217;ve been really impressed with both the &lt;a href="https://wiki.mozilla.org/Auto-tools/Projects/Marionette"&gt;Marionette&lt;/a&gt; framework and the gaiatests python module that was written up for FirefoxOS. Writing the above test took just 5 minutes &amp;#8212; and &lt;a href="https://github.com/mozilla/eideticker/blob/master/src/tests/b2g/appscrolling/scroll.py"&gt;the code&lt;/a&gt; is quite straightforward. Quite the pleasant change from my various efforts in Android automation.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Eideticker: Limitations in cross-browser performance testing</title>
  <link rel="alternate" href="http://www.example.com/2013/03/eideticker-limitations-in-cross-browser-performance-testing.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2013-03-eideticker-limitations-in-cross-browser-performance-testing-html</id>
  <published>2013-03-20T04:00:00Z</published>
  <updated>2013-03-20T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Last summer I &lt;a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/"&gt;wrote a bit&lt;/a&gt; about using &lt;a href="https://wiki.mozilla.org/Project_Eideticker"&gt;Eideticker&lt;/a&gt; to measure the relative performance of Firefox for Android versus other browsers (Chrome, stock, etc.). At the time I was pretty optimistic about Eideticker&amp;#8217;s usefulness as a truly &amp;#8220;objective&amp;#8221; measure of user experience that would give us a more accurate view of how we compared against the competition than traditional benchmarking suites (which more often than not, measure things that a user will never see normally when browsing the web). Since then, there&amp;#8217;s been some things that I&amp;#8217;ve discovered, as well as some developments in terms of the &amp;#8220;state of the art&amp;#8221; in mobile browsing that have caused me to reconsider that view &amp;#8212; while I haven&amp;#8217;t given up entirely on this concept (and I&amp;#8217;m still very much convinced of eideticker&amp;#8217;s utility as an internal benchmarking tool), there&amp;#8217;s definitely some limitations in terms of what we can do that I&amp;#8217;m not sure how to overcome.&lt;/p&gt;

&lt;p&gt;Essentially, there are currently three different types of Eideticker performance tests:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Animation tests: Measure the smoothness of an animation by comparing frames and seeing how many are different. Currently the only example of this is the &lt;a href="http://eideticker.wrla.ch/#/samsung-gn/clock/fps"&gt;canvas &amp;#8220;clock&amp;#8221; test&lt;/a&gt;, but many others are possible.&lt;/li&gt;
 &lt;li&gt;Startup tests: Measure the amount of time it takes from when the application is launched to when the browser is fully running/available. There are currently two variants of this test in the dashboard, both measure the amount of time taken to fully render Firefox&amp;#8217;s home screen (the only difference between the two is whether the browser profile is fully initialized). The &lt;a href="http://eideticker.wrla.ch/#/samsung-gn/startup-abouthome-dirty/timetostableframe"&gt;dirty profile&lt;/a&gt; benchmark probably most closely resembles what a user would usually experience.&lt;/li&gt;
 &lt;li&gt;Scrolling tests: Measure the amount of undrawn areas when the user is panning a website. Most of the current eideticker tests are of this kind. A good example of this is the &lt;a href="http://eideticker.wrla.ch/#/samsung-gn/taskjs/fps"&gt;taskjs benchmark&lt;/a&gt;.&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;In this blog post, I&amp;#8217;m going to focus on startup and scrolling tests. Animation tests are interesting, but they are also generally the sorts of tests that are easiest to measure in synthetic ways (e.g. by putting a frame counter in your javascript code) and have thus far not been a huge focus for Eideticker development.&lt;/p&gt;

&lt;p&gt;As it turns out, it&amp;#8217;s unfortunately been rather difficult to create truly objective tests which measure the difference between browsers in these two categories. I&amp;#8217;ll go over them in order.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Startup tests&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There are essentially two types of startup tests: one where you measure the amount of time to get to the browser&amp;#8217;s home screen when you explicitly launch the app (e.g. by pressing the Firefox icon in the app chooser), another where you load a web page in a browser from another app (e.g. by clicking on a link in the Twitter application).&lt;/p&gt;

&lt;p&gt;The first is actually fairly easy to test across browsers, although we are not currently. There&amp;#8217;s not really a good reason for that, it was just an oversight, so I filed &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=852744"&gt;bug 852744&lt;/a&gt; to add something like this.&lt;/p&gt;

&lt;p&gt;The second case (startup to the browser&amp;#8217;s homescreen) is a bit more difficult. The problem here is that, in a nutshell, an apples to apples comparison is very difficult if not impossible simply because different browsers do different things when the user presses the application icon. Here&amp;#8217;s what we see with Firefox:&lt;/p&gt;

&lt;p&gt;&lt;img src="/files/eideticker/firefox-startup.png" style="width:25%;" /&gt;&lt;/p&gt;

&lt;p&gt;And here&amp;#8217;s what we see with Chrome:&lt;/p&gt;

&lt;p&gt;&lt;img src="/files/eideticker/chrome-startup.png" style="width:25%;" /&gt;&lt;/p&gt;

&lt;p&gt;And here&amp;#8217;s what we see with the stock browser:&lt;/p&gt;

&lt;p&gt;&lt;img src="/files/eideticker/stock-startup.png" style="width:25%;" /&gt;&lt;/p&gt;

&lt;p&gt;As you can see Chrome and the stock browser are totally different: they try to &amp;#8220;restore&amp;#8221; the browser back to its state from the last time (in Chrome&amp;#8217;s case, I was last visiting taskjs.org, in Stock&amp;#8217;s case, I was just on the homepage).&lt;/p&gt;

&lt;p&gt;Personally I prefer Firefox&amp;#8217;s behaviour (generally I want to browse somewhere new when I press the icon on my phone), but that&amp;#8217;s really beside the point. It&amp;#8217;s possible to hack around what chrome is doing by restoring the profile between sessions to some sort of clean &amp;#8220;new tab&amp;#8221; state, but at that point you&amp;#8217;re not really reproducing a realistic user scenario. Sure, we can draw a comparison, but how valid is it really? It seems to me that the comparison is mostly only useful in a very broad &amp;#8220;how quickly does the user see something useful&amp;#8221; sense.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Panning tests&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I had quite a bit of hope for these initially. They seemed like a place where Eideticker could do something that conventional benchmarking suites couldn&amp;#8217;t, as things like panning a web page are not presently possible to do in JavaScript. The main measure I tried to compare against was something called &amp;#8220;checkerboarding&amp;#8221;, which essentially represents the amount of time that the user waits for the page to redraw when panning around.&lt;/p&gt;

&lt;p&gt;At the time that I wrote these tests, most browsers displayed regions that were not yet drawn while panning using the page background. We figured that it would thus be possible to detect regions of the page which were not yet drawn by looking for the background color while initiating a panning action. I thus hacked up existing web pages to have a magenta background, then wrote some image analysis code to detect regions that were that color (under the assumption that magenta is only rarely seen in webpages). It worked pretty well.&lt;/p&gt;

&lt;p&gt;The world&amp;#8217;s moved on a bit since I wrote that: modern browsers like Chrome and Firefox use something like progressive drawing to display a lower resolution &amp;#8220;tile&amp;#8221; where possible in this case, so the user at least sees something resembling the actual page while panning on a slower device. To see what I mean, try visiting a slow-to-render site like taskjs.org and try panning down quickly. You should see something like this (click to expand):&lt;/p&gt;

&lt;p&gt;&lt;a href="/files/eideticker/firefox-partialy-drawn.png"&gt;&lt;img src="/files/eideticker/firefox-partialy-drawn.png" style="width:50%;" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Unfortunately, while this is certainly a better user experience, it is not so easy to detect and measure. For Firefox, we&amp;#8217;ve disabled this behaviour so that we see the old checkerboard pattern. This is useful for our internal measurements (we can see both if our drawing code as well as our heuristics about when to draw are getting better or worse over time) but it only works for us.&lt;/p&gt;

&lt;p&gt;If anyone has any suggestions on what to do here, let me know as I&amp;#8217;m a bit stuck. There are other metrics we could still compare against (i.e. how smooth is the panning animation aka frames per second?) but these aren&amp;#8217;t nearly as interesting.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Documentation for mozdevice</title>
  <link rel="alternate" href="http://www.example.com/2013/03/documentation-for-mozdevice.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2013-03-documentation-for-mozdevice-html</id>
  <published>2013-03-11T04:00:00Z</published>
  <updated>2013-03-11T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Just wanted to give a quick heads up that as part of the ateam&amp;#8217;s ongoing effort to improve the documentation of our automated testing infrastructure, we now have &lt;a href="https://mozbase.readthedocs.org/en/latest/mozdevice.html"&gt;online documentation&lt;/a&gt; for mozdevice, the python library we use for interacting with Android- and FirefoxOS-based devices in automated testing.&lt;/p&gt;

&lt;p&gt;Mozdevice is used in pretty much every one of our testing frameworks that has mobile support, including mochitest, reftest, &lt;a href="https://wiki.mozilla.org/Buildbot/Talos"&gt;talos&lt;/a&gt;, &lt;a href="https://github.com/mozilla/autophone"&gt;autophone&lt;/a&gt;, and &lt;a href="https://wiki.mozilla.org/Project_Eideticker"&gt;eideticker&lt;/a&gt;. Additionally, mozdevice is used by release engineering to clean up, monitor, and otherwise manage 
 &lt;strike&gt;our hundred-odd&lt;/strike&gt; the 1200*** tegra and panda development boards that we use in &lt;a href="http://tbpl.mozilla.org"&gt;tbpl&lt;/a&gt;. See &lt;a href="https://hg.mozilla.org/build/tools/file/tip/sut_tools"&gt;sut_tools&lt;/a&gt; (old, buildbot-based, what we currently use) and &lt;a href="https://github.com/mozilla/mozpool"&gt;mozpool&lt;/a&gt; (the new and shiny future).&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Thanks to Dustin Mitchell for the correction.&lt;/li&gt;&lt;/ul&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Follow up on &amp;#8220;Finding a Camera for Eideticker&amp;#8221;</title>
  <link rel="alternate" href="http://www.example.com/2013/03/follow-up-on-8220-finding-a-camera-for-eideticker-8221.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2013-03-follow-up-on-8220-finding-a-camera-for-eideticker-8221-html</id>
  <published>2013-03-08T05:00:00Z</published>
  <updated>2013-03-08T05:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Quick update on my &lt;a href="http://wrla.ch/blog/2013/02/finding-a-camera-for-eideticker/"&gt;last post&lt;/a&gt; about finding some kind of camera suitable for use in automated performance testing of fennec and b2g with eideticker. Shortly after I wrote that, I found out about a company called &lt;a href="http://ptgrey.com"&gt;Point Grey Research&lt;/a&gt; which manufactures custom web cameras intended for exactly the sorts of things we&amp;#8217;re doing. Initial results with the &lt;a href="http://ww2.ptgrey.com/USB3/Flea3"&gt;Flea3 camera&lt;/a&gt; I ordered from them are quite promising:&lt;/p&gt;

&lt;video width="400px" src="/files/eideticker/pointgrey-taskjs.webm" controls="controls"&gt;&lt;/video&gt;

&lt;p&gt;(the actual capture is even higher quality than that would suggest&amp;#8230; some detail is lost in the conversion to webm)&lt;/p&gt;

&lt;p&gt;There seems to be some sort of issue with the white balance in that capture which is causing a flashing effect (I suspect this just involves flipping some kind of software setting&amp;#8230; still trying to grok their SDK), and we&amp;#8217;ll need to create some sort of enclosure for the setup so ambient light doesn&amp;#8217;t interfere with the capture&amp;#8230; but overall I&amp;#8217;m pretty optimistic about this baby. 60 frames per second, very high resolution (1280&amp;#215;960), no issues with HDMI (since it&amp;#8217;s just a USB camera), relatively inexpensive.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Finding a camera for Eideticker</title>
  <link rel="alternate" href="http://www.example.com/2013/02/finding-a-camera-for-eideticker.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2013-02-finding-a-camera-for-eideticker-html</id>
  <published>2013-02-19T05:00:00Z</published>
  <updated>2013-02-19T05:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;&lt;em&gt;[ For more information on the Eideticker software I&amp;#8217;m referring to, see &lt;a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/"&gt;this entry&lt;/a&gt; ]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Ok, so as I mentioned &lt;a href="http://wrla.ch/blog/2013/02/eideticker-for-firefoxos/"&gt;last time&lt;/a&gt; I&amp;#8217;ve been looking into making Eideticker work for devices without native HDMI output by capturing their output with some kind of camera. So far I&amp;#8217;ve tried four different DSLRs for this task, which have all been inadequate for different reasons. I was originally just going to write an email about this to a few concerned parties, but then figured I may as well structure it into a blog post. Maybe someone will find it useful (or better yet, have some ideas.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Elmo MO&amp;ndash;1&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is the device I mentioned last time. Easy to set up, plays nicely with the Decklink capture card we&amp;#8217;re using for Eideticker. It seemed almost perfect, except for that:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;The image quality is &lt;em&gt;really&lt;/em&gt; bad (beaten even by $200 standard digital camera). Tons of noise, picture quality really bad. Not *necessarily* a deal breaker, but it still sucks.&lt;/li&gt;
 &lt;li&gt;More importantly, there seems to be no way of turning off the auto white balance adjustment. This makes automated image analysis impossible if the picture changes, as is highlighted in this video:   
  &lt;video width="400px" src="/files/eideticker/elmo-white-balance-problem.webm" controls="controls"&gt;&lt;/video&gt;&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Canon Rebel T4i&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is the first camera that was recommended to me at the camera shop I&amp;#8217;ve been going to. It does have an HDMI output signal, but it&amp;#8217;s not &amp;#8220;clean&amp;#8221;. This &lt;a href="http://www.hireacamera.com/blog/index.asp?post=canon-eos-650d--hdmi-explained"&gt;blog post&lt;/a&gt; explains the details better than I could. Next.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Nikon D600&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Supposedly does native clean 720p output, but unfortunately the &lt;a href="http://vimeo.com/49952287"&gt;output is in a &amp;#8220;box&amp;#8221;&lt;/a&gt; so isn&amp;#8217;t recognized by the Decklink cards that we&amp;#8217;re using (which insist on a full 1280&amp;#215;720 HDMI signal to work). Next.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Nikon D800&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It is possible to configure this one to not put a box around the output, so the Decklink card does recognize it. Except that the camera shuts off the HDMI signal whenever the input parameters change on the card or the signal input is turned on, which essentially makes it useless for Eideticker (this happens every time we start the Eideticker harness). Quite a shame, as the HDMI signal is quite nice otherwise.&lt;/p&gt;

&lt;p&gt;&amp;#8212;&lt;/p&gt;

&lt;p&gt;To be clear, with the exception of the Elmo all the devices above seem like fine cameras, and should more than do for manual captures of B2G or Android phones (which is something we probably want to do anyway). But for Eideticker, we need something that works in automation, and none of the above fit the bill. I guess I could explore using a &amp;#8220;real&amp;#8221; video camera as opposed to a DSLR acting like one, though I suspect I might run into some of the same sorts of issues depending on how the HDMI output of those devices behaves.&lt;/p&gt;

&lt;p&gt;Part of me wonders whether a custom solution wouldn&amp;#8217;t work better. How complicated could it be to construct your own digital camera anyway? 😉 Hook up a fancy camera sensor to a &lt;a href="http://pandaboard.org"&gt;pandaboard&lt;/a&gt;, get it to output through the HDMI port, and then we&amp;#8217;re set? Or better yet, maybe just get a fancy webcam like the &lt;a href="http://en.wikipedia.org/wiki/PlayStation_Eye"&gt;Playstation Eye&lt;/a&gt; and hook it up directly to a computer? That would eliminate the need for our expensive video capture card setup altogether.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Eideticker for FirefoxOS</title>
  <link rel="alternate" href="http://www.example.com/2013/02/eideticker-for-firefoxos.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2013-02-eideticker-for-firefoxos-html</id>
  <published>2013-02-01T05:00:00Z</published>
  <updated>2013-02-01T05:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;&lt;em&gt;[ For more information on the Eideticker software I&amp;#8217;m referring to, see &lt;a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/"&gt;this entry&lt;/a&gt; ]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s a long overdue update on where we&amp;#8217;re at with Eideticker for FirefoxOS. While we&amp;#8217;ve had a good amount of success getting &lt;a href="http://eideticker.wrla.ch"&gt;useful, actionable data&lt;/a&gt; out of Eideticker for Android, so far we haven&amp;#8217;t been able to replicate that success for FirefoxOS. This is not for lack of trying: first, &lt;a href="http://nakubu.com/"&gt;Malini Das&lt;/a&gt; and then me have been working at it since summer 2012.&lt;/p&gt;

&lt;p&gt;When it comes right down to it, instrumenting Eideticker for B2G is just a whole lot more complex. On Android, we could take the operating system (including support for all the things we needed, like HDMI capture) as a given. The only tricky part was instrumenting the capture so the right things happened at the right moment. With FirefoxOS, we need to run these tests on entire builds of a whole operating system which was constantly changing. Not nearly as simple. That said, I&amp;#8217;m starting to see light at the end of the tunnel.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Platforms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We initially selected the &lt;a href="http://pandaboard.org"&gt;pandaboard&lt;/a&gt; as the main device to use for eideticker testing, for two reasons. First, it&amp;#8217;s the same hardware platform we&amp;#8217;re targeting for other b2g testing in tbpl (mochitest, reftest, etc.), and is the platform we&amp;#8217;re using for running Gaia UI tests. Second, unlike every other device that we&amp;#8217;re prototyping FirefoxOS on (to my knowledge), it has HDMI-out capability, so we can directly interface it with the Eideticker video capture setup.&lt;/p&gt;

&lt;p&gt;However, the panda also has some serious shortcomings. First, it&amp;#8217;s obviously not a platform we&amp;#8217;re shipping, so the performance we&amp;#8217;re seeing from it is subject to different factors that we might not see with a phone actually shipped to users. For the same reason, we&amp;#8217;ve had many problems getting B2G running reliably on it, as it&amp;#8217;s not something most developers have been hacking on a day to day basis. Thanks to the heroic efforts of Thomas Zimmerman, we&amp;#8217;ve mostly got things working ok now, but it was a fairly long road to get here (several months last fall).&lt;/p&gt;

&lt;p&gt;More recently, we became aware of something called an &lt;a href="http://www.elmousa.com/"&gt;Elmo&lt;/a&gt; which might let us combine
 &lt;br /&gt;the best of both worlds. An elmo is really just a tiny mounted video camera with a bunch of outputs, and is I understand most commonly used to project documents in a classroom/presentation setting. However, it seems to do a great job of capturing mobile phones in action as well.&lt;/p&gt;

&lt;video width="400px" src="/files/eideticker/startup-test-elmo.webm" controls="controls"&gt;&lt;/video&gt;

&lt;p&gt;The nice thing about using an external camera for the video capture part of eideticker is that we are no longer limited to devices with HDMI out &amp;#8212; we can run the standard set of automated tests on ANYTHING. We&amp;#8217;ve already used this to some success in getting some videos of FirefoxOS startup times versus Android on the Unagi (a development phone that we&amp;#8217;re using internally) for manual analysis. Automating this process may be trickier because of the fact that the video capture is no longer &amp;#8220;perfect&amp;#8221;, but we may be able to work around that (more discussion about this later).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;FirefoxOS web page tests&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;These are the same tests we run on Android. They should give us an idea of roughly where our performance when browsing / panning web sites like CNN. So far, I&amp;#8217;ve only run these tests on the Pandaboard and they are INCREDIBLY slow (like 1&amp;ndash;3 frames per second when scrolling). So much so that I have to think there is something broken about our hardware acceleration on this platform.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;FirefoxOS application tests&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;These are some new tests written in a framework that allows you to script arbitrary interactions in FirefoxOS, like launching applications or opening the task switcher.&lt;/p&gt;

&lt;p&gt;I&amp;#8217;m pretty happy with this. It seems to work well. The only problems I&amp;#8217;m seeing with this is with the platform we&amp;#8217;re running these tests on. With the pandaboard, applications look weird (since the screen resolution doesn&amp;#8217;t remotely resemble the 320&amp;#215;480 resolution on our current devices) and performance is abysmal. Take, for example, this capture of application switching performance, which operates only at roughly 3&amp;ndash;4 fps:&lt;/p&gt;

&lt;video width="400px" src="/files/eideticker/b2g-appswitching-video.webm" controls="controls"&gt;&lt;/video&gt;

&lt;p&gt;&lt;strong&gt;So what now?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#8217;m not 100% sure yet (partly it will depend on what others say as well as my own investigation), but I have a feeling that capturing video of real devices running FirefoxOS using the Elmo is the way forward. First, the hardware and driver situation will be much more representative of what we&amp;#8217;ll actually be shipping to users. Second, we can flash new builds of FirefoxOS onto them automatically, unlike the pandaboards where you currently either need to manually flash and reset (a time consuming and error prone process) or set up an instance of &lt;a href="https://github.com/djmitche/mozpool"&gt;mozpool&lt;/a&gt; (which I understand is quite complicated).&lt;/p&gt;

&lt;p&gt;The main use case I see with eideticker-on-panda would be where we wanted to run a suite of tests on checkin (in tbpl-like fashion) and we&amp;#8217;d need to scale to many devices. While cool, this sounds like an expensive project (both in terms of time and hardware) and I think we&amp;#8217;d do better with getting something slightly smaller-scale running first.&lt;/p&gt;

&lt;p&gt;So, the real question is whether or not the capture produced by the Elmo is amenable to the same analysis that we do on the raw HDMI output. At the very least, some of eideticker&amp;#8217;s image analysis code will have to be adapted to handle a much &amp;#8220;noisier&amp;#8221; capture. As opposed to capturing the raw HDMI signal, we now have to deal with the real world and its irritating fluctuations in ambient light levels and all that the rest. I have no doubt it is *possible* to compensate for this (after all this is what the human eye/brain does all the time), but the question is how much work it will be. Can&amp;#8217;t speak for anyone else at Mozilla, but I&amp;#8217;m not sure if I really have the time to start a Ph.D-level research project in computational vision. 😉 I&amp;#8217;m optimistic that won&amp;#8217;t be necessary, but we&amp;#8217;ll just have to wait and see.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Using the dm utility to interact with Android or FirefoxOS devices</title>
  <link rel="alternate" href="http://www.example.com/2012/10/using-the-dm-utility-to-interact-with-android-or-firefoxos-devices.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2012-10-using-the-dm-utility-to-interact-with-android-or-firefoxos-devices-html</id>
  <published>2012-10-18T04:00:00Z</published>
  <updated>2012-10-18T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;I promised a few people I&amp;#8217;d blog about this, so here you go.&lt;/p&gt;

&lt;p&gt;To help with the business of making Android or FirefoxOS devices do our bidding, &lt;a href="https://wiki.mozilla.org/Auto-tools"&gt;Mozilla Automation &amp;#38; Tools&lt;/a&gt; developed a Python library called &lt;a href="https://github.com/mozilla/mozbase/tree/master/mozdevice"&gt;mozdevice&lt;/a&gt; which allows you to control these devices either using the Android Debug Bridge protocol (which is actually not Android specific: FirefoxOS devices use it too) or the &lt;a href="https://wiki.mozilla.org/Auto-tools/Projects/SUTAgent"&gt;System Under Test protocol&lt;/a&gt; (a Mozilla-specific thing).&lt;/p&gt;

&lt;p&gt;Anyone familiar with debugging these devices is doubtless familiar with adb, which provides a command line interface that allows you to push/pull files, run a shell, etc. To help test our python code (as well as expand the scope of what&amp;#8217;s possible on the command line), I created a similar utility a few months ago called &amp;#8220;dm&amp;#8221; which provides a command-line interface to the aforementioned mozdevice code. It&amp;#8217;s shipped as part of mozdevice, and testing it out is pretty simple if you have virtualenv installed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;virtualenv mozdevice
cd mozdevice
./bin/pip install mozdevice
source bin/activate
dm --help&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I generally use this utility for two things:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;
  &lt;p&gt;Testing out mozdevice code. For example, today we discovered an (unfortunate) &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=803177"&gt;bug&lt;/a&gt; in devicemanagerADB&amp;#8217;s killProcess routine. It was easy to verify both the problem and my fix did what I expected by starting my custom build of fennec and running this command:&lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;dm --package-name org.mozilla.fennec_wlach killapp org.mozilla.fennec_wlach&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;(yes, it&amp;#8217;s a bit unfortunate that this bug occurred in the first place: devicemanagerADB really needs unit tests)&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;Day-to-day menial tasks, like getting device info/status, capturing screenshots, etc. You can get a full list of what this utility is capable of by running &amp;#8211;help. E.g.:&lt;/p&gt;
  &lt;p&gt;``` (mozbase)wlach@eideticker:~/src/eideticker$ dm &amp;mdash;help Usage: dm [options] &amp;lt;command&amp;gt; [&amp;lt;args&amp;gt;]&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;device commands:  info [os|id|uptime|systime|screen|memory|processes] - get  information on a specified aspect of the device (if no argument  given, print all available information)  install &amp;lt;file&amp;gt; - push this package file to the device and install it  killapp &amp;lt;process name&amp;gt; - kills any processes with a particular name  on device  launchapp &amp;lt;appname&amp;gt; &amp;lt;activity name&amp;gt; &amp;lt;intent&amp;gt; &amp;lt;URL&amp;gt; - launches  application on device  ls &amp;lt;remote&amp;gt; - list files on device  ps - get information on running processes on device  pull &amp;lt;local&amp;gt; [remote] - copy file/dir from device  push &amp;lt;local&amp;gt; &amp;lt;remote&amp;gt; - copy file/dir to device  rm &amp;lt;remote&amp;gt; - remove file from device  rmdir &amp;lt;remote&amp;gt; - recursively remove directory from device  screencap &amp;lt;png file&amp;gt; - capture screenshot of device in action  shell &amp;lt;command&amp;gt; - run shell command on device&lt;/p&gt;

&lt;p&gt;Options:  -h, &amp;mdash;help show this help message and exit  -v, &amp;mdash;verbose Verbose output from DeviceManager  &amp;mdash;host=HOST Device hostname (only if using TCP/IP)  -p PORT, &amp;mdash;port=PORT Custom device port (if using SUTAgent or adb-over-tcp)  -m DMTYPE, &amp;mdash;dmtype=DMTYPE  DeviceManager type (adb or sut, defaults to adb)  -d HWID, &amp;mdash;hwid=HWID HWID  &amp;mdash;package-name=PACKAGENAME  Packagename (if using DeviceManagerADB) ```&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Before you ask, yes, it&amp;amp;#8217;s technically possible to do much of this with the original adb utility. But (1) some of our internal stuff can&amp;amp;#8217;t use adb a variety of reasons and (2) some of the tasks above (e.g. launching an app, getting a screenshot) involve considerably more typing with adb than with dm. So it&amp;amp;#8217;s still a win. &amp;lt;/li&amp;gt; &amp;lt;/ol&amp;gt; 

Happy command-lining!&lt;/code&gt;&lt;/pre&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Catching problems early with python</title>
  <link rel="alternate" href="http://www.example.com/2012/10/catching-problems-early-with-python.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2012-10-catching-problems-early-with-python-html</id>
  <published>2012-10-15T04:00:00Z</published>
  <updated>2012-10-15T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Just a few quick notes on how to avoid a class of errors I&amp;#8217;ve been seeing in Mozilla&amp;#8217;s automation over the last year. Since python interprets code dynamically, it&amp;#8217;s pretty easy for problems like undefined variables to slip through, especially if they&amp;#8217;re in a codepath that isn&amp;#8217;t frequently tested. The most recent example I found was in some cleanup-after-error code for remote mochitest/reftest, which tried to call &amp;#8220;self.cleanup&amp;#8221; from a standalone method.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def main():
      ...
      try:
        dm.recordLogcat()
        retVal = mochitest.runTests(options)
        logcat = dm.getLogcat()
      except:
        print "TEST-UNEXPECTED-FAIL | %s | Exception caught while running tests." % sys.exc_info()[1]
        mochitest.stopWebServer(options)
        mochitest.stopWebSocketServer(options)
        try:
            self.cleanup(None, options)
        except:
            pass
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href="http://hg.mozilla.org/mozilla-central/file/942ed5747b63/testing/mochitest/runtestsremote.py#l481"&gt;testing/mochitest/runtestsremote.py&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We&amp;#8217;re calling cleanup as if it were a class variable, but we&amp;#8217;re not inside any class! It&amp;#8217;s easy to see what will happen if you try to run some similar code from the python console:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; self.cleanup()
Traceback (most recent call last):
  File "&amp;amp;lt;stdin&amp;gt;", line 1, in &amp;amp;lt;module&amp;gt;
NameError: name 'self' is not defined&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, because we&amp;#8217;re in a blanket try&amp;#8230;except, we will never see an error. The cleanup code will never be called, instead the exception is immediately caught and subsequently ignored. Probably not the end of the world in this case (there are other parts of our mobile automation which will perform the same cleanup later), but it&amp;#8217;s easy to imagine where this would be a more serious problem.&lt;/p&gt;

&lt;p&gt;There&amp;#8217;s two very easy ways to help stop errors like this before they hit our code:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;
  &lt;p&gt;Try to avoid using a blanket try&amp;#8230;except. In addition to catching legitimate problems which we want to ignore (in the remote case for example, devicemanager exceptions), it also catches (and thus obscures) things like syntax, name, or type errors. Instead, try just catching the specific exception you&amp;#8217;re looking for. For example, we might rewrite the case above as:&lt;/p&gt;``` try:&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt; mochitest.cleanup(None, options) except devicemanager.DMError:  print &amp;ldquo;WARNING: Device error while cleaning up&amp;rdquo; &lt;code&gt;2. pyflakes, pyflakes, pyflakes. [Pyflakes][2] is a fantastic tool for analyzing your python code for common problems. It&amp;amp;#8217;s kind of analagous to [jslint][3], for those of you familiar with that. Here&amp;amp;#8217;s what happens when we run pyflakes against the offending code:&lt;/code&gt; wlach@eideticker:~/src/mozilla-central$ pyflakes testing/mochitest/runtestsremote.py  testing/mochitest/runtestsremote.py:7: &amp;lsquo;time&amp;rsquo; imported but unused testing/mochitest/runtestsremote.py:481: undefined name &amp;lsquo;self&amp;rsquo; testing/mochitest/runtestsremote.py:500: undefined name &amp;lsquo;self&amp;rsquo; ```&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;I&amp;amp;#8217;ve found pyflakes to be an indispensable part of my workflow. I generally run it after making any substantial change to a python file, and certainly before pushing anything to be consumed by others.&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ultimately there&amp;#8217;s no substitute for actually thoroughly testing your code, no matter what language you&amp;#8217;re using. But using the right techniques and tools can certainly make your life easier.&lt;/p&gt;

&lt;p&gt;[ for those wondering, a fix for the issue mentioned in this post is part of &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=801652"&gt;bug 801652&lt;/a&gt; ]&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Say hello to frof</title>
  <link rel="alternate" href="http://www.example.com/2012/09/say-hello-to-frof.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2012-09-say-hello-to-frof-html</id>
  <published>2012-09-25T04:00:00Z</published>
  <updated>2012-09-25T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Inspired by the work I&amp;#8217;d been doing with Benoit Girard to &lt;a href="http://wrla.ch/blog/2012/09/more-eideticker-happenings-profiling-and-startup-testing/"&gt;integrate the Firefox Profiler with Eideticker&lt;/a&gt;, I decided to create an easy-to-use python script to help with gathering profiles on Fennec, which I call &lt;a href="https://github.com/wlach/frof"&gt;frof&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Frof pretty considerably reduces the amount of busywork you need to do to gather a profile. Instead of a rather complicated multi-step process to initialize fennec with the right parameters for profiling, downloading profiles, etc., you can just run the frof script like so:
 &lt;br /&gt;&lt;code&gt;&amp;lt;br /&amp;gt;
frof org.mozilla.fennec http://wrla.ch mywonderfulprofile.zip&amp;lt;br /&amp;gt;&lt;/code&gt;
 &lt;br /&gt;Assuming that frof was bootstrapped correctly (and your phone is connected to your computer in debugging mode), this should start up fennec automatically with that URL loaded. Now, just perform whatever action you want to profile on your phone, then press enter in the terminal when you&amp;#8217;re done. Voila, instant profile trace which you can examine, post to bugs, etc. All the other details are automated.&lt;/p&gt;

&lt;p&gt;Backstory: the inspiration for frof came from a personal itch of mine, the fact that leaflet.js maps seem to be causing out-of-memory errors on Fennec when zooming is enabled (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=784580"&gt;bug 784580&lt;/a&gt;). I wanted to be able to capture some profiles to see what was going on, but the &lt;a href="https://developer.mozilla.org/en-US/docs/Performance/Profiling_with_the_Built-in_Profiler#Profiling_Firefox_mobile"&gt;current instructions&lt;/a&gt; on MDN seem a bit unwieldly. I figured I&amp;#8217;d get lots of mileage out of a tool to make this easier (especially if I was going to get into a profile, edit, debug cycle), so I spent a few hours dilligently copying the logic we put into eideticker to gather profiles into a standalone script.&lt;/p&gt;

&lt;p&gt;&lt;a href="http://wrla.ch/blog/2012/09/say-hello-to-frof/frof-profile-leaflet/" rel="attachment wp-att-760"&gt;&lt;img src="/files/2012/09/frof-profile-leaflet.png" alt="A profile I generated of a leaflet map with frof" title="frof profile leaflet" width="703" height="365" class="alignnone size-full wp-image-760" srcset="/files/2012/09/frof-profile-leaflet-300x155.png 300w, /files/2012/09/frof-profile-leaflet.png 703w" sizes="(max-width: 703px) 100vw, 703px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Unfortunately in my case, the gecko profile didn&amp;#8217;t tell me much, aside from the fact that Gecko didn&amp;#8217;t seem to be the culprit (remember that on Android we also have lots of Java-based frontend code to contend with, which the profiler doesn&amp;#8217;t measure). I&amp;#8217;m going to stare more at the Java code and dig into the various high-level tools that Android provides for profiling performance and memory usage. My current hypothesis is that the problem is the screenshot code and the CSS transitions that leaflet generates when zooming. In the mean time, the only thing I have to show for my foray away from writing tools for Mozilla is &amp;#8230; yet another tool.&lt;/p&gt;

&lt;p&gt;Oh well, it could be worse. My fervent hope is that frof will be helpful for both Fennec developers and QA. Let me know if you wind up using it!&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">More Eideticker happenings: Profiling and startup testing</title>
  <link rel="alternate" href="http://www.example.com/2012/09/more-eideticker-happenings-profiling-and-startup-testing.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2012-09-more-eideticker-happenings-profiling-and-startup-testing-html</id>
  <published>2012-09-13T04:00:00Z</published>
  <updated>2012-09-13T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;&lt;em&gt;[ For more information on the Eideticker software I&amp;#8217;m referring to, see &lt;a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/"&gt;this entry&lt;/a&gt; ]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Just wanted to give some updates on a few new Eideticker features which have landed in the past week.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Profiling support&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;While Eideticker is a great tool for observing the external behaviour of the mobile browser, it hasn&amp;#8217;t been able to tell us much about what&amp;#8217;s going on inside. If something&amp;#8217;s slow, why is it slow? If it&amp;#8217;s slower than it was the day before, what&amp;#8217;s the cause? If it&amp;#8217;s faster? What explains the deviations in test results from one run to the other?&lt;/p&gt;

&lt;p&gt;Thanks to &lt;a href="http://benoitgirard.wordpress.com/"&gt;Benoit Girard&lt;/a&gt; (+ a little bit of integration work from me), we can now start providing answers to these questions. Eideticker now has a mode that allows us to capture a &lt;a href="https://developer.mozilla.org/en-US/docs/Performance/Profiling_with_the_Built-in_Profiler"&gt;sampling profile&lt;/a&gt; of the application while the video capture is ongoing. From the dashboard, you can now get access said profile, just by clicking on a link.&lt;/p&gt;

&lt;p&gt;&lt;a href="http://wrla.ch/blog/2012/09/more-eideticker-happenings-profiling-and-startup-testing/dash-with-link-to-profile/" rel="attachment wp-att-737"&gt;&lt;img src="/files/2012/09/dash-with-link-to-profile.png" alt="" title="Eideticker dashboard with link to profile" width="764" height="555" class="alignnone size-full wp-image-737" srcset="/files/2012/09/dash-with-link-to-profile-300x217.png 300w, /files/2012/09/dash-with-link-to-profile.png 764w" sizes="(max-width: 764px) 100vw, 764px" /&gt;&lt;/a&gt;
 &lt;br /&gt;&lt;a href="http://wrla.ch/blog/2012/09/more-eideticker-happenings-profiling-and-startup-testing/profiler-et-screenshot/" rel="attachment wp-att-740"&gt;&lt;img src="/files/2012/09/profiler-et-screenshot.png" alt="" title="Profile of Eideticker Capture" width="852" height="425" class="alignnone size-full wp-image-740" srcset="/files/2012/09/profiler-et-screenshot-300x149.png 300w, /files/2012/09/profiler-et-screenshot.png 852w" sizes="(max-width: 852px) 100vw, 852px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Note that the profile is not yet synchronized precisely to the videocapture (the profile works over the entire run of the browser), but Benoit is busily making that happen. That should hopefully land soon, in the mean time we still have some pretty useful data.&lt;/p&gt;

&lt;p&gt;To say I&amp;#8217;m excited about this would be an understatement. I think it has the potential to open up a whole new world of understanding of why our mobile (and desktop, someday) browser performs the way it does.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Startup / pageload testing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Eideticker has had support for measuring startup and page load time for a few months now, but I hadn&amp;#8217;t yet hooked it up to the dashboard. As of today, it now is. There&amp;#8217;s a bunch of different angles that are interesting to measure here (new vs. old profiles, whether the browser has been launched since boot, launching web applications, loading about:home or loading a web page, &amp;#8230;) which I&amp;#8217;ll get to in due course. For now, we at least have a baseline of how long it takes to see the Firefox homescreen on a Galaxy Nexus:&lt;/p&gt;

&lt;video width="400px" src="http://wrla.ch/eideticker/dashboard/videos/video-1347572002.11.webm" controls="controls"&gt;&lt;/video&gt;

&lt;p&gt;Of course, this is hooked up to the profiling support previously mentioned. Here&amp;#8217;s &lt;a href="http://people.mozilla.com/~bgirard/cleopatra/?zippedProfile=profiles/sps-profile-1347572285.4.zip&amp;amp;#038;videoCapture=videos/video-1347572285.4.webm"&gt;an example profile&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I&amp;#8217;ve already filed &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=791106"&gt;one bug&lt;/a&gt; based on the data gathered so far.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Oh hai, I upergaded yer Eideticker</title>
  <link rel="alternate" href="http://www.example.com/2012/09/oh-hai-i-upergaded-yer-eideticker.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2012-09-oh-hai-i-upergaded-yer-eideticker-html</id>
  <published>2012-09-05T04:00:00Z</published>
  <updated>2012-09-05T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;&lt;em&gt;[ For more information on the Eideticker software I&amp;#8217;m referring to, see &lt;a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/"&gt;this entry&lt;/a&gt; ]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="http://wrla.ch/blog/2012/09/oh-hai-i-upergaded-yer-eideticker/multi-device-dash/" rel="attachment wp-att-709"&gt;&lt;img src="/files/2012/09/multi-device-dash.png" alt="" title="multi-device-dash" width="771" height="576" class="alignnone size-full wp-image-709" srcset="/files/2012/09/multi-device-dash-300x224.png 300w, /files/2012/09/multi-device-dash.png 771w" sizes="(max-width: 771px) 100vw, 771px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;More on this to come, but just a quick note that the client-side URL schema for the Eideticker dashboard has been changed, as we now gather benchmarks for more than one device (Samsung Galaxy Nexus benchmarks FTW). To get the new and improved dashboard, please just go to the root:&lt;/p&gt;

&lt;p&gt;&lt;a href="http://wrla.ch/eideticker/dashboard"&gt;http://wrla.ch/eideticker/dashboard&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Old style URLs like &lt;code&gt;http://wrla.ch/eideticker/dashboard/#/taskjs-scrolling/checkerboard&lt;/code&gt; will no longer work. Sorry for any broken links, this is the price of progress. 😉&lt;/p&gt;

&lt;p&gt;Note that some benchmarks for the Galaxy Nexus are still missing. This is a known problem and will be fixed soon.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">The evolution of simulating events in Eideticker: from monkeys to orangutans</title>
  <link rel="alternate" href="http://www.example.com/2012/07/the-evolution-of-simulating-events-in-eideticker-from-monkeys-to-orangutans.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2012-07-the-evolution-of-simulating-events-in-eideticker-from-monkeys-to-orangutans-html</id>
  <published>2012-07-10T04:00:00Z</published>
  <updated>2012-07-10T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;&lt;em&gt;[ For more information on the Eideticker software I&amp;#8217;m referring to, see &lt;a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/"&gt;this entry&lt;/a&gt; ]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I just merged a new approach I&amp;#8217;ve been using to simulate touch events into the master branch of Eideticker called &lt;a href="http://github.com/wlach/orangutan"&gt;Orangutan&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href="http://wrla.ch/blog/2012/07/the-evolution-of-simulating-events-in-eideticker-from-monkeys-to-orangutns/orangutan/" rel="attachment wp-att-696"&gt;&lt;img src="/files/2012/07/orangutan.jpg" alt="Image of Orangutan" title="orangutan" width="415" height="600" class="alignnone size-full wp-image-696" srcset="/files/2012/07/orangutan-207x300.jpg 207w, /files/2012/07/orangutan.jpg 415w" sizes="(max-width: 415px) 100vw, 415px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As I&amp;#8217;ve mentioned before, we really need to simulate actual user gestures when doing this type of testing to measure real-world performance with Eideticker. Up to now, I&amp;#8217;ve been using google&amp;#8217;s &lt;a href="http://developer.android.com/tools/help/monkeyrunner_concepts.html"&gt;MonkeyRunner&lt;/a&gt; tool to do this. I was always a little skeptical about its approach (which involved using a privileged tool written in Java with special access to Android&amp;#8217;s windowing system), but up until recently I&amp;#8217;d managed to get around its &lt;a href="http://code.google.com/p/android/issues/detail?id=27955"&gt;issues&lt;/a&gt; with a &lt;a href="https://github.com/mozilla/eideticker/commit/8d034212bc38c1fc80b9fe792c0b06919c74141a"&gt;successively&lt;/a&gt; &lt;a href="https://github.com/mozilla/eideticker/commit/55d63960dc0a5090cee00fe917a851db082ee0fd"&gt;more&lt;/a&gt; &lt;a href="https://github.com/mozilla/eideticker/commit/686882d32fb25a700afec35c39e53a73658688e3"&gt;complicated&lt;/a&gt; series of hacks.&lt;/p&gt;

&lt;p&gt;Unfortunately, I finally came up with a problem that I couldn&amp;#8217;t figure out how to fix: monkeyrunner doesn&amp;#8217;t attach precise timing information to the events it generates, which completely throws off Google Chrome for Android when you try to simulate a pan gesture. I&amp;#8217;ve tried just about every way of using the existing functionality (both the networked mode and the &amp;#8220;script&amp;#8221; mode), but nothing seemed to help. My conclusion is that the only way of continuing to use monkey would be to create a fix for the software itself, which implies forking and extending the entire Android Open Source Project. As noble a goal as that might be, doing that across all the major Android versions I want to support (2.2, 2.3, 4.0 and now 4.1) was more work than I felt like taking on (not to mention the question of how to deploy that work). I decided to build something entirely new which did not have this requirement.&lt;/p&gt;

&lt;p&gt;Enter Orangutan. Unlike MonkeyRunner, Orangutan simply injects events directly into low-level the kernel device file that represents an Android device&amp;#8217;s touch screen. It&amp;#8217;s fast (written in native C), trivial to build, and seems to work seamlessly with any application I&amp;#8217;ve tried using it with (including Google Chrome for Android). Most interestingly for Mozilla, this interface is also present on Firefox OS (&lt;a href="https://wiki.mozilla.org/B2G"&gt;Boot to Gecko&lt;/a&gt;) based devices, so we should be able to use Orangutan there to support both Eideticker and any other testing framework which needs to test real-world user input test cases. Exciting times!&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Mobile Firefox: Measuring How a Browser Feels</title>
  <link rel="alternate" href="http://www.example.com/2012/06/mobile-firefox-measuring-how-a-browser-feels.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2012-06-mobile-firefox-measuring-how-a-browser-feels-html</id>
  <published>2012-06-26T04:00:00Z</published>
  <updated>2012-06-26T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;blockquote&gt;
 &lt;p&gt;A while back, I began work on a new test framework for mobile browsers called Eideticker, which aims to benchmark browsers by capturing them on HMDI video, then running image analysis on the result. I wrote about this in a blog post entitled, “&lt;a href="http://wrla.ch/blog/2011/11/measuring-what-the-user-sees/"&gt;Measuring what the user sees&lt;/a&gt;.” Some seven months later, we are about to release a new version of Firefox for Android and Eideticker has played a major role in qualifying its performance and identifying areas for improvement along the way.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I thought it would be worth publicizing some of the results that we have seen so far and explain why Eideticker has been useful. This post aims to explain the ideas behind Eideticker and hopes to inspire ideas on how to further improve objective cross-browser benchmarks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Idea 1: Put cross-browser performance tests on a more rigorous footing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;One of the problems with existing benchmarks is that the graphical performance that they measure is entirely synthetic. So when something like Microsoft&amp;#8217;s fishbowl demo claims 50 frames per second, that is based entirely on an internal measurement. There is no guarantee that is what the user is actually seeing. For all we know, it could be throwing half those frames away. To say nothing of the fact that measuring the results could interfere with the results themselves!&lt;/p&gt;

&lt;p&gt;With Eideticker, we only analyze what the user sees (under the assumption that what the user sees is what comes out of the device&amp;#8217;s HDMI out). To measure frame rate, Eideticker painstakingly analyzes a video capture to see the difference between each frame. Only if one frame is qualitatively different than the one before it will it consider it to be a step forward in the animation progression. There is no room for a browser to &amp;#8220;cheat&amp;#8221; by, for example, throwing a frame away. Here are two example frames from an Eideticker capture of an animated clock, along with a visual representation of the difference it measured between them:&lt;/p&gt;

&lt;table style="border:none;"&gt;
 &lt;tr&gt;
  &lt;td&gt;&lt;/td&gt; 
  &lt;td&gt;&lt;/td&gt; 
  &lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: The red area of the graphic on the right indicates a region that Eideticker has detected as having changed between the two images. The black area of the graphic indicates a region that is unchanged.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Seeing visual evidence like this increases our confidence that things are truly better than they were before. For example, Eideticker very clearly, and accurately, measured the improvement when &lt;a href="http://benoitgirard.wordpress.com/2012/05/15/off-main-thread-compositing-omtc-and-why-it-matters/"&gt;Off Main Thread Compositing&lt;/a&gt; landed. We saw a big performance jump in the afore-mentioned clock benchmark:
 &lt;br /&gt;
 &lt;br /&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;
 &lt;br /&gt;&lt;em&gt;Note: Nightly = the new Firefox for Android as of that date (an incremental step towards what was just released), XUL = Previous Firefox for Android, Stock = Default browser that comes with Android 2.2.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Idea 2: Measure subjective performance based on actual user interaction patterns&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Measuring real browser output is useful, but the problem is that these benchmarks do not actually measure how a user experiences the Web. Does an animated image of a clock or a &lt;a href="http://ie.microsoft.com/testdrive/Performance/FishIETank/"&gt;fishtank&lt;/a&gt; actually represent a normal user experience? Thanks to the development of mobile gaming, this is slowly changing, but I was still say “no.”  The majority of users time spent mobile browsing is spent on news websites, Wikipedia, Facebook and I Can Haz Cheezburger. &lt;a href="#_msocom_1"&gt;&lt;/a&gt; For these sites, I would submit that there are two things users care about:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;
  &lt;p&gt;When I swipe to move the content (e.g. to scroll down to see more content on &lt;a href="http://CNN.com/"&gt;CNN.com&lt;/a&gt;), does it respond instantaneously and in a pleasing manner? Do I see a nice smooth animation or a jerky mess?&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;When the content moves, do I see new content right away? Or do I have to wait a few moments while the view updates (this slow load is also called checkerboarding)?&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;&amp;nbsp;
 &lt;br /&gt;I think the key here is to measure what the user sees. Internal measurements for anything other than what the user experiences are misleading and inconsistent across browsers.&lt;/p&gt;

&lt;p&gt;For the first item, I believe the best indication of perceived responsiveness and smoothness is found by measuring the number of frames that are displayed in response to user interaction. As with all measurements, it is not perfect, but one can safely say that if there are only a few frames changed in response to a swipe, the experience is going to be jerky.&lt;/p&gt;

&lt;p&gt;Compare these two videos of panning on &lt;a href="http://CNN.com/"&gt;CNN.com&lt;/a&gt;. The first is the previous Firefox for Android, clocking in at about 12.3 frames per second (fps). The second is the recent Firefox for Android , clocking in at 23 fps using Eideticker&amp;#8217;s internal measurements:&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;table style="border: none;"&gt;
 &lt;tr&gt;
  &lt;td&gt;
   &lt;video width="200px" height="240" src="/files/2012/06/xul-fennec-cnn.webm" controls="controls"&gt;&lt;/video&gt;&lt;/td&gt; 
  &lt;td&gt;
   &lt;video width="200px" height="240" src="/files/2012/06/native-fennec-cnn.webm" controls="controls"&gt;&lt;/video&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;For the second, we can easily measure how quickly a user sees content by setting the background color of the page to an obvious color (in Eideticker we use magenta), overlaying an image on top, then measuring the amount of the page that is magenta while panning around. Since all modern browsers just use the background color of the page when something is to be redrawn (or at least can be configured that way), it&amp;#8217;s easy to compare across browsers. You can see in the videos above that the new Native Firefox does much better than the old one in this regard, at least on this benchmark. Here&amp;#8217;s an image of Eideticker detecting checkerboarding on a capture (red indicates checkerboarded areas):&lt;/p&gt;

&lt;table style="border:none;"&gt;
 &lt;tr&gt;
  &lt;td&gt;&lt;/td&gt; 
  &lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;&lt;/p&gt;

&lt;table style="border: none;"&gt;
 &lt;tr&gt;
  &lt;td&gt;&lt;/td&gt; 
  &lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Note: The red area of the graphic on the right indicates a region that Eideticker has detected as checkerboarded (i.e. undrawn). The black area of the graphic indicates a region that is fully drawn.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The important thing in both cases is that these captures represent &lt;strong&gt;a real user experience&lt;/strong&gt;. The swipe gestures are synthesized such that they are interpreted by Android as an actual touch event. This is important: using tricks like &lt;a href="https://developer.mozilla.org/en/DOM/window.scrollTo"&gt;javascript scrollTo&lt;/a&gt; inside your Web page to pan would not actually engage the renderer in quite the same way. On Firefox for Android (and probably other browsers as well, though I haven&amp;#8217;t checked), the response to a touch event is always handled inside the browser internal rendering engine to give the expected &amp;#8220;physical feel.&amp;#8221; Manually moving the viewport would give completely different results since so much of the fancy code we use to reduce the redraw delay would not be engaged.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Overall, I am very happy with how Eideticker has allowed us to track and measure improvements in Firefox for Android. In developing Eideticker, we aimed to create a benchmark that measured how users actually interact with a browser – not how abstract measurements claim how great a browser is.  As we move ahead with new projects to improve Firefox for Android, Eideticker will continue to be useful in making sure that using our browser is the best experience that it can be, especially combined with other tools like Benoit Girard&amp;#8217;s &lt;a href="http://benoitgirard.wordpress.com/2012/03/30/writing-a-profiler/"&gt;sampling profiler&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For more information on Eideticker, feel free to visit &lt;a href="https://wiki.mozilla.org/Project_Eideticker"&gt;its homepage&lt;/a&gt; on &lt;a href="http://wiki.mozilla.org/"&gt;wiki.mozilla.org&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Mass code relicensing with facebook&amp;#8217;s codemod</title>
  <link rel="alternate" href="http://www.example.com/2012/06/mass-code-relicensing-with-facebook-8217-s-codemod.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2012-06-mass-code-relicensing-with-facebook-8217-s-codemod-html</id>
  <published>2012-06-07T04:00:00Z</published>
  <updated>2012-06-07T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Recently the Firefox source repository (mozilla-central) was converted over recently to a new license with a &lt;a href="http://www.mozilla.org/MPL/headers/"&gt;lovely short boilerplate&lt;/a&gt;. This is great, but here in &lt;a href="https://wiki.mozilla.org/Auto-tools"&gt;automation and tools&lt;/a&gt;, we have a fairly large number of projects that live outside of the main tree but for which the new license still applies. A few weeks ago, I wanted to move one our projects over (mozbase), but didn&amp;#8217;t want to spend hours manually editing text files. I understand that a script was used to convert mozilla-central, but a quick google didn&amp;#8217;t turn it up. &lt;strong&gt;[ edit:** thanks to Ed Morley for pointing out to me that it lives here: &lt;a href="http://hg.mozilla.org/users/gerv_mozilla.org/relic/"&gt;http://hg.mozilla.org/users/gerv_mozilla.org/relic/&lt;/a&gt; **]&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I surely could have asked about where this script is, but this problem gave me an excuse to try something that I&amp;#8217;d been meaning to for a while: Facebook&amp;#8217;s &lt;a href="https://github.com/facebook/codemod"&gt;codemod&lt;/a&gt;. Codemod is a neat little command-line utility which aims to help with mass refactorings of code. All you have to do is provide a few regular expressions to replace, and off it goes. I tried it out with mozbase, and the results were great: 5 minutes spent coming up with a regular expression and jiggering with command line options, and the job was done.&lt;/p&gt;

&lt;p&gt;I had the desire to do this again today for &lt;a href="http://github.com/mozilla/eideticker"&gt;Eideticker&lt;/a&gt;, and decided to document the (extremely simple) process for posterity. I just used this simple command line&amp;#8230;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;../codemod/src/codemod.py --extensions py -m '# \*\*\*\*\* BEGIN LICENSE.*END LICENSE BLOCK \*\*\*\*\*' '# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this file,\n# You can obtain one at http://mozilla.org/MPL/2.0/.'&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#8230; et voila, out popped &lt;a href="https://github.com/mozilla/eideticker/commit/9456933670fb4590af3060f4ff40d11271859b8d"&gt;shiny new boilerplate&lt;/a&gt;.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Ghetto retroscope with ffmpeg and the &amp;lt;video&amp;gt; tag</title>
  <link rel="alternate" href="http://www.example.com/2012/05/ghetto-retroscope-with-ffmpeg-and-the-lt-video-gt-tag.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2012-05-ghetto-retroscope-with-ffmpeg-and-the-lt-video-gt-tag-html</id>
  <published>2012-05-07T04:00:00Z</published>
  <updated>2012-05-07T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;So yesterday we had a small get-together at my place, which gave me the opportunity to try something I&amp;#8217;d been meaning to do for a while: build my own retroscope.&lt;/p&gt;

&lt;p&gt;The idea is pretty simple: have a webcam record bits and pieces of a social event, then play them back on-the-spot a few minutes/hours later. I first heard about the concept from reading Nat Friedman&amp;#8217;s &lt;a href="http://nat.org/blog/2005/10/the-retroscope/"&gt;blog entry&lt;/a&gt; from 2005 &amp;#8212; if you read that, you see that he just hooked up a video camera to his TiVo. 7 years in the future, laptop webcams are ubiquitous and we have the awesome HTML5  tag, so I figured it would be easy to knock up something interesting in short order with zero custom hardware.&lt;/p&gt;

&lt;p&gt;Having only remembered that I wanted to do this about 30 minutes before people were scheduled to start arriving, I didn&amp;#8217;t have much time to do anything really perfect. I settled on using &lt;a href="http://stackoverflow.com/questions/7500763/command-line-streaming-webcam-with-audio-from-ubuntu-server-in-webm-format"&gt;this little snippet&lt;/a&gt; from stackoverflow to generate short (5 second) movies on my laptop, then used scp to copy them over and display a montage of them in an auto-refreshing webpage on my &amp;#8220;television&amp;#8221; (which is a Mac-Mini connected to a large computer monitor). Despite being a total hack job, the end result generated much amusement. I think this is a bit different from what Nat originally did (it sounds from his blog like his retroscope played back longer segments), but I think the end result is actually a bit more fun.&lt;/p&gt;

&lt;p&gt;&lt;a href="http://wrla.ch/blog/2012/05/ghetto-retroscope-with-ffmpeg-and-the-video-tag/retroscope-screenshot/" rel="attachment wp-att-598"&gt;&lt;img src="/files/2012/05/retroscope-screenshot-1024x694.png" alt="" title="retroscope-screenshot" width="640" height="433" class="alignnone size-large wp-image-598" srcset="/files/2012/05/retroscope-screenshot-300x203.png 300w, /files/2012/05/retroscope-screenshot-1024x694.png 1024w, /files/2012/05/retroscope-screenshot.png 1319w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Perhaps unfortunately, but probably ultimately for the best, only a few snippets from the actual night got stored away. One example is this gem:&lt;/p&gt;

&lt;p&gt;(yes, that handsome fellow with the Pernot is me)&lt;/p&gt;

&lt;p&gt;I thought it might be fun to release the slightly-cleaned up results of this experiment as opensource for others to play with, so I created a small project for it on github. Unlike the original version, no complicated scp scheme is required &amp;#8212; I just reused Joel Maher&amp;#8217;s most excellent mozhttpd library from &lt;a href="http://github.com/mozilla/mozbase"&gt;mozbase&lt;/a&gt; to run a web server in the same process as the capture logic. All you need to do is run the server on a Linux machine with a webcam and connect to it with a web browser from any other machine on your local network.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://github.com/wlach/retroscope"&gt;https://github.com/wlach/retroscope&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Enjoy!&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Launching random web browsers on Android</title>
  <link rel="alternate" href="http://www.example.com/2012/05/launching-random-web-browsers-on-android.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2012-05-launching-random-web-browsers-on-android-html</id>
  <published>2012-05-04T04:00:00Z</published>
  <updated>2012-05-04T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Ok, this is somewhat mundane, but I&amp;#8217;ve already had to do it twice (and helped someone do something similar on #mobile), so I figured I might as well blog about it for posterity.&lt;/p&gt;

&lt;p&gt;For various automation tasks (notably the &lt;a href="http://wrla.ch/eideticker/dashboard/"&gt;Eideticker dashboard&lt;/a&gt; and the &lt;a href="http://mrcote.info/phonedash/#/rawfennecstartup"&gt;cross-browser startup tests&lt;/a&gt;), we need to be able to launch an Android browser on the command line (via adb shell or our own custom SUTAgent). This is a bit of a black art, but you can find references on how to do this on stackoverflow and other places. The magic incantation is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;am start -a android.intent.action.VIEW -n &amp;amp;lt;application/intent&amp;amp;gt; -d &amp;amp;lt;url&amp;amp;gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, for example, to launch Fennec, you&amp;#8217;d run this on the Android command prompt:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;am start -a android.intent.action.VIEW -n org.mozilla.fennec/.App -d http://mygreatsite.info&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ok, easy enough, but what if we want to launch a new browser that we just downloaded (e.g. Google Chrome)? Where do we get the application and intent names?&lt;/p&gt;

&lt;p&gt;The short answer is that you need to reach into the apk and dig. 😉 There&amp;#8217;s probably many ways of doing this, but here&amp;#8217;s what I do (which has the distinct advantage of not needing to compile, download or run weird java applications):&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;
  &lt;p&gt;Copy the apk onto your machine (the apk should be in /data/app: if you have a rooted phone, you should be able to copy that off to your machine).&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;Extract &lt;em&gt;AndroidManifest.xml&lt;/em&gt; from the apk (it&amp;#8217;s just a .zip) and run &lt;a href="http://android-random.googlecode.com/svn/trunk/axml2xml/axml2xml.pl"&gt;axml2xml.pl&lt;/a&gt; on it.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;Examine the resultant xml file and look for the &lt;strong&gt;&lt;/strong&gt; tag. It should have a property called &lt;strong&gt;&lt;/strong&gt; which is the package name. For example:&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;We can see pretty clearly that the application name in this case is &lt;strong&gt;com.android.chrome&lt;/strong&gt; (you can also get this by running ps when using the application)&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;Finally, look for a tag called &lt;strong&gt;&lt;/strong&gt; with an &lt;strong&gt;&lt;/strong&gt; tag with &lt;strong&gt;&lt;/strong&gt; as the &lt;strong&gt;android-name&lt;/strong&gt; property. Scan up for the overarching &lt;strong&gt;activity&lt;/strong&gt; tag, whose &lt;strong&gt;android-name&lt;/strong&gt; property. This is the activity name. For example:&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;Likewise here we see that the activity name we want is &lt;strong&gt;.Main&lt;/strong&gt; (which Android explicitly expands out to &lt;strong&gt;com.android.chrome.Main&lt;/strong&gt;)&lt;/p&gt;

&lt;p&gt;Armed with this information, you should now have enough information to launch the application. Furthering the example above, here&amp;#8217;s how to start Chrome on Android via adb&amp;#8217;s shell:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;am start -a android.intent.action.VIEW -n com.android.chrome/.Main -d http://mygreatsite.info&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hope this helps someone, somewhere.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Eideticker with less eideticker</title>
  <link rel="alternate" href="http://www.example.com/2012/04/eideticker-with-less-eideticker.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2012-04-eideticker-with-less-eideticker-html</id>
  <published>2012-04-24T04:00:00Z</published>
  <updated>2012-04-24T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;&lt;em&gt;[ For more information on the Eideticker software I&amp;#8217;m referring to, see &lt;a href="http://wrla.ch/blog/2011/11/measuring-what-the-user-sees/"&gt;this entry&lt;/a&gt; ]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;tl;dr: You can now run the standard eideticker benchmarks easily on any Android phone without any kind of specialized hardware.&lt;/p&gt;

&lt;p&gt;So Eideticker is pretty great at comparing relative performance between different browsers and generally measuring things in an absolutely neutral way. Unfortunately it&amp;#8217;s a bit of a pain to use it at the moment to catch regressions: the software still has a few bugs and encoding/decoding/analyzing the capture still takes a great deal of time. Not to mention the fact that it currently requires specialized hardware (though that will soon be less of a concern at least inside MoCo, where we have a bunch of Eideticker boxes on order for the Toronto and Mountain View offices).&lt;/p&gt;

&lt;p&gt;A few months ago, Chris Lord wrote up some great code to internally measure the amount of checkerboarding going on in Fennec. I&amp;#8217;ve thought for a while that it would be a neat idea to hook this up to the Eideticker harness, and today I finally did so. After installing Eideticker, you can now run the benchmark on any machine against an arbitrary fennec build just by typing this from the eideticker root directory:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;adb shell setprop log.tag.GeckoLayerRendererProf DEBUG
./bin/get-metric-for-build.py --no-capture --get-internal-checkerboard-stats --num-runs 3 nightly.apk src/tests/scrolling/taskjs.org/index.html&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In return, you&amp;#8217;ll get some nice clean results like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;=== Internal Checkerboard Stats (sum of percents, not percentage) ===
[167.34348, 171.871015, 175.3420296]&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Just to be sure that the results were comparable, I did a quick set of runs on the Eideticker machine in Mountain View with both internal checkerboard statistics gathering and HDMI capturing enabled.&lt;/p&gt;

&lt;table&gt;
 &lt;tr&gt;
  &lt;td&gt;Stats file&lt;/td&gt; 
  &lt;td&gt;HDMI capturing&lt;/td&gt;&lt;/tr&gt;
 &lt;tr&gt;
  &lt;td&gt;167.34348&lt;/td&gt; 
  &lt;td&gt;177.022&lt;/td&gt;&lt;/tr&gt;
 &lt;tr&gt;
  &lt;td&gt;171.87&lt;/td&gt; 
  &lt;td&gt;184.46&lt;/td&gt;&lt;/tr&gt;
 &lt;tr&gt;
  &lt;td&gt;175.34&lt;/td&gt; 
  &lt;td&gt;184.44&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;While the results aren&amp;#8217;t identical (we measure number of frames differently inside Fennec than we do with Eideticker, for one thing), they do seem roughly correlated. So go forth, benchmark and tweak! 😉&lt;/p&gt;

&lt;p&gt;P.S. If you&amp;#8217;ve been following mobile automation, you might be asking why I don&amp;#8217;t just suggest running Talos and Robocop on your workstation. Can&amp;#8217;t they do the same sorts of things? The short answer is that yes, they can, but unfortunately they&amp;#8217;re much more involved to set up and use at the moment. Arguably they shouldn&amp;#8217;t be, and this is something we (&lt;a href="https://wiki.mozilla.org/Auto-tools/"&gt;Mozilla tools &amp;amp; automation&lt;/a&gt;) need to work on. We&amp;#8217;ll get there eventually (and help would be welcome!). For now, hacks like this should help with getting out the first release of Fennec by providing a fast, easy to use tool for bisection and analysis.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">GoFaster dashboard back online</title>
  <link rel="alternate" href="http://www.example.com/2012/04/gofaster-dashboard-back-online.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2012-04-gofaster-dashboard-back-online-html</id>
  <published>2012-04-19T04:00:00Z</published>
  <updated>2012-04-19T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Build times for mozilla-central are a major factor in developer productivity. Faster build times mean more people using try (reducing breakage) and more fine-grained regression ranges (reducing the impact of breakages). As a side benefit, it allows us to avoid buying and maintaining more hardware (or put new hardware to better use). About a half-year ago, we set up a project called BuildFaster to try to bring these times down, setting the ambitious goal of getting build times (from checkin to tests done) down to 2 hours. We didn&amp;#8217;t quite succeed, though we did make some major strides. As part of this project, we also developed a dashboard to track our progress and narrow down the major bottlenecks which were keeping up our build times.&lt;/p&gt;

&lt;p&gt;Unfortunately, this dashboard went down earlier this year with the rest of Brasstacks and we hadn&amp;#8217;t had the chance to bring it back up. I&amp;#8217;m pleased to announce that thanks to Jonathan Griffin, it&amp;#8217;s finally &lt;a href="http://brasstacks.mozilla.com/gofaster/#/"&gt;back online&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While no one is actively working on build performance at the moment (at least to my knowledge), it&amp;#8217;s still useful to keep track of build times to make sure that we don&amp;#8217;t regress. Anecdotally, it has seemed to me that the time needed to get results from try has been pretty stable over the last while, and this is borne out by the results:&lt;/p&gt;

&lt;p&gt;&lt;a href="http://wrla.ch/blog/2012/04/gofaster-dashboard-back-online/gofaster-dashboard-update-2012/" rel="attachment wp-att-529"&gt;&lt;img src="/files/2012/04/gofaster-dashboard-update-2012.png" alt="" title="gofaster-dashboard-update-2012" width="628" height="508" class="alignnone size-full wp-image-529" srcset="/files/2012/04/gofaster-dashboard-update-2012-300x242.png 300w, /files/2012/04/gofaster-dashboard-update-2012.png 628w" sizes="(max-width: 628px) 100vw, 628px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As the cliche goes: no news is good news.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Yet more adventures in mobile performance analysis</title>
  <link rel="alternate" href="http://www.example.com/2012/04/yet-more-adventures-in-mobile-performance-analysis.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2012-04-yet-more-adventures-in-mobile-performance-analysis-html</id>
  <published>2012-04-05T04:00:00Z</published>
  <updated>2012-04-05T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;&lt;em&gt;[ For more information on the Eideticker software I&amp;#8217;m referring to, see &lt;a href="http://wrla.ch/blog/2011/11/measuring-what-the-user-sees/"&gt;this entry&lt;/a&gt; ]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Participated in an &lt;a href="https://wiki.mozilla.org/Fennec/NativeUI/checkerboarding"&gt;interesting meeting&lt;/a&gt; on checkerboarding in Firefox for Android yesterday. As a reminder, checkerboarding refers to the amount of time you spend waiting to see the full page after you do a swipe on your mobile device, and it&amp;#8217;s a big issue right now &amp;#8211; so much so that it puts our delivery goal for the new native browser at risk.&lt;/p&gt;

&lt;p&gt;It seems like we have a number of strategies for improving performance which will likely solve the problem, but we need to be able to measure improvements to make sure that we&amp;#8217;re making progress. This is one of the places where Eideticker could be useful (especially with regards to measuring us against the competition), though there are a few things that we need to add before it&amp;#8217;s going to be as useful as it could be. The most urgent, as I understand, is to come up with a suite of tests which accurately represent the set of pages that we&amp;#8217;re having issues with. The current main measure of checkerboarding that we&amp;#8217;re using with eideticker is taskjs.org which, while an interesting test case in some ways, doesn&amp;#8217;t accurately represent the sort of site that the user would normally go to in the wild (and thus be annoyed by). 😉&lt;/p&gt;

&lt;p&gt;This is going to take a few days (and a lot of review: I&amp;#8217;m definitely no expert when it comes to this stuff) to get right, but I just added two tests for the New York Times which I think are a step in the right direction of being more representative of real-world use cases. Have a look here:&lt;/p&gt;

&lt;p&gt;&lt;a href="http://wrla.ch/eideticker/dashboard/#/nytimes-scrolling"&gt;http://wrla.ch/eideticker/dashboard/#/nytimes-scrolling&lt;/a&gt;
 &lt;br /&gt;&lt;a href="http://wrla.ch/eideticker/dashboard/#/nytimes-zooming"&gt;http://wrla.ch/eideticker/dashboard/#/nytimes-zooming&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The results here actually aren&amp;#8217;t as bad as I would have expected/remembered. There amount of checkerboarding after a zoom out is a bit annoying (I understand this a known issue with font caching, or something) but not too terrible. Still, any improvements that show up here will probably apply across a wide variety of sites, as the design patterns on the New York Times site are very common.&lt;/p&gt;

&lt;p&gt;(P.S. yes, I know I promised a comparison with Google Chrome for Android last time&amp;#8230; rest assured that&amp;#8217;s still coming soon!)&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">An even better way of taking screenshots on Android</title>
  <link rel="alternate" href="http://www.example.com/2012/04/an-even-better-way-of-taking-screenshots-on-android.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2012-04-an-even-better-way-of-taking-screenshots-on-android-html</id>
  <published>2012-04-03T04:00:00Z</published>
  <updated>2012-04-03T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Just thought I&amp;#8217;d mention this because I found it handy.&lt;/p&gt;

&lt;p&gt;A while back AaronMT &lt;a href="http://aaronmt.com/?p=527"&gt;wrote up&lt;/a&gt; some clever instructions on taking Android screenshots by dumping the contents of &amp;#8216;/dev/fb0&amp;#8217; and running ffmpeg on the results. This is useful, but you need to know the resolution of the device you have connected to pass the right arguments to ffmpeg. Wouldn&amp;#8217;t it be better if you had just one script that would work for whatever device you had plugged in?&lt;/p&gt;

&lt;p&gt;In fact, there is a way to do this using the &lt;a href="http://developer.android.com/guide/developing/tools/MonkeyRunner.html"&gt;monkeyrunner&lt;/a&gt; utility. Intended mainly as a tool for synthesizing input on Android (more on that some other time), you can also easily get a capture of the Android screen with its python/jython API (assuming you have the Android SDK installed). Here&amp;#8217;s a quick script which does the job:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from com.android.monkeyrunner import MonkeyRunner, MonkeyDevice
import os
import sys

if len(sys.argv) != 2:
    print "Usage: %s &amp;amp;lt;filename&amp;gt;" % os.path.basename(sys.argv[0])
    sys.exit(1)

device = MonkeyRunner.waitForConnection()
result = device.takeSnapshot()
result.writeToFile(sys.argv[1], 'png')&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Copy that into a file called capture.py (or whatever), then run it like so:
 &lt;br /&gt;&lt;code&gt;&amp;lt;br /&amp;gt;
monkeyrunner capture.py screenshot.png&amp;lt;br /&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;And you&amp;#8217;re off to the races! Nice screenshot, no utilities or non-essential command line arguments required!&lt;/p&gt;

&lt;p&gt;(credit to &lt;a href="http://stackoverflow.com/a/9311243/295132"&gt;this stackoverflow answer&lt;/a&gt; for the idea)&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Eideticker dashboard update</title>
  <link rel="alternate" href="http://www.example.com/2012/03/eideticker-dashboard-update.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2012-03-eideticker-dashboard-update-html</id>
  <published>2012-03-22T04:00:00Z</published>
  <updated>2012-03-22T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;&lt;em&gt;[ For more information on the Eideticker software I&amp;#8217;m referring to, see &lt;a href="http://wrla.ch/blog/2011/11/measuring-what-the-user-sees/"&gt;this entry&lt;/a&gt; ]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Since my &lt;a href="http://wrla.ch/blog/2012/03/announcing-the-eideticker-mobile-performance-dashboard/"&gt;first Eideticker dashboard post&lt;/a&gt; was &lt;a href="http://download.cnet.com/8301-2007_4-57401261-12/whats-going-on-with-firefox-for-android/?tag=epicStories"&gt;so well received&lt;/a&gt;, I thought I&amp;#8217;d give a quick update on another metric that I just brought online: checkerboarding (a.k.a. the amount of time you spend waiting to see the full page after you do a swipe on your mobile device).&lt;/p&gt;

&lt;p&gt;&lt;a href="http://wrla.ch/blog/2012/03/eideticker-dashboard-update/checkerboard_dashboard/" rel="attachment wp-att-503"&gt;&lt;img src="/files/2012/03/checkerboard_dashboard.png" alt="" title="checkerboard_dashboard" width="715" height="485" class="alignnone size-full wp-image-503" srcset="/files/2012/03/checkerboard_dashboard-300x203.png 300w, /files/2012/03/checkerboard_dashboard.png 715w" sizes="(max-width: 715px) 100vw, 715px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[ &lt;a href="http://wrla.ch/eideticker/dashboard/#/checkerboarding"&gt;link to real thing&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;Unfortunately the news here is not as good as before: as the numbers indicate, the new Native Fennec currently performs substantially worse than the version in Android market. This is a known issue, and is currently being tracked in &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=719447"&gt;bug 719447&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Next up: Seeing how we do against Google Chrome for Android.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Announcing the Eideticker mobile performance dashboard</title>
  <link rel="alternate" href="http://www.example.com/2012/03/announcing-the-eideticker-mobile-performance-dashboard.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2012-03-announcing-the-eideticker-mobile-performance-dashboard-html</id>
  <published>2012-03-16T04:00:00Z</published>
  <updated>2012-03-16T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Over the last while, Clint Talbert and I have been working on setting up automatic mobile performance tests using Eideticker (a framework to measure perceived Firefox performance by video capturing automated browser interactions: for more information, see my &lt;a href="http://wrla.ch/blog/2011/11/measuring-what-the-user-sees/"&gt;earlier post&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;There&amp;#8217;s many reasons why this is interesting, but probably the most important one is that it can measure differences reliably across different types of mobile browsers. Currently I&amp;#8217;m testing the old XUL fennec, the Android stock browser, and the latest nightlies.&lt;/p&gt;

&lt;p&gt;I&amp;#8217;m pleased to announce that the first iteration of the dashboard is available for public consumption, on my site.&lt;/p&gt;

&lt;p&gt;&lt;a href="http://wrla.ch/eideticker/dashboard/#/canvas"&gt;http://wrla.ch/eideticker/dashboard/#/canvas&lt;/a&gt;&lt;/p&gt;

&lt;div class="figure"&gt;&lt;img src="/files/2012/03/eideticker-results.png" alt="Eideticker Results" /&gt;
 &lt;p class="caption"&gt;Eideticker Results&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;The demo is pretty cheesey (just click on any of the datapoints to see the video capture), but nonetheless does seem to illustrate some interesting differences between the three browsers. The big jump in performance for nightly comes from the landing of the Maple branch, which happened earlier this week. Hopefully this validates some of the work that the mobile/graphics team has been doing over the past while. Exciting times!&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Playing with pandas</title>
  <link rel="alternate" href="http://www.example.com/2012/02/playing-with-pandas.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2012-02-playing-with-pandas-html</id>
  <published>2012-02-10T05:00:00Z</published>
  <updated>2012-02-10T05:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;For the last few days I&amp;#8217;ve been experimenting with getting a Pandaboard running Android 4.0, continuing the work that Clint Talbert &lt;a href="http://cmtalbert.wordpress.com/2011/10/12/pandaboard-status/"&gt;started in the fall&lt;/a&gt; to get these boards for use as a replacement for the Tegra in Mozilla&amp;#8217;s android automation. The first objective is to get a reproducible build going, after that we&amp;#8217;ll try to get some of our custom tools (&lt;a href="https://wiki.mozilla.org/Mobile/Fennec_Unittests/Remote_Testing"&gt;SUTAgent&lt;/a&gt; &amp;#38; friends) installed by default.&lt;/p&gt;

&lt;p&gt;So far this has been&amp;#8230; interesting. Much as Clint did before, I thought I&amp;#8217;d document some of the notes on what I did in the hopes that they&amp;#8217;ll be helpful to other people trying to do similar things.&lt;/p&gt;

&lt;p&gt;Getting things up and running is a two step process. First, you build the beast. This part is straightforward, just follow the instructions here:&lt;/p&gt;

&lt;p&gt;At least the build part is more or less straightforward. Just follow the instructions here:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="http://source.android.com/source/building.html"&gt;http://source.android.com/source/building.html&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;&lt;a href="http://source.android.com/source/building-devices.html"&gt;http://source.android.com/source/building-devices.html&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Note that you almost certainly want to build in the &amp;#8220;eng&amp;#8221; configuration, which is rooted and (apparently) has some extra tools installed.&lt;/p&gt;

&lt;p&gt;Installing it is a little more tricky. The way they want you to do this is put the pandaboard into a special mode and copy the stuff you built onto an sdcard. Seem a little funny to you? Yeah, it does to me too. Why not just build an sdcard image directly?
 &lt;br /&gt;Nonetheless, this is the officially supported way of imaging a pandaboard, so let&amp;#8217;s just follow it until we can think of a better way of doing things. The instructions for doing this on the pandaboard are located in the source tree here:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;&amp;lt;a href="http://source-android.frandroid.com/device/ti/panda/README"&amp;gt;device/ti/panda/README&amp;lt;/a&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;These are mostly correct as far as I can tell, but there&amp;#8217;s a few gotchas. First, you need to run the commands mentioned as root unless you&amp;#8217;ve configured USB to be configurable by your user. Second, most of those commands are not in the path by default so you&amp;#8217;ll need to specify the full path to e.g. the fastboot utility. The instructions &lt;a href="http://fosiao.com/node/19"&gt;here&lt;/a&gt; cover these exception cases: I recommend following them instead.&lt;/p&gt;

&lt;p&gt;One thing which neither document mentions is that you really need to make sure your sdcard is wiped completely clean before using fastboot. The &amp;#8220;oem format&amp;#8221; step only recreates the partition table, it doesn&amp;#8217;t delete any corrupted partitions. If you reboot while these are still in place, it will try to bring up your corrupted version of Android, not the fastboot console. I spent quite some time debugging why I couldn&amp;#8217;t properly flash the operating system before realizing this. Easiest way to get around this is to dd &lt;code&gt;/dev/zero&lt;/code&gt; onto the sdcard before beginning the flashing process.&lt;/p&gt;

&lt;p&gt;Also, while not strictly necessary to get something up and running, I recommend highly getting an HDMI monitor as well as a serialUSB adapter. The former is useful to see if your Android device actually successfully booted up, the latter is useful for debugging boot issues where you don&amp;#8217;t get that far (the serial console is always available from boot).&lt;/p&gt;

&lt;p&gt;So, after painfully learning about the above caveats, I have managed to get things mostly working. I can see the ICS homescreen on my attached HDMI monitor and interact with it if I attach a USB mouse. The one gotcha is that both ethernet and WIFI networking are totally broken. Plugging in an ethernet cable or connecting to a WIFI network seems to result in the machine randomly rebooting, with the logs saying nothing useful. Both of these things are ostensibly supposed to be working according to the latest I&amp;#8217;ve read from Google so I&amp;#8217;m not exactly sure what&amp;#8217;s going on. Investigations will continue.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Yet more checkerboarding analysis</title>
  <link rel="alternate" href="http://www.example.com/2012/01/yet-more-checkerboarding-analysis.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2012-01-yet-more-checkerboarding-analysis-html</id>
  <published>2012-01-25T05:00:00Z</published>
  <updated>2012-01-25T05:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;I&amp;#8217;ve been spending a bit more time on refining the checkerboarding tests in Eideticker that I talked about last time. Most of my work has been focused on making the results as representative of a real world scenario as possible, to that effect I&amp;#8217;ve been working on:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Changed the test case from a web site of my own concoction to a more realistic example (the &lt;a href="http://taskjs.org"&gt;taskjs.org&lt;/a&gt; site)&lt;/li&gt;
 &lt;li&gt;Use actual Android native events (via &lt;a href="http://developer.android.com/guide/developing/tools/monkeyrunner_concepts.html"&gt;MonkeyRunner&lt;/a&gt;) to synthesize touch-based scrolling instead of simulating the event in JavaScript (which exercises a completely different codepath).&lt;/li&gt;
 &lt;li&gt;Fixing various synchronization issues to make results more repeatable. Before captures were of wildly variable lengths, which made the numbers extremely suspect. There&amp;#8217;s probably still a few issues, but much less than before.&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;The end result of this is a framework that gives much more meaningful results. The bad news is that the results that I&amp;#8217;m measuring don&amp;#8217;t show a very positive picture for where we&amp;#8217;re at with the native re-write of Firefox. Even relative to the version of mobile Firefox which is currently on the Android Market, we still have some catching up to do. Here&amp;#8217;s some video of the &amp;#8220;old&amp;#8221; firefox in action:&lt;/p&gt;

&lt;video src="/files/eideticker/taskjs_xul.webm" width="600px" controls="controls"&gt;&lt;/video&gt;

&lt;p&gt;And here&amp;#8217;s the Native fennec (what we&amp;#8217;re currently offering in nightly, with some minor modifications by me to change the way the &amp;#8220;checkerboard&amp;#8221; is drawn for analysis purposes):&lt;/p&gt;

&lt;video src="/files/eideticker/taskjs_native.webm" width="600px" controls="controls"&gt;&lt;/video&gt;

&lt;p&gt;The numbers behind this comparison:&lt;/p&gt;

&lt;table&gt;
 &lt;tr&gt;
  &lt;td&gt;Platform&lt;/td&gt; 
  &lt;td&gt;Percent checkerboarding over run of test&lt;/td&gt;&lt;/tr&gt;
 &lt;tr&gt;
  &lt;td&gt;Old Fennec&lt;/td&gt; 
  &lt;td&gt;2%&lt;/td&gt;&lt;/tr&gt;
 &lt;tr&gt;
  &lt;td&gt;Native Fennec&lt;/td&gt; 
  &lt;td&gt;57%&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;(by the way, this performance regression is filed as &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=719447"&gt;bug 719447&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;I know there&amp;#8217;s lots of great effort going into improving this situation, so I have hope that we&amp;#8217;ll be doing much better on this metric in the coming days/weeks. The process for creating these videos/analyses is mostly automated at this point, so my plan is to create a small dashboard (ala &lt;a href="http://arewefastyet.com"&gt;arewefastyet.com&lt;/a&gt;) to measure these numbers over time on the latest nightlies. Stay tuned!&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Measuring reduced checkerboarding in mobile Fennec</title>
  <link rel="alternate" href="http://www.example.com/2012/01/measuring-reduced-checkerboarding-in-mobile-fennec.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2012-01-measuring-reduced-checkerboarding-in-mobile-fennec-html</id>
  <published>2012-01-03T05:00:00Z</published>
  <updated>2012-01-03T05:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;After my &lt;a href="http://wrla.ch/blog/2011/12/year-end-eideticker-update/"&gt;post&lt;/a&gt; on measuring checkerboarding in mobile Firefox, &lt;a href="http://cmtalbert.wordpress.com/"&gt;Clint Talbert&lt;/a&gt; (my fearless manager) suggested I run a before and after test to measure the improvement that just landed as part of &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=709152"&gt;bug 709512&lt;/a&gt;. After a bit of cleanup, I did so, measuring the delta between my build on December 20th and the latest version of Aurora. The difference is pretty remarkable: at least on the LG G2X that I&amp;#8217;ve been using for testing, we&amp;#8217;ve gone from checkerboarding between 10&amp;ndash;20% of the time and not checkerboarding almost at all (in between two runs of the test with the Aurora build, there is exactly one frame that checkerboards). All credit to &lt;a href="http://chrislord.net/blog"&gt;Chris Lord&lt;/a&gt; for that!&lt;/p&gt;

&lt;p&gt;See the video evidence for yourself. Before:&lt;/p&gt;

&lt;video src="/files/eideticker/lotsocheckerboarding.webm" width="600px" controls="controls"&gt;&lt;/video&gt;

&lt;p&gt;After:&lt;/p&gt;

&lt;video src="/files/eideticker/almostnocheckerboarding.webm" width="600px" controls="controls"&gt;&lt;/video&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Year end Eideticker update</title>
  <link rel="alternate" href="http://www.example.com/2011/12/year-end-eideticker-update.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2011-12-year-end-eideticker-update-html</id>
  <published>2011-12-23T05:00:00Z</published>
  <updated>2011-12-23T05:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Just before I leave for some Christmas vacation, it&amp;#8217;s time for another update on the state of &lt;a href="http://wrla.ch/blog/2011/11/measuring-what-the-user-sees/"&gt;Eideticker&lt;/a&gt;. Since I last blogged about the software, I&amp;#8217;ve been working on the following three areas:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;Coming up with better algorithm (green screen / red screen) for both determining the area of the capture as well as the start/end of the capture. The harness was already flood filling the area with these colours at the beginning/end of the capture, but now we&amp;#8217;re actually using this information. The code&amp;#8217;s a little hacky, but it seems to work well enough for the test cases I&amp;#8217;ve been using so far.&lt;/li&gt;
 &lt;li&gt;As a demonstration, I wrote up a quick test that demonstrates checkerboarding on mobile Fennec, and wrote up a quick bit of analysis code to detect this pattern and give an overall measure of how much this test &amp;#8220;checkerboards&amp;#8221; (i.e. has regions that are not fully painted when the user scrolls). As I understand this is an area that our mobile team is currently working on this problem quite a bit, it will be interesting to watch the numbers given by this test and see if things improve.&lt;/li&gt;
 &lt;li&gt;It&amp;#8217;s a minor thing, but you can now view a complete webm movie of the captured movie right from the web interface.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;Here&amp;#8217;s a quick demonstration video that shows all the above in action. As before, you might want to watch this full screen:&lt;/p&gt;

&lt;video src="/files/eideticker/eideticker-2011-12-21.webm" width="600px" controls="controls"&gt;&lt;/video&gt;

&lt;p&gt;Happy holidays!&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Eideticker areas to explore</title>
  <link rel="alternate" href="http://www.example.com/2011/12/eideticker-areas-to-explore.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2011-12-eideticker-areas-to-explore-html</id>
  <published>2011-12-09T05:00:00Z</published>
  <updated>2011-12-09T05:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;So I got some nice feedback on my &lt;a href="http://wrla.ch/blog/2011/12/eideticker-update/"&gt;Eideticker post&lt;/a&gt; yesterday on various channels. It seems like some people are interested in hacking on the analysis portion, so I thought I&amp;#8217;d give some quick pointers and suggestions of things to look at.&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;As I mentioned yesterday, the frame analysis is rather stupid. We need to come up with a better algorithm for disambiguating input noise (small fluctuations in the HDMI signal?) from actual changes in the page. Unfortunately the breadth of things that Eideticker&amp;#8217;s meant to analyze makes this a bit difficult. I.e. edge detection probably wouldn&amp;#8217;t work for something like Microsoft&amp;#8217;s &lt;a href="http://ie.microsoft.com/testdrive/performance/psychedelicbrowsing/Default.html"&gt;psychedelic browsing demo&lt;/a&gt;. I suspect the best route here is to put some work into better understanding the nature of this &amp;#8220;noise&amp;#8221; and finding a way to filter it out explicitly.&lt;/li&gt;
 &lt;li&gt;Our analysis code is still rather slow, and is crying out to be parallelized (either by using multiple cores of the same CPU or a GPU). Burak Yiğit Kaya recommended I look into &lt;a href="http://mathema.tician.de/software/pycuda"&gt;PyCuda&lt;/a&gt; which looks interesting. It looks like there are other possibilities as well though.&lt;/li&gt;
 &lt;li&gt;Clipping capture by green screen/red screen. This should be doable by writing some relatively simple code to detect large amounts of green and red and then ignoring previous/current/subsequent frames as appropriate.&lt;/li&gt;
 &lt;li&gt;Moar test cases! It was initially suggested to use some of the classic benchmarks, but these only seem to barely work on Fennec (at least with the setup I have). I don&amp;#8217;t know if this is fixable or not, but until it is, we might be better off coming up with more reasonable/realistics measures of visual performance.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;You might be able to find other inspiration on the &lt;a href="https://wiki.mozilla.org/Project_Eideticker"&gt;Eideticker project page&lt;/a&gt; (note that some of this is out of date).&lt;/p&gt;

&lt;p&gt;You obviously need the decklink card to perform captures, but the analysis portion of Eideticker can be used/modified on any machine running Linux (Mac should also work, but is untested). To get up and running, just follow the instructions in &lt;a href="https://github.com/mozilla/eideticker/blob/master/README.md"&gt;README.md&lt;/a&gt;, dump a pregenerated capture into the captures/ directory (here&amp;#8217;s &lt;a href="http://people.mozilla.com/~wlachance/clock.zip"&gt;one&lt;/a&gt; of a clock), and off you go! The actual analysis code (such as it is) is currently located in &lt;a href="https://github.com/mozilla/eideticker/blob/master/src/videocapture/videocapture/capture.py"&gt;src/videocapture/videocapture/capture.py&lt;/a&gt; while the web interface is in &lt;a href="https://github.com/mozilla/eideticker/blob/master/src/webapp"&gt;https://github.com/mozilla/eideticker/blob/master/src/webapp&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I&amp;#8217;m going to be out later today (Friday), but I&amp;#8217;m mostly around on IRC M-F 9ish&amp;ndash;5ish EST on irc.mozilla.org #ateam as `wlach`. Feel free to pester me with questions!&lt;/p&gt;

&lt;p&gt;P.S. I didn&amp;#8217;t really cover infrastructure/automation portions above as I suspect people will find that less interesting (especially without a video capture card to test with), but you can look at my &lt;a href="http://groups.google.com/group/mozilla.tools/browse_thread/thread/a469b7909af589de#"&gt;newsgroup post&lt;/a&gt; from yesterday if you want to see what I&amp;#8217;ll likely be up to over the next few weeks.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Eideticker update</title>
  <link rel="alternate" href="http://www.example.com/2011/12/eideticker-update.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2011-12-eideticker-update-html</id>
  <published>2011-12-08T05:00:00Z</published>
  <updated>2011-12-08T05:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Since I last &lt;a href="http://wrla.ch/blog/2011/11/measuring-what-the-user-sees/"&gt;blogged&lt;/a&gt; about Eideticker, I&amp;#8217;ve made some good progress. Here&amp;#8217;s some highlights:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;Eideticker has a new, &lt;a href="https://github.com/mozilla/eideticker/blob/master/bin/runtest.py"&gt;much simpler harness&lt;/a&gt; and tests are much easier to write. Initially, I was using &lt;a href="https://wiki.mozilla.org/Buildbot/Talos"&gt;Talos&lt;/a&gt; for this task with the idea that it&amp;#8217;s better not to have duplicate code where it&amp;#8217;s not really required. Seemed like a fine idea in principle, but in practice Talos&amp;#8217;s architecture (which is really oriented around running a large sequence of tests and uploading the results to a central server) was difficult to extend to do what we need to do. At heart, eideticker really only needs to do a few things right now (start up Firefox, start videocapture, load a webpage, stop videocapture) so it&amp;#8217;s best to keep things simple.&lt;/li&gt;
 &lt;li&gt;I&amp;#8217;ve reworked the capture analysis API to use &lt;a href="http://numpy.scipy.org"&gt;numpy&lt;/a&gt; behind the scenes. It&amp;#8217;s still not quite as fast as I would like (doing a framediff analysis on a 30 second animation still takes a minute or so on my fast machine), but we&amp;#8217;re doing an order of magnitude better than before. numpy also seem to have quite the library of routines for doing the types of matrix algebra useful in image analysis, which should be helpful as the project progresses.&lt;/li&gt;
 &lt;li&gt;I added the beginnings of a fancy pants web interface for browsing captures and doing visualizations on them! I&amp;#8217;m pretty happy with how this is turning out so far, it&amp;#8217;s already been an incredibly useful tool for debugging Eideticker&amp;#8217;s analysis system and I think it will be equally useful for understanding Firefox&amp;#8217;s behaviour in general.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;Here&amp;#8217;s an example analysis session, where I examine a ~60 second capture of the fishtank demo from Microsoft, borrowed from Mark Cote&amp;#8217;s &lt;a href="http://brasstacks.mozilla.com/speedtests/results.html"&gt;speedtest&lt;/a&gt; library. You might want to view this fullscreen:&lt;/p&gt;

&lt;video src="/files/eideticker/eideticker-20111207.webm" width="600px" controls="controls"&gt;&lt;/video&gt;

&lt;p&gt;A few interesting things to note about this capture:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;Our frame comparison algorithm is still comparatively dumb, it just computes the norm of the difference in RGB values between two frames. Since there&amp;#8217;s a (very tiny) amount of noise in the capture, we have to use a threshold to determine whether two frames are the same or not. For all that, the FPS estimate it comes with for the fishtank demo seems about right (and unfortunately at 2 fps, it&amp;#8217;s not particularly good).
  &lt;br /&gt;&lt;/li&gt;
 &lt;li&gt;I added a green screen / red screen at the start / end of every capture to eliminate race conditions with starting the capture, but haven&amp;#8217;t yet actually taken those frames out of the analysis.
  &lt;br /&gt;&lt;/li&gt;
 &lt;li&gt;If you look carefully at the animation, not all of the fish that should be displaying in the demo are. I think this has to do with the new native version of Fennec that I&amp;#8217;m using to test (old versions don&amp;#8217;t exhibit this property). I filed &lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=708633"&gt;a bug&lt;/a&gt; for this.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;What&amp;#8217;s next? Well, as I mentioned last time, the real goal is to create a tool that developers will find useful. To that end, we have plans to set up an Eideticker machine in Mozilla Mountain View office that more people can use (either locally or remotely over the VPN). For this to be workable, I need to figure out how to get the full setup working on &amp;#8220;demand&amp;#8221;. Most of the setup already allows this, with one big exception: the actual Android device that we want to capture video from. The LG G2X that I&amp;#8217;m currently using works fine when I have physical access to it, but as far as I can tell it&amp;#8217;s not possible to get it outputting proper video of an application unless it&amp;#8217;s in an unlocked state (which it obviously isn&amp;#8217;t most of the time).&lt;/p&gt;

&lt;p&gt;My current thinking is that a &lt;a href="http://pandaboard.org/"&gt;Panda Board&lt;/a&gt; running a Vanilla version of Android might be a good candidate for a permanently-connected device. It is capable of HDMI output, doesn&amp;#8217;t have unwanted the bells and whistles of a physical phone (e.g. a lock screen), and should be much reliable due to its physical networking. So far I haven&amp;#8217;t &lt;a href="http://ask.linaro.org/questions/361/hdmi-output-on-android-build"&gt;had much luck&lt;/a&gt; getting it the video output working with the Decklink capture card, but I&amp;#8217;ve only just started trying. Work will continue.&lt;/p&gt;

&lt;p&gt;If I can somehow figure that out, and smooth out some of the rough edges with the web interface and capture API, I think the stage will be set for us all to do some pretty interesting stuff! Looking forward to it.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">A planet to call our own</title>
  <link rel="alternate" href="http://www.example.com/2011/11/a-planet-to-call-our-own.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2011-11-a-planet-to-call-our-own-html</id>
  <published>2011-11-18T05:00:00Z</published>
  <updated>2011-11-18T05:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Just a quick note that a planet for Mozilla Tools &amp;#38; Automation (the so-called &amp;#8220;a team&amp;#8221;) is now up, thanks to Reed Loden. With the exception of &lt;a href="http://k0s.org/mozilla/blog"&gt;Jeff Hammel&lt;/a&gt;, everyone there was already being syndicated on &lt;a href="http://planet.mozilla.org"&gt;Planet Mozilla&lt;/a&gt;, but this should offer a more focused feed of our doings for those who can&amp;#8217;t always keep up with the firehose. Have a look:&lt;/p&gt;

&lt;p&gt;&lt;a href="http://planet.mozilla.org/ateam"&gt;http://planet.mozilla.org/ateam&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Who should care? Well, we maintain all the major testing frameworks like Mochitest, Reftest, and Talos as well as automated tooling for QA like &lt;a href="https://developer.mozilla.org/en/Mozmill"&gt;Mozmill&lt;/a&gt;. Our latest work is focused on making sure that Firefox is as robust, responsive, and performant as possible on desktop and mobile. In short, if you&amp;#8217;re writing or verifying code from mozilla-central, what we&amp;#8217;re doing probably affects you. Please let us know what you think about our projects and whether there&amp;#8217;s anything we can do to make your job easier: we&amp;#8217;re listening.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Quick bonus note&lt;/strong&gt;: It&amp;#8217;s not immediately obvious (or at least it wasn&amp;#8217;t to me), but Mozilla has some fairly finely tuned infrastructure for running planets. If your team or group wants one, it&amp;#8217;s definitely better to plug into that instead of rolling your own. 😉 Reed Loden is the maintainer and the source &lt;a href="http://viewvc.svn.mozilla.org/vc/projects/planet/"&gt;lives in subversion&lt;/a&gt;.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Measuring what the user sees</title>
  <link rel="alternate" href="http://www.example.com/2011/11/measuring-what-the-user-sees.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2011-11-measuring-what-the-user-sees-html</id>
  <published>2011-11-11T05:00:00Z</published>
  <updated>2011-11-11T05:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;I&amp;#8217;ve been spending the last month or so at Mozilla prototyping a new project called &lt;a href="https://wiki.mozilla.org/Project_Eideticker"&gt;Eideticker&lt;/a&gt; which aims to use video capture data and image/frame analysis for performance measurement of &lt;a href="https://wiki.mozilla.org/Mobile/Fennec/Android"&gt;Firefox Mobile&lt;/a&gt;. It&amp;#8217;s still in quite a rough state, but it&amp;#8217;s now complete enough that I thought it would be worth spending a bit of time describing both its motivation and how it works.&lt;/p&gt;

&lt;p&gt;First, a bit of an introduction. Up to now, our automated performance tools have used entirely synthetic benchmarks (how long til we get the onload event? how many ms since we last hit the main loop?) to gather performance information. As we&amp;#8217;ve found out, there&amp;#8217;s a lot you can measure with synthetic benchmarks. Tools like &lt;a href="https://wiki.mozilla.org/Buildbot/Talos"&gt;Talos&lt;/a&gt; have proven themselves by catching performance regressions on a very regular basis.&lt;/p&gt;

&lt;p&gt;Still, there&amp;#8217;s many things that synthetic benchmarks can&amp;#8217;t easily or reliably measure. For example, it&amp;#8217;s nice to know that a page has triggered an &amp;#8220;onload&amp;#8221; event (and the sooner it does that, the better), but what does the browser look like before then? If it&amp;#8217;s a complicated or image intensive page, it might take 10 or 15 seconds to load. In this interval, user studies have clearly shown that an application displaying &lt;em&gt;something&lt;/em&gt; sooner rather than later is always desirable if it&amp;#8217;s not possible to display everything immediately (due to network traffic, CPU constraints, whatever). It&amp;#8217;s this area of user-perceived performance that Eideticker aims to help with. Eideticker creates a system to capture live data of what the browser is displaying, then performs image/frame analysis on the result to see how we&amp;#8217;re actually doing on these inherently subjective metrics. The above was just one example, others might include:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Measuring amount of time it takes to actually see the start page from time of launch.&lt;/li&gt;
 &lt;li&gt;Measuring amount of time you see the checkboard pattern after panning the browser.&lt;/li&gt;
 &lt;li&gt;Measuring the visual artifacts while loading a complicated page (how long does it take to display something? how long until we get something close to the final expected result? how long until we get the actual final result?)&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;It turns out that it&amp;#8217;s possible to put together a system that does this type of analysis using off-the-shelf components. We&amp;#8217;re still very much in the early phase, but initial signs are promising. The initial test system has the following pieces:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;A Linux workstation equipped with a Decklink extreme 3D video capture card&lt;/li&gt;
 &lt;li&gt;An Android phone with HDMI output (currently using the LG G2X)&lt;/li&gt;
 &lt;li&gt;A version of talos modified to video capture the results of a test.&lt;/li&gt;
 &lt;li&gt;A bit of python code to actually analyze the video capture data.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;So far, I&amp;#8217;ve got the system working end-to-end for two simple cases. The first is the &amp;#8220;pageload&amp;#8221; case. This lets you capture the results of loading any page within a talos pageset. Here&amp;#8217;s a quick example of the movie we generate from a tsvg test:&lt;/p&gt;

&lt;video src="http://people.mozilla.com/~wlachance/eideticker-map.webm" width="50%" height="50%" controls="controls"&gt;&lt;/video&gt;

&lt;p&gt;Here&amp;#8217;s another example, a color cycle test (actually the first test case I created, as a throwaway):&lt;/p&gt;

&lt;video src="http://people.mozilla.com/~wlachance/eideticker-colorcycle.webm" width="50%" height="50%" controls="controls"&gt;&lt;/video&gt;

&lt;p&gt;After the video is captured, the next step is to analyze it! As described above (and in further detail on the &lt;a href="https://wiki.mozilla.org/Project_Eideticker"&gt;Eideticker wiki page&lt;/a&gt;), there&amp;#8217;s lots of things we could measure but the easiest thing is probably just to count the number of unique frames and derive a frame rate for the capture based on that (the higher the better, obviously). Based on an initial prototype from Chris Jones, I&amp;#8217;ve started work on a &lt;a href="https://github.com/mozilla/eideticker/blob/master/src/videocapture/videocapture/capture.py"&gt;python library&lt;/a&gt; to do exactly this. Assuming you have an eideticker capture handy, you can run a tool called &amp;#8220;analyze.py&amp;#8221; on the command line, and it&amp;#8217;ll give you its best guess of the # of unique frames:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;&amp;lt;br /&amp;gt;
(eideticker)wlach@eideticker:~/src/eideticker$ bin/analyze.py  ./src/talos/talos/captures/capture-2011-11-11T11:23:51.627183.zip&amp;lt;br /&amp;gt;
Unique frames: 121/272&amp;lt;br /&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;(There are currently some rough edges with this: we&amp;#8217;re doing frame comparisons based on per-pixel changes, but the video capture data is slightly noisy so sometimes a pixel changes its value even when nothing has actually happened in the browser)&lt;/p&gt;

&lt;p&gt;So that&amp;#8217;s what I&amp;#8217;ve got working so far. What&amp;#8217;s next? Short term, we have some &lt;a href="https://wiki.mozilla.org/Auto-tools/Goals/2011Q4#Eideticker"&gt;specific high-level goals&lt;/a&gt; about where we want to be with the system by the end of the quarter. The big unfinished pieces are getting an end-to-end test involving real user interaction (typing into the URL bar, etc.) going and turning this prototype system into something that&amp;#8217;s easy for others to duplicate and is robust enough to be easily extended. Hopefully this will come together fairly quickly now that the basics are in place.&lt;/p&gt;

&lt;p&gt;The longer term picture really depends on feedback from the community. Unlike many of the projects we work on in &lt;a href="https://wiki.mozilla.org/Auto-tools"&gt;automation &amp;#38; tools&lt;/a&gt;, Eideticker is &lt;strong&gt;not&lt;/strong&gt; meant to be something that&amp;#8217;s run on every checkin. Rather, it&amp;#8217;s intended to be a useful tool that can be run on an as needed basis by developers and QA. We obviously have our own ideas on how something like this might be useful (and what a reasonable user interface might be), but I&amp;#8217;ve found in cases like this it&amp;#8217;s much better to go to the people who will actually be using this thing. So with that in mind, here&amp;#8217;s a call for feedback. I have two very specific questions:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Is there a specific problem you&amp;#8217;ve been working on that a framework like this might be helpful for?&lt;/li&gt;
 &lt;li&gt;What do you think of the current workflow model described in the &lt;a href="http://github.com/mozilla/eideticker/README.md"&gt;README&lt;/a&gt;?&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;My goal is to make something that people will love, so please do let me know what you think. Nothing about this project is cast in stone and the last thing I want is to deliver a product that people don&amp;#8217;t actually want to use.&lt;/p&gt;

&lt;p&gt;Equally, while Eideticker is being written primarily with the goal of making Mobile Firefox better (and in the slightly-less short term, desktop Firefox and &lt;a href="https://github.com/andreasgal/B2G"&gt;Boot to Gecko&lt;/a&gt;), much of it is broadly applicable to any user-facing mobile or desktop application. If you think some component of Eideticker might be interesting to your project and want to collaborate, feel free to get in touch.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Faster, but not quite there yet&amp;#8230;</title>
  <link rel="alternate" href="http://www.example.com/2011/10/faster-but-not-quite-there-yet-8230.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2011-10-faster-but-not-quite-there-yet-8230-html</id>
  <published>2011-10-25T04:00:00Z</published>
  <updated>2011-10-25T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;So as others have been posting about, we&amp;#8217;ve been making &lt;a href="http://atlee.ca/blog/2011/10/17/going-faster/"&gt;some headway&lt;/a&gt; on our progress on the GoFaster project. Unfortunately it seems like we&amp;#8217;re still some distance away from reaching our magic number of a 2 hour turnaround for each revision pushed.&lt;/p&gt;

&lt;p&gt;&lt;a href="http://wrla.ch/blog/2011/10/faster-but-not-quite-there-yet/gofaster-e2e-graph-oct25/" rel="attachment wp-att-312"&gt;&lt;img src="/files/2011/10/gofaster-e2e-graph-oct25.png" alt="" title="gofaster-e2e-graph-oct25" width="632" height="511" class="alignnone size-full wp-image-312" srcset="/files/2011/10/gofaster-e2e-graph-oct25-300x242.png 300w, /files/2011/10/gofaster-e2e-graph-oct25.png 632w" sizes="(max-width: 632px) 100vw, 632px" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It&amp;#8217;s a bit hard to see the exact number on the graph (&lt;a href="https://bugzilla.mozilla.org/show_bug.cgi?id=697277"&gt;someone should fix that&lt;/a&gt;), but we seem to teetering around an average of 3 hours at this point. Looking at our &lt;a href="http://brasstacks.mozilla.com/gofaster/#/buildcharts"&gt;build charts&lt;/a&gt;, it seems like the critical path has shifted in many cases from Windows to MacOS X. Is there something we can do to close the gap there? Or is there a more general fix which would lead to substantial savings? If you have any thoughts, or would like to help out, we&amp;#8217;re scheduled to have a &lt;a href="https://wiki.mozilla.org/ReleaseEngineering/BuildFaster/Meetings/2011-10-26"&gt;short meeting tomorrow&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Anyone is welcome to join, but note that we&amp;#8217;re practical, results-oriented people. Crazy ideas are fun, but we&amp;#8217;re most interested in proposals that have measurable data behind them and can be implemented in reasonable amounts of time.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">&amp;#8220;Developers can&amp;#8217;t do UI&amp;#8221;</title>
  <link rel="alternate" href="http://www.example.com/2011/10/-8220-developers-can-8217-t-do-ui-8221.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2011-10-8220-developers-can-8217-t-do-ui-8221-html</id>
  <published>2011-10-07T04:00:00Z</published>
  <updated>2011-10-07T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Despite making a dramatic shift from front-end development to &lt;a href="https://wiki.mozilla.org/Auto-tools"&gt;back-end stuff&lt;/a&gt; since I started at Mozilla a few months ago, I&amp;#8217;ve still had occasion to have to do a fair bit of user-facing code, even if an audience of other developers is a bit more limited than what I&amp;#8217;ve been used to. Since my mission is to make the rest of Mozilla more productive, it&amp;#8217;s worth putting a bit of time and intention into the user interface for my stuff. If I can reduce learning curves or streamline day-to-day workflows, that&amp;#8217;s a win for everyone since they can spend that much more time rocking at their jobs (whether that be release engineering, platform work, or whatever). This brings up a point that I&amp;#8217;ve had in the back of my mind for a while:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Despite conventional wisdom, developers can design half-decent user interfaces (if they try)!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I used to be certain that a project really needed graphic designers and/or usability experts to provide guidance on UI issues, but my experience over the last few years with iOS/web development has made me reconsider. Sure, pixel pushing and vector art is never going to be a programmer&amp;#8217;s strong suit (and there&amp;#8217;s certain high-level techniques that take years of study to acquire/understand), but the basic principles behind good UI design are accessible to anyone. There&amp;#8217;s really only three core skills:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;
  &lt;p&gt;An ability to put yourself in the shoes of the user. Who are you designing for, and what are they trying to accomplish? How can I streamline my UI to help them quickly solve the task at hand? This is one of the reasons why I find &lt;a href="http://en.wikipedia.org/wiki/User_story"&gt;user stories&lt;/a&gt; so helpful.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;An understanding of common vocabulary for describing/designing applications and knowing what is &amp;#8220;good&amp;#8221;. Unfortunately I haven&amp;#8217;t found anything like this for the web, but &lt;a href="http://developer.apple.com/library/ios/#documentation/UserExperience/Conceptual/MobileHIG/UEBestPractices/UEBestPractices.html#//apple_ref/doc/uid/TP40006556-CH20-SW1"&gt;Apple&amp;#8217;s human interface guidelines&lt;/a&gt; have some good general advice on this (just ignore the stuff specific to phones/tablet apps if that&amp;#8217;s not what you&amp;#8217;re doing).&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;A willingness to iterate. The best ideas usually aren&amp;#8217;t apparent immediately, and may only come out of a back forth. It&amp;#8217;s been my experience that the more constructive dialog there is between people actively involved in the project on user experience issues, the better the end result is likely to be.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;For example, one of the things that release engineering has found most useful in the GoFaster Dashboard has been the &lt;a href="http://brasstacks.mozilla.com/gofaster/#/buildcharts"&gt;build charts&lt;/a&gt;. Believe it or not, the idea for that view started out as this &lt;a href="http://people.mozilla.com/~wlachance/overall-build-and-test-area.png"&gt;useless piece of junk&lt;/a&gt; (I can say that because I created it). It was only after a good half hour back and forth on irc between myself, jgriffin, and jmaher (all of us backend/tool developers) that we came up with the view that inspired so much &lt;a href="http://jagriffin.wordpress.com/2011/09/06/gofaster-deeper-data-analysis/"&gt;good analysis&lt;/a&gt; on the project.&lt;/p&gt;

&lt;p&gt;All this is not to say that usability experts and graphic designers don&amp;#8217;t have special skills that are worthy of respect. Indeed, if you&amp;#8217;re a designer and would like to get involved with our work, please &lt;a href="https://wiki.mozilla.org/Auto-tools#Want_to_Help.3F"&gt;join us&lt;/a&gt;, we&amp;#8217;d love your help. My only point is that on a project where a design resource isn&amp;#8217;t available, thinking explicitly about usability is still worthwhile. And even where you have a UX expert on staff, programmers can have useful feedback too. Good UI is everyone&amp;#8217;s responsibility!&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Changes!</title>
  <link rel="alternate" href="http://www.example.com/2011/09/changes.html?utm_source=Mozilla&amp;utm_medium=Atom" />
  <id>urn:http-www-example-com:-2011-09-changes-html</id>
  <published>2011-09-01T04:00:00Z</published>
  <updated>2011-09-01T04:00:00Z</updated>
  <author>
   <name>The Unknown Author</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;A bit quiet here for the last few months. What&amp;#8217;s been happening?&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;I got married and had a &lt;a href="http://www.flickr.com/photos/wlach/sets/72157626998555629"&gt;wonderful honeymoon&lt;/a&gt; in France.
  &lt;br /&gt;&lt;/li&gt;
 &lt;li&gt;I started a fantastic new job with Mozilla&amp;#8217;s &lt;a href="https://wiki.mozilla.org/Auto-tools/"&gt;tools &amp;#38; automation&lt;/a&gt; group. Currently working on bringing down the build/test times for Firefox (part of a project called &lt;a href="http://brasstacks.mozilla.com/gofaster/"&gt;GoFaster&lt;/a&gt;), which has been really interesting.
  &lt;br /&gt;&lt;/li&gt;
 &lt;li&gt;I moved into a fantastic new apartment in an old victorian building near Vend&amp;ocirc;me metro.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;In short, life has been treating me really well! More updates soon.&lt;/p&gt;&lt;/html&gt;</content></entry></feed>