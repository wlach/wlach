<!DOCTYPE html>
<html lang="en">
  <head>

    <meta charset="utf-8" />
    <title>Posts tagged 'Data Visualization'</title>
    <meta name="description" content="Posts tagged 'Data Visualization'" />
    <meta name="author" content="William Lachance" />
    <meta name="keywords" content="Data Visualization" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="icon" href="/favicon.ico" />
    <link rel="canonical" href="https://wrla.ch/tags/Data-Visualization.html" />

    <!-- CSS -->
    <link rel="stylesheet" type="text/css" href="/css/style.css" />
    <link
      rel="stylesheet"
      type="text/css"
      href="/css/pygments.css"
    />
    <link
      rel="stylesheet"
      type="text/css"
      href="/css/scribble.css"
    />
    <!-- Feeds -->
    <link
      rel="alternate"
      type="application/atom+xml"
      href="/feeds/Data-Visualization.atom.xml"
      title="Atom Feed"
    />
    <link
      rel="alternate"
      type="application/rss+xml"
      href="/feeds/Data-Visualization.rss.xml"
      title="RSS Feed"
    />
    <!-- JS -->
    <script type="text/javascript">
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-xxxxx', 'auto');
      ga('send', 'pageview');
    </script>
  </head>
  <body>
    <nav
      class="flex items-center justify-between flex-wrap bg-gray-800 py-1 px-8"
    >
      <div class="flex items-center flex-shrink-0 text-gray-400 mr-6">
        <div class="p-1">
          <a href="/index.html"
            ><img
              src="/img/wlach_icon.png"
              width="32"
              height="32"
              class="p rounded"
          /></a>
        </div>
        <div class="p-1">
          <a
            href="/index.html"
            class="text-gray-200 font-semibold text-xl tracking-tight hover:text-white"
            >wlach log</a
          >
        </div>
      </div>
      <div class="flex-grow lg:flex lg:items-center">
        <div class="text-sm lg:flex-grow">
          <a
            href="/About.html"
            class="mt-4 lg:inline-block lg:mt-0 hover:text-white mr-4 text-gray-600"
          >
            About</a>
          <a
            class="mt-4 lg:inline-block lg:mt-0 text-gray-600 hover:text-white mr-4"
            href="/feeds/Data-Visualization.atom.xml"
            >Atom</a
          >
          <a
            class="mt-4 lg:inline-block lg:mt-0 text-gray-600 hover:text-white mr-4"
            href="/feeds/Data-Visualization.rss.xml"
            >RSS</a
          >
        </div>
      </div>
    </nav>
    <div id="content" class="container max-w-screen-md px-8 py-4 mx-auto">
       <p class="less-important">Showing posts tagged <em>Data Visualization</em></p>  <article>
  <header>
    <h2><a href="/blog/2018/02/derived-versus-direct/">Derived versus direct</a></h2>
    <p class="index-date">Feb 12th, 2018</p>
    <p><span class="tags"><a href="/tags/Mozilla.html">Mozilla</a>  <a href="/tags/Data-Visualization.html">Data Visualization</a>  <a href="/tags/Mission-Control.html">Mission Control</a></span></p>
  </header>

<p>To attempt to make complex phenomena more understandable, we often use derived measures when representing Telemetry data at Mozilla. For error rates for example, we often measure things in terms of &ldquo;X per khours of use&rdquo; (where X might be &ldquo;main crashes&rdquo;, &ldquo;appearance of the slow script dialogue&rdquo;). I.e. instead of showing a raw <em>count</em> of errors we show a rate. Normally this is a good thing: it allows the user to easily compare two things which might have different raw numbers for whatever reason but where you&rsquo;d normally expect the ratio to be similar. For example, we see that although the <em>uptake</em> of the newly-released Firefox 58.0.2 is a bit slower than 58.0.1, the overall crash rate (as sampled every 5 minutes) is more or less the same after about a day has rolled around:</p>

<p><img srcset="/files/2018/02/main_crashes_normalized.png" /></p>

<p>On the other hand, looking at raw counts doesn&rsquo;t really give you much of a hint on how to interpret the results. Depending on the scale of the graph, the actual rates could actually resolve to being vastly different:</p>

<p><img srcset="/files/2018/02/main_crashes_raw.png" /></p>

<p>Ok, so this simple tool (using a ratio) is useful. Yay! Unfortunately, there is one case where using this technique can lead to a very deceptive visualization: when the number of samples is really small, a few outliers can give a really false impression of what&rsquo;s really happening. Take this graph of what the crash rate looked like <em>just after</em> Firefox 58.0 was released:</p>

<p><img srcset="/files/2018/02/relative_small_crash_counts.png" /></p>

<p>10 to 100 errors per 1000 hours, say it isn&rsquo;t so? But wait, how many errors do we have absolutely? Hovering over a representative point in the graph with the normalization (use of a ratio) turned off:</p>

<p><img srcset="/files/2018/02/absolute_small_crash_counts.png" /></p>

<p>We&rsquo;re really only talking about something between 1 to 40 crashes events over a relatively small number of usage hours. This is clearly so little data that we can&rsquo;t (and shouldn&rsquo;t) draw any kind of conclusion whatsoever.</p>

<p>Ok, so that&rsquo;s just science 101: don&rsquo;t jump to conclusions based on small, vastly unrepresentative samples. Unfortunately due to human psychology people tend to assume that charts like this are authoritative and represent something real, absent an explanation otherwise &mdash; and the use of a ratio obscured the one fact (extreme lack of data) that would have given the user a hint on how to correctly interpret the results. Something to keep in mind as we build our tools.</p> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2017/10/better-or-worse-by-what-measure/">Better or worse: by what measure?</a></h2>
    <p class="index-date">Oct 26th, 2017</p>
    <p><span class="tags"><a href="/tags/Mozilla.html">Mozilla</a>  <a href="/tags/Data-Visualization.html">Data Visualization</a>  <a href="/tags/Mission-Control.html">Mission Control</a></span></p>
  </header>

<p>Ok, after a series of posts extolling the virtues of my current project, it&rsquo;s time to take a more critical look at some of its current limitations, and what we might do about them. In my <a href="/blog/2017/10/mission-control/">introductory post</a>, I talked about how Mission Control can let us know how &ldquo;crashy&rdquo; a new release is, within a short interval of it being released. I also alluded to the fact that things appear considerably worse when something first goes out, though I didn&rsquo;t go into a lot of detail about how and why that happens.</p>

<p>It just so happens that a new point release (56.0.2) just went out, so it&rsquo;s a perfect opportunity to revisit this issue. Let&rsquo;s take a look at what the graphs are saying (each of the images is also a link to the dashboard where they were generated):</p>

<p><a href="https://data-missioncontrol.dev.mozaws.net/#/release/windows/content_crashes?timeInterval=172740&amp;percentile=99&amp;normalized=1&amp;disabledVersions=&amp;versionGrouping=version&amp;startTime=1508990400"><img srcset="/files/2017/10/missioncontrol_windows_content_crashes_56.0.2.png 2x" /></a></p>

<p>ZOMG! It looks like 56.0.2 is off the charts relative to the two previous releases (56.0 and 56.0.1). Is it time to sound the alarm? Mission control abort? Well, let&rsquo;s see what happens the last time we rolled something new out, say 56.0.1:</p>

<p><a href="https://data-missioncontrol.dev.mozaws.net/#/release/windows/content_crashes?timeInterval=345540&amp;percentile=99&amp;normalized=1&amp;disabledVersions=&amp;versionGrouping=version&amp;startTime=1507435200"><img srcset="/files/2017/10/missioncontrol_windows_content_crashes_56.0.1.png 2x" /></a></p>

<p>We see the exact same pattern. Hmm. How about 56.0?</p>

<p><a href="https://data-missioncontrol.dev.mozaws.net/#/release/windows/content_crashes?timeInterval=431940&amp;percentile=99&amp;normalized=1&amp;disabledVersions=&amp;versionGrouping=version&amp;startTime=1506398400"><img srcset="/files/2017/10/missioncontrol_windows_content_crashes_56.png 2x" /></a></p>

<p>Yep, same pattern here too (actually slightly worse).</p>

<p>What could be going on? Let&rsquo;s start by reviewing what these time series graphs are based on. Each point on the graph represents the number of crashes reported by telemetry &ldquo;main&rdquo; pings corresponding to that channel/version/platform within a five minute interval, divided by the number of usage hours (how long users have had Firefox open) also reported in that interval. A main ping is submitted under <a href="https://firefox-source-docs.mozilla.org/toolkit/components/telemetry/telemetry/data/main-ping.html">a few circumstances</a>:</p>

<ul>
 <li>The user shuts down Firefox</li>
 <li>It’s been about 24 hours since the last time we sent a main ping.</li>
 <li>The user starts Firefox after Firefox failed to start properly</li>
 <li>The user changes something about Firefox’s environment (adds an addon, flips a user preference)</li></ul>

<p>A high crash rate either means a larger number of crashes over the same number of usage hours, or a lower number of usage hours over the same number of crashes. There are several likely explanations for why we might see this type of crashy behaviour immediately after a new release:</p>

<ul>
 <li>A Firefox update is applied after the user restarts their browser for any  reason, including their browser crash. Thus a user whose browser crashes a  lot (for any reason), is more prone to update to the latest version sooner  than a user that doesn’t crash as much.</li>
 <li>Inherently, any crash data submitted to telemetry after a new version is  released will have a low number of usage hours attached, because the  client would not have had a chance to use it much (because it&rsquo;s so new).</li></ul>

<p>Assuming that we&rsquo;re reasonably satisfied with the above explanation, there&rsquo;s a few things we could try to do to correct for this situation when implementing an &ldquo;alerting&rdquo; system for mission control (the next item on my todo list for this project):</p>

<ul>
 <li>Set &ldquo;error&rdquo; thresholds for each crash measure sufficiently high that  we don&rsquo;t consider these high initial values an error (i.e. only alert  if there is are 500 crashes per 1k hours).</li>
 <li>Only trigger an error threshold when some kind of minimum quantity of  usage hours has been observed (this has the disadvantage of potentially  obscuring a serious problem until a large percentage of the user population  is affected by it).</li>
 <li>Come up with some expected range of what we expect a value to be for  when a new version of firefox is first released and ratchet  that down as time goes on (according to some kind of model of our previous expectations).</li></ul>

<p>The initial specification for this project called for just using raw thresholds for these measures (discounting usage hours), but I&rsquo;m becoming increasingly convinced that won&rsquo;t cut it. I&rsquo;m not a quality control expert, but 500 crashes for 1k hours of use sounds completely unacceptable if we&rsquo;re measuring things at all accurately (which I believe we are given a sufficient period of time). At the same time, generating 20&ndash;30 “alerts” every time a new release went out wouldn’t particularly helpful either. Once again, we’re going to have to do this the hard way&hellip;</p>

<p>&mdash;</p>

<p>If this sounds interesting and you have some react/d3/data visualization skills (or would like to gain some), <a href="/blog/2017/10/mission-control-ready-for-contributions/">learn about contributing to mission control</a>.</p>

<p>Shout out to <a href="https://chuttenblog.wordpress.com/">chutten</a> for reviewing this post and providing feedback and additions.</p> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2017/10/mission-control-ready-for-contributions/">Mission Control: Ready for contributions</a></h2>
    <p class="index-date">Oct 20th, 2017</p>
    <p><span class="tags"><a href="/tags/Mozilla.html">Mozilla</a>  <a href="/tags/Data-Visualization.html">Data Visualization</a>  <a href="/tags/Mission-Control.html">Mission Control</a></span></p>
  </header>

<p>One of the great design decisions that was made for <a href="https://treeherder.mozilla.org">Treeherder</a> was a strict seperation of the client and server portions of the codebase. While its backend was moderately complicated to get up and running (especially into a state that looked at all like what we were running in production), you could get its web frontend running (pointed against the production data) just by starting up a simple node.js server. This dramatically lowered the barrier to entry, for Mozilla employees and casual contributors alike.</p>

<p>I knew right from the beginning that I wanted to take the same approach with <a href="https://wlach.github.io/blog/2017/10/mission-control/">Mission Control</a>. While the full source of the project is available, unfortunately it isn&rsquo;t presently possible to bring up the full stack with real data, as that requires privileged access to the athena/parquet error aggregates table. But since the UI is self-contained, it&rsquo;s quite easy to bring up a development environment that allows you to freely browse the cached data which is stored server-side (essentially: <code>git clone https://github.com/mozilla/missioncontrol.git &amp;&amp; yarn install &amp;&amp; yarn start</code>).</p>

<p>In my experience, the most interesting problems when it comes to projects like these center around the question of how to present extremely complex data in a way that is intuitive but not misleading. Probably 90% of that work happens in the frontend. In the past, I&rsquo;ve had pretty good luck finding contributors for my projects (especially <a href="/tags/Perfherder.html">Perfherder</a>) by doing call-outs on this blog. So let it be known: If Mission Control sounds like an interesting project and you know <a href="https://reactjs.org/">React</a>/<a href="http://redux.js.org/">Redux</a>/<a href="https://d3js.org/">D3</a>/<a href="https://www.metricsgraphicsjs.org/">MetricsGraphics</a> (or want to learn), let&rsquo;s work together!</p>

<p>I&rsquo;ve created some <a href="https://github.com/mozilla/missioncontrol/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">good first bugs</a> to tackle in the github issue tracker. From there, I have a galaxy of other work in mind to improve and enhance the usefulness of this project. Please get in touch with me (wlach) on <a href="https://wiki.mozilla.org/IRC">irc.mozilla.org</a> #missioncontrol if you want to discuss further.</p> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2017/10/mission-control/">Mission Control</a></h2>
    <p class="index-date">Oct 6th, 2017</p>
    <p><span class="tags"><a href="/tags/Mozilla.html">Mozilla</a>  <a href="/tags/Data-Visualization.html">Data Visualization</a>  <a href="/tags/Mission-Control.html">Mission Control</a></span></p>
  </header>

<p>Time for an overdue post on the mission control project that I&rsquo;ve been working on for the past few quarters, since I transitioned to the data platform team.</p>

<p>One of the gaps in our data story when it comes to Firefox is being able to see how a new release is doing in the immediate hours after release. Tools like <a href="https://crash-stats.mozilla.com/home/product/Firefox">crashstats</a> and the <a href="https://telemetry.mozilla.org/new-pipeline/evo.html">telemetry evolution dashboard</a> are great, but it can take many hours (if not days) before you can reliably see that there is an issue in a metric that we care about (number of crashes, say). This is just too long &mdash; such delays unnecessarily retard rolling out a release when it is safe (because there is a paranoia that there might be some lingering problem which we we&rsquo;re waiting to see reported). And if, somehow, despite our abundant caution a problem <em>did</em> slip through it would take us some time to recognize it and roll out a fix.</p>

<p>Enter mission control. By hooking up a high-performance spark streaming job directly to our ingestion pipeline, we can now be able to detect within moments whether firefox is performing unacceptably within the field according to a particular measure.</p>

<p>To make the volume of data manageable, we create a grouped data set with the raw count of the various measures (e.g. main crashes, content crashes, slow script dialog counts) along with each unique combination of dimensions (e.g. platform, channel, release).</p>

<p>Of course, all this data is not so useful without a tool to visualize it, which is what I&rsquo;ve been spending the majority of my time on. The idea is to be able to go from a top level description of what&rsquo;s going on a particular channel (release for example) all the way down to a detailed view of how a measure has been performing over a time interval:</p>

<p><img srcset="/files/2017/10/missioncontrol-ui.png 2x" /></p>

<p>This particular screenshot shows the volume of content crashes (sampled every 5 minutes) over the last 48 hours on windows release. You&rsquo;ll note that the later version (56.0) seems to be much crashier than earlier versions (55.0.3) which would seem to be a problem except that the populations are not directly comparable (since the profile of a user still on an older version of Firefox is rather different from that of one who has already upgraded). This is one of the still unsolved problems of this project: finding a reliable, automatable baseline of what an &ldquo;acceptable result&rdquo; for any particular measure might be.</p>

<p>Even still, the tool can still be useful for exploring a bunch of data quickly and it has been progressing rapidly over the last few weeks. And like almost everything Mozilla does, both the <a href="https://github.com/mozilla/missioncontrol/">source</a> and <a href="https://data-missioncontrol.dev.mozaws.net/">dashboard</a> are open to the public. I&rsquo;m planning on flagging some easier bugs for newer contributors to work on in the next couple weeks, but in the meantime if you&rsquo;re interested in this project and want to get involved, feel free to look us up on irc.mozilla.org #missioncontrol (I&rsquo;m there as &lsquo;wlach&rsquo;).</p> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2015/08/more-perfherder-updates/">More Perfherder updates</a></h2>
    <p class="index-date">Aug 7th, 2015</p>
    <p><span class="tags"><a href="/tags/Data-Visualization.html">Data Visualization</a>  <a href="/tags/Mozilla.html">Mozilla</a>  <a href="/tags/Perfherder.html">Perfherder</a></span></p>
  </header>

<p>Since my last update, we&rsquo;ve been trucking along with improvements to <a href="https://wiki.mozilla.org/Auto-tools/Projects/Perfherder">Perfherder</a>, the project for making Firefox performance sheriffing and analysis easier.</p>

<p><strong>Compare visualization improvements</strong></p>

<p>I&rsquo;ve been spending quite a bit of time trying to fix up the display of information in the compare view, to address feedback from developers and hopefully generally streamline things. <a href="https://blog.mozilla.org/vdjeric/">Vladan</a> (from the perf team) referred me to <a href="https://mozillians.org/en-US/u/bwinton/">Blake Winton</a>, who provided tons of awesome suggestions on how to present things more concisely.</p>

<p>Here&rsquo;s an old versus new picture:</p>

<table>
 <tr>
  <td><a href="/files/2015/07/Screen-Shot-2015-07-14-at-3.53.20-PM.png"><img src="/files/2015/07/Screen-Shot-2015-07-14-at-3.53.20-PM-300x206.png" alt="Screen Shot 2015-07-14 at 3.53.20 PM" width="300" height="206" class="alignnone size-medium wp-image-1218" srcset="/files/2015/07/Screen-Shot-2015-07-14-at-3.53.20-PM-300x206.png 300w, /files/2015/07/Screen-Shot-2015-07-14-at-3.53.20-PM.png 980w" sizes="(max-width: 300px) 100vw, 300px" /></a></td> 
  <td><a href="/files/2015/08/Screen-Shot-2015-08-07-at-1.57.39-PM.png"><img src="/files/2015/08/Screen-Shot-2015-08-07-at-1.57.39-PM-300x178.png" alt="Screen Shot 2015-08-07 at 1.57.39 PM" width="300" height="178" class="alignnone size-medium wp-image-1229" srcset="/files/2015/08/Screen-Shot-2015-08-07-at-1.57.39-PM-300x178.png 300w, /files/2015/08/Screen-Shot-2015-08-07-at-1.57.39-PM.png 1003w" sizes="(max-width: 300px) 100vw, 300px" /></a></td></tr></table>

<p>Summary of significant changes in this view:</p>

<ul>
 <li>Removed or consolidated several types of numerical information which were overwhelming or confusing (e.g. presenting both numerical and percentage standard deviation in their own columns).</li>
 <li>Added tooltips all over the place to explain what&rsquo;s being displayed.</li>
 <li>Highlight more strongly when it appears there aren&rsquo;t enough runs to make a definitive determination on whether there was a regression or improvement.</li>
 <li>Improve display of visual indicator of magnitude of regression/improvement (providing a pseudo-scale showing where the change ranges from 0% : 20%+).</li>
 <li>Provide more detail on the two changesets being compared in the header and make it easier to retrigger them (thanks to Mike Ling).</li>
 <li>Much better and more intuitive error handling when something goes wrong (also thanks to Mike Ling).</li></ul>

<p>The point of these changes isn&rsquo;t necessarily to make everything &ldquo;immediately obvious&rdquo; to people. We&rsquo;re not building general purpose software here: Perfherder will always be a rather specialized tool which presumes significant domain knowledge on the part of the people using it. However, even for our audience, it turns out that there&rsquo;s a lot of room to improve how our presentation: reducing the amount of extraneous noise helps people zero in on the things they really need to care about.</p>

<p>Special thanks to everyone who took time out of their schedules to provide so much good feedback, in particular <a href="http://avih.github.io/">Avi Halmachi</a>, <a href="http://glandium.org/blog/">Glandium</a>, and <a href="http://elvis314.wordpress.com/">Joel Maher</a>.</p>

<p>Of course more suggestions are always welcome. Please <a href="https://treeherder.mozilla.org/perf.html#/comparechooser">give it a try</a> and <a href="https://bugzilla.mozilla.org/enter_bug.cgi?product=Tree%20Management&amp;component=Perfherder">file bugs against the perfherder component</a> if you find anything you&rsquo;d like to see changed or improved.</p>

<p><strong>Getting the word out</strong></p>

<pre><code>Hammersmith:mozilla-central wlach$ hg push -f try
pushing to ssh://hg.mozilla.org/try
no revisions specified to push; using . to avoid pushing multiple heads
searching for changes
remote: waiting for lock on repository /repo/hg/mozilla/try held by 'hgssh1.dmz.scl3.mozilla.com:8270'
remote: got lock after 4 seconds
remote: adding changesets
remote: adding manifests
remote: adding file changes
remote: added 1 changesets with 1 changes to 1 files
remote: Trying to insert into pushlog.
remote: Inserted into the pushlog db successfully.
remote:
remote: View your change here:
remote:   https://hg.mozilla.org/try/rev/e0aa56fb4ace
remote:
remote: Follow the progress of your build on Treeherder:
remote:   https://treeherder.mozilla.org/#/jobs?repo=try&amp;revision=e0aa56fb4ace
remote:
remote: It looks like this try push has talos jobs. Compare performance against a baseline revision:
remote:   https://treeherder.mozilla.org/perf.html#/comparechooser?newProject=try&amp;newRevision=e0aa56fb4ace</code></pre>

<p>Try pushes incorporating Talos jobs now automatically link to perfherder&rsquo;s compare view, both in the output from mercurial and in the emails the system sends. One of the challenges we&rsquo;ve been facing up to this point is just letting developers know that Perfherder <em>exists</em> and it can help them either avoid or resolve performance regressions. I believe this will help.</p>

<p><strong>Data quality and ingestion improvements</strong></p>

<p>Over the past couple weeks, we&rsquo;ve been comparing our regression detection code when run against Graphserver data to Perfherder data. In doing so, we discovered that we&rsquo;ve sometimes been using the wrong algorithm (geometric mean) to summarize some of our tests, leading to unexpected and less meaningful results. For example, the v8_7 benchmark uses a custom weighting algorithm for its score, to account for the fact that the things it tests have a particular range of expected values.</p>

<p>To hopefully prevent this from happening again in the future, we&rsquo;ve decided to move the test summarization code out of Perfherder back into Talos (<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1184966">bug 1184966</a>). This has the additional benefit of creating a stronger connection between the content of the Talos logs and what Perfherder displays in its comparison and graph views, which has thrown people off in the past.</p>

<p><strong>Continuing data challenges</strong></p>

<p>Having better tools for visualizing this stuff is great, but it also highlights some continuing problems we&rsquo;ve had with data quality. It turns out that our automation setup often produces <em>qualitatively different</em> performance results for the exact same set of data, depending on when and how the tests are run.</p>

<p>A certain amount of random noise is always expected when running performance tests. As much as we might try to make them uniform, our testing machines and environments are just not 100% identical. That we expect and can deal with: our standard approach is just to retrigger runs, to make sure we get a representative sample of data from our population of machines.</p>

<p>The problem comes when there&rsquo;s a <em>pattern</em> to the noise: we&rsquo;ve already noticed that tests run on the weekends produce different results (see Joel&rsquo;s post from a year ago, <a href="https://elvis314.wordpress.com/2014/10/30/a-case-of-the-weekends/">&ldquo;A case of the weekends&rdquo;</a>) but it seems as if there&rsquo;s other circumstances where one set of results will be different from another, depending on the time that each set of tests was run. Some tests and platforms (e.g. the a11yr suite, MacOS X 10.10) seem particularly susceptible to this issue.</p>

<p>We need to find better ways of dealing with this problem, as it can result in a lot of wasted time and energy, for both sheriffs and developers. See for example <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1190877">bug 1190877</a>, which concerned a completely spurious regression on the tresize benchmark that was initially blamed on some changes to the media code: in this case, Joel speculates that the linux64 test machines we use might have changed from under us in some way, but we really don&rsquo;t know yet.</p>

<p>I see two approaches possible here:</p>

<ol>
 <li>Figure out what&rsquo;s causing the same machines to produce qualitatively different result distributions and address that. This is of course the ideal solution, but it requires coordination with other parts of the organization who are likely quite busy and might be hard.</li>
 <li>Figure out better ways of detecting and managing these sorts of case. I have noticed that the standard deviation inside the results when we have spurious regressions/improvements tends to be higher (see for example <a href="https://treeherder.mozilla.org/perf.html#/compare?originalProject=mozilla-inbound&amp;originalRevision=4d0818791d07&amp;newProject=mozilla-inbound&amp;newRevision=5e130ad70aa7">this compare view</a> for the aforementioned &ldquo;regression&rdquo;). Knowing what we do, maybe there&rsquo;s some statistical methods we can use to detect bad data?</li></ol>

<p>For now, I&rsquo;m leaning towards (2). I don&rsquo;t think we&rsquo;ll ever completely solve this problem and I think coming up with better approaches to understanding and managing it will pay the largest dividends. Open to other opinions of course!</p> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2015/04/a-virtual-petri-dish/">A virtual petri dish</a></h2>
    <p class="index-date">Apr 25th, 2015</p>
    <p><span class="tags"><a href="/tags/Data-Visualization.html">Data Visualization</a></span></p>
  </header>

<p>Was feeling a bit restless today, so I decided to build something on a theme I&rsquo;d been thinking of since, oh gosh, I guess high school &#8212; an ecosystem simulation.</p>

<p>My original concept for it had three different types of entities &#8212; grass, rabbits, and foxes wandering around in a fixed environment. Each would eat the previous and try to reproduce. Both the rabbits and foxes need to continually eat to survive, otherwise they will die. The grass will just grow unprompted. I think I may have picked up the idea from elsewhere, but am not sure (it&rsquo;s been nearly 17 years after all).</p>

<p>I suppose the urge to do this comes from my fascination with the concepts of birth, death, and rebirth. Conway&rsquo;s <a href="http://en.wikipedia.org/wiki/Conway%27s_Game_of_Life">game of life</a> is probably the most famous computer representation of this sort of theme, but I always found the behavior slightly too contrived and simple to be deeply satisfying to me (at least from the point of view of representing this concept: the game is certainly interesting for other reasons). Conway&rsquo;s simulation is completely deterministic and only has one type of entity, the cell. There&rsquo;s an element of randomness and hierarchy in the real world, and I wanted to represent these somehow.</p>

<p>It was remarkably easy to get things going using my preferred toolkit for these things (Javascript and Canvas) &#8212; about 3 hours to get something on the screen, then a bunch of tweaking until I found the behavior I wanted. Either I&rsquo;m getting smarter or the tools to build these things are getting better. Probably the latter.</p>

<p>In the end, I only wound up having rabbits and grass in my simulation in this iteration and went for a very abstract representation of what was going on (colored squares for everything!). It turns out that no more than that was really necessary to create something that held my interest. Here&rsquo;s a screenshot (doesn&rsquo;t really do it justice):</p>

<p><a href="/files/2015/04/Screen-Shot-2015-04-25-at-10.24.21-PM.png"><img src="/files/2015/04/Screen-Shot-2015-04-25-at-10.24.21-PM.png" alt="Screen Shot 2015-04-25 at 10.24.21 PM" width="1002" height="1006" class="alignnone size-full wp-image-1198" srcset="/files/2015/04/Screen-Shot-2015-04-25-at-10.24.21-PM-150x150.png 150w, /files/2015/04/Screen-Shot-2015-04-25-at-10.24.21-PM-298x300.png 298w, /files/2015/04/Screen-Shot-2015-04-25-at-10.24.21-PM.png 1002w" sizes="(max-width: 1002px) 100vw, 1002px" /></a></p>

<p>If you&rsquo;d like to check it out for yourself, I put a copy on my website <a href="http://wrla.ch/eco">here</a>. It probably requires a fairly fancy computer to run at a decent speed (I built it using a 2014 MacBook Pro and made very little effort to optimize it). If that doesn&rsquo;t work out for you, I put up a <a href="https://youtu.be/LwLFw1_GGnU">video capture of the simulation on youtube</a>.</p>

<p>The math and programming behind the simulation is completely arbitrary and anything but rigorous. There are probably a bunch of bugs and unintended behaviors. This has all probably been done a million times before by people I&rsquo;ve never met and never will. I&rsquo;m ok with that.</p>

<p><strong>Update</strong>: <a href="https://github.com/wlach/ecoautomata">Source now on github</a>, for those who want to play with it and submit pull requests.</p> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2015/03/perfherder-update-summary-series-drilldown/">Perfherder update: Summary series drilldown</a></h2>
    <p class="index-date">Mar 27th, 2015</p>
    <p><span class="tags"><a href="/tags/Data-Visualization.html">Data Visualization</a>  <a href="/tags/Mozilla.html">Mozilla</a>  <a href="/tags/Perfherder.html">Perfherder</a></span></p>
  </header>

<p>Just wanted to give another quick Perfherder update. Since the <a href="http://wrla.ch/blog/2015/02/measuring-e10s-vs-non-e10s-performance-with-perfherder/">last time</a>, I&rsquo;ve added summary series (which is what GraphServer shows you), so we now have (in theory) the best of both worlds when it comes to Talos data: aggregate summaries of the various suites we run (tp5, tart, etc), with the ability to dig into individual results as needed. This kind of analysis wasn&rsquo;t possible with Graphserver and I&rsquo;m hopeful this will be helpful in tracking down the root causes of Talos regressions more effectively.</p>

<p>Let&rsquo;s give an example of where this might be useful by showing how it can highlight problems. Recently we tracked a regression in the Customization Animation Tests (CART) suite from the commit in <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1128354">bug 1128354</a>. Using <a href="https://mozillians.org/en-US/u/mishravikas/">Mishra Vikas</a>&#8216;s new &ldquo;highlight revision mode&rdquo; in Perfherder (combined with the revision hash when the regression was pushed to inbound), we can quickly zero in on the location of it:</p>

<p><a href="/files/2015/03/Screen-Shot-2015-03-27-at-3.18.28-PM.png"><img src="/files/2015/03/Screen-Shot-2015-03-27-at-3.18.28-PM-1024x498.png" alt="Screen Shot 2015-03-27 at 3.18.28 PM" width="474" height="230" class="alignnone size-large wp-image-1184" srcset="/files/2015/03/Screen-Shot-2015-03-27-at-3.18.28-PM-300x146.png 300w, /files/2015/03/Screen-Shot-2015-03-27-at-3.18.28-PM-1024x498.png 1024w, /files/2015/03/Screen-Shot-2015-03-27-at-3.18.28-PM.png 1167w" sizes="(max-width: 474px) 100vw, 474px" /></a></p>

<p>It does indeed look like things ticked up after this commit for the CART suite, but why? By clicking on the datapoint, you can open up a subtest summary view beneath the graph:</p>

<p><a href="/files/2015/03/Screen-Shot-2015-03-27-at-2.35.25-PM.png"><img src="/files/2015/03/Screen-Shot-2015-03-27-at-2.35.25-PM.png" alt="Screen Shot 2015-03-27 at 2.35.25 PM" width="936" height="438" class="alignnone size-full wp-image-1175" srcset="/files/2015/03/Screen-Shot-2015-03-27-at-2.35.25-PM-300x140.png 300w, /files/2015/03/Screen-Shot-2015-03-27-at-2.35.25-PM.png 936w" sizes="(max-width: 936px) 100vw, 936px" /></a></p>

<p>We see here that it looks like the 3-customize-enter-css.all.TART entry ticked up a bunch. The related test 3-customize-enter-css.half.TART ticked up a bit too. The changes elsewhere look minimal. But is that a trend that holds across the data over time? We can add some of the relevant subtests to the overall graph view to get a closer look:</p>

<p><a href="/files/2015/03/Screen-Shot-2015-03-27-at-2.36.49-PM.png"><img src="/files/2015/03/Screen-Shot-2015-03-27-at-2.36.49-PM-1024x503.png" alt="Screen Shot 2015-03-27 at 2.36.49 PM" width="474" height="232" class="alignnone size-large wp-image-1176" srcset="/files/2015/03/Screen-Shot-2015-03-27-at-2.36.49-PM-300x147.png 300w, /files/2015/03/Screen-Shot-2015-03-27-at-2.36.49-PM-1024x503.png 1024w, /files/2015/03/Screen-Shot-2015-03-27-at-2.36.49-PM.png 1155w" sizes="(max-width: 474px) 100vw, 474px" /></a></p>

<p>As is hopefully obvious, this confirms that the affected subtest continues to hold its higher value while another test just bounces around more or less in the range it was before.</p>

<p>Hope people find this useful! If you want to play with this yourself, you can access the perfherder UI at <a href="http://treeherder.mozilla.org/perf.html">http://treeherder.mozilla.org/perf.html</a>.</p> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2014/03/it-s-all-about-the-entropy/">It&rsquo;s all about the entropy</a></h2>
    <p class="index-date">Mar 14th, 2014</p>
    <p><span class="tags"><a href="/tags/Data-Visualization.html">Data Visualization</a>  <a href="/tags/Eideticker.html">Eideticker</a>  <a href="/tags/FirefoxOS.html">FirefoxOS</a>  <a href="/tags/Mozilla.html">Mozilla</a></span></p>
  </header>

<p><em>[ For more information on the Eideticker software I&rsquo;m referring to, see <a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/">this entry</a> ]</em></p>

<p>So recently I&rsquo;ve been exploring new and different methods of measuring things that we care about on FirefoxOS &#8212; like startup time or amount of <a href="http://www.masonchang.com/blog/2014/3/2/wow-such-checkerboard">checkerboarding</a>. With Android, where we have a mostly clean signal, these measurements were pretty straightforward. Want to measure startup times? Just capture a video of Firefox starting, then compare the frames pixel by pixel to see how much they differ. When the pixels aren&rsquo;t that different anymore, we&rsquo;re &ldquo;done&rdquo;. Likewise, to measure checkerboarding we just calculated the areas of the screen where things were not completely drawn yet, frame-by-frame.</p>

<p>On FirefoxOS, where we&rsquo;re using a camera to measure these things, it has not been so simple. I&rsquo;ve already discussed this with respect to startup time in a <a href="http://wrla.ch/blog/2013/10/automatically-measuring-startup-load-time-with-eideticker/">previous post</a>. One of the ideas I talk about there is &ldquo;entropy&rdquo; (or the amount of unique information in the frame). It turns out that this is a pretty deep concept, and is useful for even more things than I thought of at the time. Since this is probably a concept that people are going to be thinking/talking about for a while, it&rsquo;s worth going into a little more detail about the math behind it.</p>

<p>The <a href="http://en.wikipedia.org/wiki/Shannon_entropy">wikipedia article</a> on information theoretic entropy is a pretty good introduction. You should read it. It all boils down to this formula:</p>

<p><img src="/files/2014/03/wikipedia-entropy-formula.png" alt="wikipedia-entropy-formula" width="401" height="37" class="alignnone size-full wp-image-1014" srcset="/files/2014/03/wikipedia-entropy-formula-300x27.png 300w, /files/2014/03/wikipedia-entropy-formula.png 401w" sizes="(max-width: 401px) 100vw, 401px" /></p>

<p>You can see this section of the wikipedia article (and the various articles that it links to) if you want to break down where that comes from, but the short answer is that given a set of random samples, the more different values there are, the higher the entropy will be. Look at it from a probabilistic point of view: if you take a random set of data and want to make predictions on what future data will look like. If it is highly random, it will be harder to predict what comes next. Conversely, if it is more uniform it is easier to predict what form it will take.</p>

<p>Another, possibly more accessible way of thinking about the entropy of a given set of data would be &ldquo;how well would it compress?&rdquo;. For example, a bitmap image with nothing but black in it could compress very well as there&rsquo;s essentially only 1 piece of unique information in it repeated many times &#8212; the black pixel. On the other hand, a bitmap image of completely randomly generated pixels would probably compress very badly, as almost every pixel represents several dimensions of unique information. For all the statistics terminology, etc. that&rsquo;s all the above formula is trying to say.</p>

<p>So we have a model of entropy, now what? For Eideticker, the question is &#8212; how can we break the frame data we&rsquo;re gathering down into a form that&rsquo;s amenable to this kind of analysis? The approach I took (on the recommendation of <a href="http://brainacle.com/calculating-image-entropy-with-python-how-and-why.html">this article</a>) was to create a histogram with 256 bins (representing the number of distinct possibilities in a black &#38; white capture) out of all the pixels in the frame, then run the formula over that. The exact function I wound up using looks like this:</p>

<pre><code>def _get_frame_entropy((i, capture, sobelized)):
    frame = capture.get_frame(i, True).astype('float')
    if sobelized:
        frame = ndimage.median_filter(frame, 3)

        dx = ndimage.sobel(frame, 0)  # horizontal derivative
        dy = ndimage.sobel(frame, 1)  # vertical derivative
        frame = numpy.hypot(dx, dy)  # magnitude
        frame *= 255.0 / numpy.max(frame)  # normalize (Q&amp;D)

    histogram = numpy.histogram(frame, bins=256)[0]
    histogram_length = sum(histogram)
    samples_probability = [float(h) / histogram_length for h in histogram]
    entropy = -sum([p * math.log(p, 2) for p in samples_probability if p != 0])

    return entropy</code></pre>

<p><a href="https://github.com/mozilla/eideticker/blob/master/src/videocapture/videocapture/entropy.py#L10">[Context]</a></p>

<p>The &ldquo;sobelized&rdquo; bit allows us to optionally convolve the frame with a sobel filter before running the entropy calculation, which removes most of the data in the capture except for the edges. This is especially useful for FirefoxOS, where the signal has quite a bit of random noise from ambient lighting that artificially inflate the entropy values even in places where there is little actual &ldquo;information&rdquo;.</p>

<p>This type of transformation often reveals very interesting information about what&rsquo;s going on in an eideticker test. For example, take this video of the user panning down in the contacts app:</p>

<div style="width: 640px; " class="wp-video">
 <video class="wp-video-shortcode" id="video-1012-2" width="640" height="917" preload="metadata" controls="controls">
  <source type="video/webm" src="/files/2014/03/contacts-scrolling-movie.webm?_=2" /><a href="/files/2014/03/contacts-scrolling-movie.webm">/files/2014/03/contacts-scrolling-movie.webm</a></video></div>

<p>If you graph the entropies of the frame of the capture using the formula above you, you get a graph like this:</p>

<p><a href="/files/2014/03/contacts-scrolling-entropy-graph.png"><img src="/files/2014/03/contacts-scrolling-entropy-graph.png" alt="contacts scrolling entropy graph" width="933" height="482" class="alignnone size-full wp-image-1022" srcset="/files/2014/03/contacts-scrolling-entropy-graph-300x154.png 300w, /files/2014/03/contacts-scrolling-entropy-graph.png 933w" sizes="(max-width: 933px) 100vw, 933px" /></a>
 <br /><a href="http://eideticker.wrla.ch/b2g/detail.html?id=3f7b7c88a9ed11e380c5f0def1767b24#/framesobelentropies">[Link to original]</a></p>

<p>The Y axis represents entropy, as calculated by the code above. There is no inherently &ldquo;right&rdquo; value for this &#8212; it all depends on the application you&rsquo;re testing and what you expect to see displayed on the screen. In general though, higher values are better as it indicates more frames of the capture are &ldquo;complete&rdquo;.</p>

<p>The region at the beginning where it is at about 5.0 represents the contacts app with a set of contacts fully displayed (at startup). The &ldquo;flat&rdquo; regions where the entropy is at roughly 4.25? Those are the areas where the app is &ldquo;checkerboarding&rdquo; (blanking out waiting for graphics or layout engine to draw contact information). Click through to the original and swipe over the graph to see what I mean.</p>

<p>It&rsquo;s easy to see what a hypothetical ideal end state would be for this capture: a graph with a smooth entropy of about 5.0 (similar to the start state, where all contacts are fully drawn in). We can track our progress towards this goal (or our deviation from it), by watching the eideticker b2g dashboard and seeing if the summation of the entropy values for frames over the entire test increases or decreases over time. If we see it generally increase, that probably means we&rsquo;re seeing less checkerboarding in the capture. If we see it decrease, that might mean we&rsquo;re now seeing checkerboarding where we weren&rsquo;t before.</p>

<p>It&rsquo;s too early to say for sure, but over the past few days the trend has been positive:</p>

<p><a href="/files/2014/03/entropy-levels-climbing.png"><img src="/files/2014/03/entropy-levels-climbing.png" alt="entropy-levels-climbing" width="822" height="529" class="alignnone size-full wp-image-1025" srcset="/files/2014/03/entropy-levels-climbing-300x193.png 300w, /files/2014/03/entropy-levels-climbing.png 822w" sizes="(max-width: 822px) 100vw, 822px" /></a>
 <br /><a href="http://eideticker.wrla.ch/b2g/#/inari/b2g-contacts-scrolling/overallentropy">[Link to original]</a></p>

<p>(note that there were some problems in the way the tests were being run before, so results before the 12th should not be considered valid)</p>

<p>So one concept, at least two relevant metrics we can measure with it (startup time and checkerboarding). Are there any more? Almost certainly, let&rsquo;s find them!</p> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2013/10/automatically-measuring-startup-load-time-with-eideticker/">Automatically measuring startup / load time with Eideticker</a></h2>
    <p class="index-date">Oct 17th, 2013</p>
    <p><span class="tags"><a href="/tags/Data-Visualization.html">Data Visualization</a>  <a href="/tags/Eideticker.html">Eideticker</a>  <a href="/tags/FirefoxOS.html">FirefoxOS</a>  <a href="/tags/Mozilla.html">Mozilla</a></span></p>
  </header>

<p>So we&rsquo;ve been using Eideticker to automatically measure startup/pageload times for about a year now on Android, and more recently on FirefoxOS as well (albeit not automatically). This gives us nice and pretty graphs like this:</p>

<p><a href="/files/2013/10/flot-startup-times-gn.png"><img src="/files/2013/10/flot-startup-times-gn.png" alt="flot-startup-times-gn" width="620" height="568" class="alignnone size-full wp-image-986" srcset="/files/2013/10/flot-startup-times-gn-300x274.png 300w, /files/2013/10/flot-startup-times-gn.png 620w" sizes="(max-width: 620px) 100vw, 620px" /></a></p>

<p>Ok, so we&rsquo;re generating numbers and graphing them. That&rsquo;s great. But what&rsquo;s really going on behind the scenes? I&rsquo;m glad you asked. The story is a bit different depending on which platform you&rsquo;re talking about.</p>

<p><strong>Android</strong></p>

<p>On Android we connect Eideticker to the device&rsquo;s HDMI out, so we count on a nearly pixel-perfect signal. In practice, it isn&rsquo;t quite, but it is within a few RGB values that we can easily filter for. This lets us come up with a pretty good mechanism for determining when a page load or app startup is finished: just compare frames, and say we&rsquo;ve &ldquo;stopped&rdquo; when the pixel differences between frames are negligible (previously defined at 2048 pixels, now 4096 &#8212; see below). Eideticker&rsquo;s new frame difference view lets us see how this works. Look at this graph of application startup:</p>

<p><a href="/files/2013/10/frame-difference-android-startup.png"><img src="/files/2013/10/frame-difference-android-startup.png" alt="frame-difference-android-startup" width="803" height="514" class="alignnone size-full wp-image-973" srcset="/files/2013/10/frame-difference-android-startup-300x192.png 300w, /files/2013/10/frame-difference-android-startup.png 803w" sizes="(max-width: 803px) 100vw, 803px" /></a>
 <br /><a href="http://eideticker.wrla.ch/#/samsung-gn/startup-abouthome-dirty/timetostableframe">[Link to original]</a></p>

<p>What&rsquo;s going on here? Well, we see some huge jumps in the beginning. This represents the animated transitions that Android makes as we transition from the SUTAgent application (don&rsquo;t ask) to the beginnings of the FirefoxOS browser chrome. You&rsquo;ll notice though that there&rsquo;s some more changes that come in around the 3 second mark. This is when the site bookmarks are fully loaded. If you load the original page (link above) and swipe your mouse over the graph, you can see what&rsquo;s going on for yourself.</p>

<p>This approach is not completely without problems. It turns out that there is sometimes some minor churn in the display even when the app is for all intents and purposes started. For example, <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=922770">sometimes the scrollbar fading out of view can result in a significantish pixel value change</a>, so I recently upped the threshold of pixels that are different from 2048 to 4096. We also recently encountered a <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=926997">silly problem</a> with a random automation app displaying &ldquo;toasts&rdquo; which caused results to artificially spike. More tweaking may still be required. However, on the whole I&rsquo;m pretty happy with this solution. It gives useful, undeniably objective results whose meaning is easy to understand.</p>

<p><strong>FirefoxOS</strong></p>

<p>So as mentioned previously, we use a camera on FirefoxOS to record output instead of HDMI output. Pretty unsurprisingly, this is much noisier. See this movie of the contacts app starting and note all the random lighting changes, for example:</p>

<div style="width: 409px; " class="wp-video"><!--[if lt IE 9]><![endif]-->
 <video class="wp-video-shortcode" id="video-972-1" width="409" height="580" preload="metadata" controls="controls">
  <source type="video/webm" src="/files/2013/10/contacts-b2g-aug30-load-taphomescreen1.webm?_=1" /> <a href="/files/2013/10/contacts-b2g-aug30-load-taphomescreen1.webm">/files/2013/10/contacts-b2g-aug30-load-taphomescreen1.webm</a></video></div>

<p>My experience has been that pixel differences can be so great between visually identical frames on an eideticker capture on these devices that it&rsquo;s pretty much impossible to settle on when startup is done using the frame difference method. It&rsquo;s of course possible to detect very large scale changes, but the small scale ones (like the contacts actually appearing in the example above) are very hard to distinguish from random differences in the amount of light absorbed by the camera sensor. Tricks like using median filtering (a.k.a. &ldquo;blurring&rdquo;) help a bit, but not much. Take a look at this graph, for example:</p>

<p><a href="/files/2013/10/plotly-contacts-load-pixeldiff.png"><img src="/files/2013/10/plotly-contacts-load-pixeldiff.png" alt="plotly-contacts-load-pixeldiff" width="531" height="679" class="alignnone size-full wp-image-980" srcset="/files/2013/10/plotly-contacts-load-pixeldiff-234x300.png 234w, /files/2013/10/plotly-contacts-load-pixeldiff.png 531w" sizes="(max-width: 531px) 100vw, 531px" /></a>
 <br /><a href="https://plot.ly/~WilliamLachance/3">[Link to original]</a></p>

<p>You&rsquo;ll note that the pixel differences during &ldquo;static&rdquo; parts of the capture are highly variable. This is because the pixel difference depends heavily on how &ldquo;bright&rdquo; each frame is: parts of the capture which are black (e.g. a contacts icon with a black background) have a much lower difference between them than parts that are bright (e.g. the contacts screen fully loaded).</p>

<p>After a day or so of experimenting and research, I settled on an approach which seems to work pretty reliably. Instead of comparing the frames directly, I measure the <a href="http://en.wikipedia.org/wiki/Entropy">entropy</a> of the <a href="http://en.wikipedia.org/wiki/Image_histogram">histogram</a> of colours used in each frame (essentially just an indication of brightness in this case, see <a href="http://brainacle.com/calculating-image-entropy-with-python-how-and-why.html">this article</a> for more on calculating it), then compare that of each frame with the average of the same measure over 5 previous frames (to account for the fact that two frames may be arbitrarily different, but that is unlikely that a sequence of frames will be). This seems to work much better than frame difference in this environment: although there are plenty of minute differences in light absorption in a capture from this camera, the overall color composition stays mostly the same. See this graph:</p>

<p><a href="/files/2013/10/plotly-contacts-load-entropy.png"><img src="/files/2013/10/plotly-contacts-load-entropy.png" alt="plotly-contacts-load-entropy" width="546" height="674" class="alignnone size-full wp-image-979" srcset="/files/2013/10/plotly-contacts-load-entropy-243x300.png 243w, /files/2013/10/plotly-contacts-load-entropy.png 546w" sizes="(max-width: 546px) 100vw, 546px" /></a>
 <br /><a href="https://plot.ly/~WilliamLachance/5">[Link to original]</a></p>

<p>If you look closely, you can see some minor variance in the entropy differences depending on the state of the screen, but it&rsquo;s not nearly as pronounced as before. In practice, I&rsquo;ve been able to get extremely consistent numbers with a reasonable &ldquo;threshold&rdquo; of &ldquo;0.05&rdquo;.</p>

<p>In Eideticker I&rsquo;ve tried to steer away from using really complicated math or algorithms to measure things, unless all the alternatives fail. In that sense, I really liked the simplicity of &ldquo;pixel differences&rdquo; and am not thrilled about having to resort to this: hopefully the concepts in this case (histograms and entropy) are simple enough that most people will be able to understand my methodology, if they care to. Likely I will need to come up with something else for measuring responsiveness and animation smoothness (frames per second), as likely we can&rsquo;t count on light composition changing the same way for those cases. My initial thought was to use <a href="http://en.wikipedia.org/wiki/Edge_detection">edge detection</a> (which, while somewhat complex to calculate, is at least easy to understand conceptually) but am open to other ideas.</p> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2013/08/nixi-update/">NIXI Update</a></h2>
    <p class="index-date">Aug 25th, 2013</p>
    <p><span class="tags"><a href="/tags/BIXI.html">BIXI</a>  <a href="/tags/Data-Visualization.html">Data Visualization</a>  <a href="/tags/Nixi.html">Nixi</a></span></p>
  </header>

<p>I&rsquo;ve been working on a new, mobile friendly version of <a href="http://nixi.ca/">Nixi</a> on-and-off for the past year and a bit. I&rsquo;m not sure when it&rsquo;s ever going to be finished, so I thought I might as well post the work-in-progress, which has these noteworthy improvements:</p>

<ul>
 <li>Even faster than before (using the <a href="http://getbootstrap.com">Bootstrap</a> library behind the scenes, no longer using slow canvas library to update map)</li>
 <li>Sexier graphics (thanks to the aforementioned Bootstrap library)</li>
 <li>Now uses client side URLs to keep track of state as you navigate through the site. This allows you to bookmark a favorite spot (e.g. your home) and then go back to it later. For example, <a href="http://nixi.ca/#/cities/montreal/places/5605%20avenue%20de%20Gaspe">this link</a> will give you a list of BIXI docks near <a href="http://www.station-c.com/">Station C</a>, the coworking space I belong to.</li></ul>

<p>If you use <a href="http://bixi.com">BIXI</a> at all, check it out and let me know what you think!</p>

<p><a href="/files/2013/08/nixi-screenshot.png"> <img src="/files/2013/08/nixi-screenshot-1024x672.png" alt="nixi screenshot" width="640" height="420" class="alignnone size-large wp-image-927" srcset="/files/2013/08/nixi-screenshot-300x196.png 300w, /files/2013/08/nixi-screenshot-1024x672.png 1024w, /files/2013/08/nixi-screenshot.png 1266w" sizes="(max-width: 640px) 100vw, 640px" /></a></p> 
  <hr/>
</article>
<footer>
 <ul class="pagination">
  <li class="page-item disabled"><a class="page-link" href="#">
    <quote>&larr;</quote></a></li>
  <li class="page-item active"><a class="page-link" href="/tags/Data-Visualization.html">1</a></li>
  <li class="page-item"><a class="page-link" href="/tags/Data-Visualization-2.html">2</a></li>
  <li class="page-item"><a class="page-link" href="/tags/Data-Visualization-2.html">
    <quote>&rarr;</quote></a></li></ul></footer>
    </div>
    <footer class="container max-w-screen-md px-8 py-4 mx-auto less-important">
      <p>Comments / thoughts? Feel free to send an email to wlach on protonmail.com or
        (for Mozilla-related stuff) reach me at <code>wlach</code> on <a href="https://wiki.mozilla.org/Matrix">Mozilla's instance of Matrix</a>.</p>
      <p>
        Site generated by
        <a href="https://github.com/greghendershott/frog">Frog</a>.
        Post content is licensed under a
        <a rel="license" href="http://creativecommons.org/licenses/by/4.0/"
          >Creative Commons Attribution 4.0 Unported License</a
        >.
      </p>
    </footer>
  </body>
</html>