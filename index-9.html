<!DOCTYPE html>
<html lang="en">
  <head>

    <meta charset="utf-8" />
    <title>William Lachance's Log (page 9)</title>
    <meta name="description" content="William Lachance's Log (page 9)" />
    <meta name="author" content="William Lachance" />
    <meta name="keywords" content="Taskcluster, FirefoxOS, Web, Philosophy, Data, Metrics Graphics, Buddhism, all, Profiling, BIXI, iodide, Community, Iodide, Life, telemetry, Video, Meditation, Irydium, iphone, GoFaster, Bikes, Social Media, Business, Android, Treeherder, meta, GNOME, Environment, ÃŽle Sans Fil, Time, Psychology, Statistics, zen, Music, Counting, Mozilla, Eideticker, Usability, Transit to Go, Infraherder, Recurse, Ebola, Orangutan, Open Data, Nixi, Meta, ateam, Perfherder, Montreal, Docker, Talos, Telemetry, Responsiveness, Glean, Pandaboard, Food, Toronto, Mission Control, Coffee, hbus, Python, Cats, email, Data Visualization, WifiDog, SQL, Release Engineering, Polling, Performance, Free Software, Transit, mozregression, MSF" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="icon" href="/favicon.ico" />
    <link rel="canonical" href="https://wrla.ch/index-9.html" />

    <!-- CSS -->
    <link rel="stylesheet" type="text/css" href="/css/style.css" />
    <link
      rel="stylesheet"
      type="text/css"
      href="/css/pygments.css"
    />
    <link
      rel="stylesheet"
      type="text/css"
      href="/css/scribble.css"
    />
    <!-- Feeds -->
    <link
      rel="alternate"
      type="application/atom+xml"
      href="/feeds/all.atom.xml"
      title="Atom Feed"
    />
    <link
      rel="alternate"
      type="application/rss+xml"
      href="/feeds/all.rss.xml"
      title="RSS Feed"
    />
    <!-- JS -->
    <script type="text/javascript">
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-xxxxx', 'auto');
      ga('send', 'pageview');
    </script>
  </head>
  <body>
    <nav
      class="flex items-center justify-between flex-wrap bg-gray-800 py-1 px-8"
    >
      <div class="flex items-center flex-shrink-0 text-gray-400 mr-6">
        <div class="p-1">
          <a href="/index.html"
            ><img
              src="/img/wlach_icon.png"
              width="32"
              height="32"
              class="p rounded"
          /></a>
        </div>
        <div class="p-1">
          <a
            href="/index.html"
            class="text-gray-200 font-semibold text-xl tracking-tight hover:text-white"
            >wlach log</a
          >
        </div>
      </div>
      <div class="flex-grow lg:flex lg:items-center">
        <div class="text-sm lg:flex-grow">
          <a
            href="/About.html"
            class="mt-4 lg:inline-block lg:mt-0 hover:text-white mr-4 text-gray-600"
          >
            About</a>
          <a
            class="mt-4 lg:inline-block lg:mt-0 text-gray-600 hover:text-white mr-4"
            href="/feeds/all.atom.xml"
            >Atom</a
          >
          <a
            class="mt-4 lg:inline-block lg:mt-0 text-gray-600 hover:text-white mr-4"
            href="/feeds/all.rss.xml"
            >RSS</a
          >
        </div>
      </div>
    </nav>
    <div id="content" class="container max-w-screen-md px-8 py-4 mx-auto">
         <article>
  <header>
    <h2><a href="/blog/2014/03/it-8217-s-all-about-the-entropy/">It&#8217;s all about the entropy</a></h2>
    <p class="index-date">Mar 14th, 2014</p>
    <p><span class="tags"><a href="/tags/Data-Visualization.html">Data Visualization</a>  <a href="/tags/Eideticker.html">Eideticker</a>  <a href="/tags/FirefoxOS.html">FirefoxOS</a>  <a href="/tags/Mozilla.html">Mozilla</a></span></p>
  </header>

<p><em>[ For more information on the Eideticker software I&#8217;m referring to, see <a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/">this entry</a> ]</em></p>

<p>So recently I&#8217;ve been exploring new and different methods of measuring things that we care about on FirefoxOS &#8212; like startup time or amount of <a href="http://www.masonchang.com/blog/2014/3/2/wow-such-checkerboard">checkerboarding</a>. With Android, where we have a mostly clean signal, these measurements were pretty straightforward. Want to measure startup times? Just capture a video of Firefox starting, then compare the frames pixel by pixel to see how much they differ. When the pixels aren&#8217;t that different anymore, we&#8217;re &#8220;done&#8221;. Likewise, to measure checkerboarding we just calculated the areas of the screen where things were not completely drawn yet, frame-by-frame.</p>

<p>On FirefoxOS, where we&#8217;re using a camera to measure these things, it has not been so simple. I&#8217;ve already discussed this with respect to startup time in a <a href="http://wrla.ch/blog/2013/10/automatically-measuring-startup-load-time-with-eideticker/">previous post</a>. One of the ideas I talk about there is &#8220;entropy&#8221; (or the amount of unique information in the frame). It turns out that this is a pretty deep concept, and is useful for even more things than I thought of at the time. Since this is probably a concept that people are going to be thinking/talking about for a while, it&#8217;s worth going into a little more detail about the math behind it.</p>

<p>The <a href="http://en.wikipedia.org/wiki/Shannon_entropy">wikipedia article</a> on information theoretic entropy is a pretty good introduction. You should read it. It all boils down to this formula:</p>

<p><img src="/files/2014/03/wikipedia-entropy-formula.png" alt="wikipedia-entropy-formula" width="401" height="37" class="alignnone size-full wp-image-1014" srcset="/files/2014/03/wikipedia-entropy-formula-300x27.png 300w, /files/2014/03/wikipedia-entropy-formula.png 401w" sizes="(max-width: 401px) 100vw, 401px" /></p>

<p>You can see this section of the wikipedia article (and the various articles that it links to) if you want to break down where that comes from, but the short answer is that given a set of random samples, the more different values there are, the higher the entropy will be. Look at it from a probabilistic point of view: if you take a random set of data and want to make predictions on what future data will look like. If it is highly random, it will be harder to predict what comes next. Conversely, if it is more uniform it is easier to predict what form it will take.</p>

<p>Another, possibly more accessible way of thinking about the entropy of a given set of data would be &#8220;how well would it compress?&#8221;. For example, a bitmap image with nothing but black in it could compress very well as there&#8217;s essentially only 1 piece of unique information in it repeated many times &#8212; the black pixel. On the other hand, a bitmap image of completely randomly generated pixels would probably compress very badly, as almost every pixel represents several dimensions of unique information. For all the statistics terminology, etc. that&#8217;s all the above formula is trying to say.</p>

<p>So we have a model of entropy, now what? For Eideticker, the question is &#8212; how can we break the frame data we&#8217;re gathering down into a form that&#8217;s amenable to this kind of analysis? The approach I took (on the recommendation of <a href="http://brainacle.com/calculating-image-entropy-with-python-how-and-why.html">this article</a>) was to create a histogram with 256 bins (representing the number of distinct possibilities in a black &#38; white capture) out of all the pixels in the frame, then run the formula over that. The exact function I wound up using looks like this:</p>

<pre><code>def _get_frame_entropy((i, capture, sobelized)):
    frame = capture.get_frame(i, True).astype('float')
    if sobelized:
        frame = ndimage.median_filter(frame, 3)

        dx = ndimage.sobel(frame, 0)  # horizontal derivative
        dy = ndimage.sobel(frame, 1)  # vertical derivative
        frame = numpy.hypot(dx, dy)  # magnitude
        frame *= 255.0 / numpy.max(frame)  # normalize (Q&amp;D)

    histogram = numpy.histogram(frame, bins=256)[0]
    histogram_length = sum(histogram)
    samples_probability = [float(h) / histogram_length for h in histogram]
    entropy = -sum([p * math.log(p, 2) for p in samples_probability if p != 0])

    return entropy

</code></pre>

<p><a href="https://github.com/mozilla/eideticker/blob/master/src/videocapture/videocapture/entropy.py#L10">[Context]</a></p>

<p>The &#8220;sobelized&#8221; bit allows us to optionally convolve the frame with a sobel filter before running the entropy calculation, which removes most of the data in the capture except for the edges. This is especially useful for FirefoxOS, where the signal has quite a bit of random noise from ambient lighting that artificially inflate the entropy values even in places where there is little actual &#8220;information&#8221;.</p>

<p>This type of transformation often reveals very interesting information about what&#8217;s going on in an eideticker test. For example, take this video of the user panning down in the contacts app:</p>

<div style="width: 640px; " class="wp-video">
 <video class="wp-video-shortcode" id="video-1012-2" width="640" height="917" preload="metadata" controls="controls">
  <source type="video/webm" src="/files/2014/03/contacts-scrolling-movie.webm?_=2" /><a href="/files/2014/03/contacts-scrolling-movie.webm">/files/2014/03/contacts-scrolling-movie.webm</a></video></div>

<p>If you graph the entropies of the frame of the capture using the formula above you, you get a graph like this:</p>

<p><a href="/files/2014/03/contacts-scrolling-entropy-graph.png"><img src="/files/2014/03/contacts-scrolling-entropy-graph.png" alt="contacts scrolling entropy graph" width="933" height="482" class="alignnone size-full wp-image-1022" srcset="/files/2014/03/contacts-scrolling-entropy-graph-300x154.png 300w, /files/2014/03/contacts-scrolling-entropy-graph.png 933w" sizes="(max-width: 933px) 100vw, 933px" /></a>
 <br /><a href="http://eideticker.wrla.ch/b2g/detail.html?id=3f7b7c88a9ed11e380c5f0def1767b24#/framesobelentropies">[Link to original]</a></p>

<p>The Y axis represents entropy, as calculated by the code above. There is no inherently &#8220;right&#8221; value for this &#8212; it all depends on the application you&#8217;re testing and what you expect to see displayed on the screen. In general though, higher values are better as it indicates more frames of the capture are &#8220;complete&#8221;.</p>

<p>The region at the beginning where it is at about 5.0 represents the contacts app with a set of contacts fully displayed (at startup). The &#8220;flat&#8221; regions where the entropy is at roughly 4.25? Those are the areas where the app is &#8220;checkerboarding&#8221; (blanking out waiting for graphics or layout engine to draw contact information). Click through to the original and swipe over the graph to see what I mean.</p>

<p>It&#8217;s easy to see what a hypothetical ideal end state would be for this capture: a graph with a smooth entropy of about 5.0 (similar to the start state, where all contacts are fully drawn in). We can track our progress towards this goal (or our deviation from it), by watching the eideticker b2g dashboard and seeing if the summation of the entropy values for frames over the entire test increases or decreases over time. If we see it generally increase, that probably means we&#8217;re seeing less checkerboarding in the capture. If we see it decrease, that might mean we&#8217;re now seeing checkerboarding where we weren&#8217;t before.</p>

<p>It&#8217;s too early to say for sure, but over the past few days the trend has been positive:</p>

<p><a href="/files/2014/03/entropy-levels-climbing.png"><img src="/files/2014/03/entropy-levels-climbing.png" alt="entropy-levels-climbing" width="822" height="529" class="alignnone size-full wp-image-1025" srcset="/files/2014/03/entropy-levels-climbing-300x193.png 300w, /files/2014/03/entropy-levels-climbing.png 822w" sizes="(max-width: 822px) 100vw, 822px" /></a>
 <br /><a href="http://eideticker.wrla.ch/b2g/#/inari/b2g-contacts-scrolling/overallentropy">[Link to original]</a></p>

<p>(note that there were some problems in the way the tests were being run before, so results before the 12th should not be considered valid)</p>

<p>So one concept, at least two relevant metrics we can measure with it (startup time and checkerboarding). Are there any more? Almost certainly, let&#8217;s find them!</p> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2014/03/eideticker-for-firefoxos-becoming-more-useful/">Eideticker for FirefoxOS: Becoming more useful</a></h2>
    <p class="index-date">Mar 9th, 2014</p>
    <p><span class="tags"><a href="/tags/Eideticker.html">Eideticker</a>  <a href="/tags/FirefoxOS.html">FirefoxOS</a>  <a href="/tags/Mozilla.html">Mozilla</a></span></p>
  </header>

<p><em>[ For more information on the Eideticker software I&#8217;m referring to, see <a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/">this entry</a> ]</em></p>

<p>Time for a long overdue eideticker-for-firefoxos update. <a href="http://wrla.ch/blog/2013/10/automatically-measuring-startup-load-time-with-eideticker/">Last time we were here</a> (almost 5 months ago! man time flies), I was discussing methodologies for measuring startup performance. Since then, <a href="http://blargon7.com/">Dave Hunt</a> and myself have been doing lots of work to make Eideticker more robust and useful. Notably, we now have a setup in London running a suite of Eideticker tests on the latest version of FirefoxOS on the Inari on a daily basis, reporting to <a href="http://eideticker.mozilla.org/b2g">http://eideticker.mozilla.org/b2g</a>.</p>

<p><a href="/files/2014/03/b2g-contacts-startup-dashboard.png"><img src="/files/2014/03/b2g-contacts-startup-dashboard.png" alt="b2g-contacts-startup-dashboard" width="840" height="601" class="alignnone size-full wp-image-1005" srcset="/files/2014/03/b2g-contacts-startup-dashboard-300x214.png 300w, /files/2014/03/b2g-contacts-startup-dashboard.png 840w" sizes="(max-width: 840px) 100vw, 840px" /></a></p>

<p>There were more than a few false starts with and some of the earlier data is not to be entirely trusted&#8230; but it now seems to be chugging along nicely, hopefully providing startup numbers that provide a useful counterpoint to the <a href="https://datazilla.mozilla.org/b2g">datazilla startup numbers</a> we&#8217;ve already been collecting for some time. There still seem to be some minor problems, but in general I am becoming more and more confident in it as time goes on.</p>

<p>One feature that I am particularly proud of is the detail view, which enables you to see frame-by-frame what&#8217;s going on. Click on any datapoint on the graph, then open up the view that gives an account of what eideticker is measuring. Hover over the graph and you can see what the video looks like at any point in the capture. This not only lets you know that something regressed, but how. For example, in the messages app, you can scan through this view to see exactly when the first message shows up, and what exact state the application is in when Eideticker says it&#8217;s &#8220;done loading&#8221;.</p>

<p><a href="/files/2014/03/capture-detail-view.png"><img src="/files/2014/03/capture-detail-view.png" alt="Capture Detail View" width="964" height="843" class="alignnone size-full wp-image-1008" srcset="/files/2014/03/capture-detail-view-300x262.png 300w, /files/2014/03/capture-detail-view.png 964w" sizes="(max-width: 964px) 100vw, 964px" /></a>
 <br /><a href="http://eideticker.wrla.ch/b2g/framediff.html?id=3819a484a6d611e3ab89f0def1767b24">[link to original]</a></p>

<p>(apologies for the low quality of the video &#8212; should be fixed with <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=980479">this bug</a> next week)</p>

<p>As it turns out, this view has also proven to be particularly useful when working with the new entropy measurements in Eideticker which I&#8217;ve been using to measure checkerboarding (redraw delay) on FirefoxOS. More on that next week.</p> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2014/03/the-highest-form/">The highest form</a></h2>
    <p class="index-date">Mar 2nd, 2014</p>
    <p><span class="tags"><a href="/tags/Buddhism.html">Buddhism</a>  <a href="/tags/Cats.html">Cats</a>  <a href="/tags/Meditation.html">Meditation</a>  <a href="/tags/zen.html">zen</a></span></p>
  </header>

<p><a href="/files/2014/03/friedrich_shikatanza.jpg"><img src="/files/2014/03/friedrich_shikatanza.jpg" alt="friedrich_shikatanza" width="1024" height="768" class="alignnone size-full wp-image-1000" srcset="/files/2014/03/friedrich_shikatanza-300x225.jpg 300w, /files/2014/03/friedrich_shikatanza.jpg 1024w" sizes="(max-width: 1024px) 100vw, 1024px" /></a></p>

<p>He sits on the edge of my couch, stoic. At rest, yet alert for anything to happen at any moment. Whether that be a mouse running across the room (fat chance here) or me getting up to go into the kitchen. I took this picture just after zazen (meditating) &#8212; I struggle and struggle and yet right beside me sits a creature that constantly practices in the highest form (shikantaza, &#8220;just sitting&#8221;) without even trying. An example for me to follow?</p> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2013/11/mozregression-now-supports-inbound-builds/">mozregression now supports inbound builds</a></h2>
    <p class="index-date">Nov 28th, 2013</p>
    <p><span class="tags"><a href="/tags/Mozilla.html">Mozilla</a>  <a href="/tags/mozregression.html">mozregression</a></span></p>
  </header>

<p>Just wanted to send out a quick note that I recently added inbound support to <a href="http://mozilla.github.io/mozregression/">mozregression</a> for desktop builds of Firefox on Windows, Mac, and Linux.</p>

<p>For the uninitiated, mozregression is an automated tool that lets you bisect through builds of Firefox to find out when a problem was introduced. You give it the last known good date, the last known bad date and off it will go, automatically pulling down builds to test. After each iteration, it will ask you whether this build was good or bad, update the regression range accordingly, and then the cycle repeats until there are no more intermediate builds.</p>

<p>Previously, it would only use nightlies which meant a one day granularity &#8212; this meant pretty wide regression ranges, made wider in the last year by the fact that so much more is now going into the tree over the course of the day. However, with inbound support (using the new inbound archive) we now have the potential to get a much tighter range, which should be super helpful for developers. Best of all, mozregression doesn&#8217;t require any particularly advanced skills to use which means everyone in the Mozilla community can help out.</p>

<p>For anyone interested, there&#8217;s quite a bit of scope to improve mozregression to make it do more things (FirefoxOS support, easier installation&#8230;). Feel free to check out <a href="http://github.com/mozilla/mozregression">the repository</a>, the <a href="https://github.com/mozilla/mozregression/issues?state=open">issues list</a> (I just added <a href="https://github.com/mozilla/mozregression/issues/76">an easy one</a> which would make a great first bug) and ask questions on irc.mozilla.org#ateam!</p> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2013/11/meditative-practice-followup/">Meditative practice followup</a></h2>
    <p class="index-date">Nov 13th, 2013</p>
    <p><span class="tags"><a href="/tags/Buddhism.html">Buddhism</a>  <a href="/tags/Meditation.html">Meditation</a>  <a href="/tags/zen.html">zen</a></span></p>
  </header>

<p>A few months ago, I started blogging a bit about my fledgling Buddhist meditation practice, and then abruptly stopped. I thought I&#8217;d write just a few words about why I didn&#8217;t continue.</p>

<p>Over time, one of the things that I found most difficult about my practice was keeping it relatively pure. The whole point is to just sit and follow the breath with no extra motivation or hidden agenda. Given that, having it in the back of my mind to later try to explain my practice to a broad audience was at best a distraction. At worst, I worried that it might actually be hindering my progress.</p>

<p>After some thinking about it where my desire to explain this stuff came from, I determined that there was a root desire there to make the world conform to my expectations of what it *should* be. Which, if you stop and think about it, is just another form of greed. We often think of our desires as being about personal gratification (food, sex, cars, whatever) but that&#8217;s really too narrow a view &#8212; we&#8217;re social creatures, and our desires and aversions inevitably extend to the social sphere as well.</p>

<p>I suppose that sounds rather judgemental or moralistic, but it&#8217;s really not intended that way. This is just the nature of human experience, and I am certainly not exempt from that. There is probably at least some element of this greed at the root of much of my writing, whether it be discussing <a href="http://wrla.ch/blog/2013/10/automatically-measuring-startup-load-time-with-eideticker/">my latest computational vision problem at work</a> or <a href="http://wrla.ch/blog/2013/09/how-to-make-great-coffee-that-doesnt-generate-966-million-pounds-of-waste-a-year/">how I think coffee should be brewed</a> &#8212; but at least in those cases articulating myself doesn&#8217;t interfere with the activity itself.</p>

<p>A frequent misunderstanding of the practice of Buddhism is that it&#8217;s about eliminating desire. As I understand it, it&#8217;s not so much that, as it is about putting desires in proper perspective. To not be ruled by them. If I have a social purpose in the back of my head during the practice, well, that&#8217;s going to be a problem. It&#8217;ll be constantly in the background, subtly influencing what I process and how I process it (e.g. the thought &#8220;how am I going to describe <em>that</em>&#8220;). I have enough issues meditating without adding to them.</p>

<p>Moreover, one of the things I&#8217;ve realized over the last few months is that the way people process the world around the world is pretty differently. I&#8217;m lucky enough to have a mind able to sit still for (average) 20 minutes a day. Not perfectly of course &#8212; many times I feel like I&#8217;m caught up with a million random thoughts for 90% of a session, but as I understand it that&#8217;s just part of the process. At least I can sit still! I&#8217;ve since learned that this isn&#8217;t easy at all for other people (the urge to get up and do something else is overwhelming) and I really have no insight at present into what would make it easier for them (they had tried most of what I suggested to no avail). So I am a bit concerned that what I have to say would act more as a hindrance to the journey of others rather than a help.</p>

<p>All this is not to say that I&#8217;m not happy to discuss my experiences one on one with anyone who&#8217;s interested. If you&#8217;re curious, by all means feel free to contact me &#8212; though I suspect you&#8217;d probably do better reaching out to a dedicated teacher who has more experience in these matters than I. If you can&#8217;t find one, I would again recommend <a href="http://www.amazon.ca/Mindfulness-Plain-English-Anniversary-Edition/dp/0861719069">Mindfulness in Plain English</a>.</p> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2013/10/automatically-measuring-startup-load-time-with-eideticker/">Automatically measuring startup / load time with Eideticker</a></h2>
    <p class="index-date">Oct 17th, 2013</p>
    <p><span class="tags"><a href="/tags/Data-Visualization.html">Data Visualization</a>  <a href="/tags/Eideticker.html">Eideticker</a>  <a href="/tags/FirefoxOS.html">FirefoxOS</a>  <a href="/tags/Mozilla.html">Mozilla</a></span></p>
  </header>

<p>So we&#8217;ve been using Eideticker to automatically measure startup/pageload times for about a year now on Android, and more recently on FirefoxOS as well (albeit not automatically). This gives us nice and pretty graphs like this:</p>

<p><a href="/files/2013/10/flot-startup-times-gn.png"><img src="/files/2013/10/flot-startup-times-gn.png" alt="flot-startup-times-gn" width="620" height="568" class="alignnone size-full wp-image-986" srcset="/files/2013/10/flot-startup-times-gn-300x274.png 300w, /files/2013/10/flot-startup-times-gn.png 620w" sizes="(max-width: 620px) 100vw, 620px" /></a></p>

<p>Ok, so we&#8217;re generating numbers and graphing them. That&#8217;s great. But what&#8217;s really going on behind the scenes? I&#8217;m glad you asked. The story is a bit different depending on which platform you&#8217;re talking about.</p>

<p><strong>Android</strong></p>

<p>On Android we connect Eideticker to the device&#8217;s HDMI out, so we count on a nearly pixel-perfect signal. In practice, it isn&#8217;t quite, but it is within a few RGB values that we can easily filter for. This lets us come up with a pretty good mechanism for determining when a page load or app startup is finished: just compare frames, and say we&#8217;ve &#8220;stopped&#8221; when the pixel differences between frames are negligible (previously defined at 2048 pixels, now 4096 &#8212; see below). Eideticker&#8217;s new frame difference view lets us see how this works. Look at this graph of application startup:</p>

<p><a href="/files/2013/10/frame-difference-android-startup.png"><img src="/files/2013/10/frame-difference-android-startup.png" alt="frame-difference-android-startup" width="803" height="514" class="alignnone size-full wp-image-973" srcset="/files/2013/10/frame-difference-android-startup-300x192.png 300w, /files/2013/10/frame-difference-android-startup.png 803w" sizes="(max-width: 803px) 100vw, 803px" /></a>
 <br /><a href="http://eideticker.wrla.ch/#/samsung-gn/startup-abouthome-dirty/timetostableframe">[Link to original]</a></p>

<p>What&#8217;s going on here? Well, we see some huge jumps in the beginning. This represents the animated transitions that Android makes as we transition from the SUTAgent application (don&#8217;t ask) to the beginnings of the FirefoxOS browser chrome. You&#8217;ll notice though that there&#8217;s some more changes that come in around the 3 second mark. This is when the site bookmarks are fully loaded. If you load the original page (link above) and swipe your mouse over the graph, you can see what&#8217;s going on for yourself.</p>

<p>This approach is not completely without problems. It turns out that there is sometimes some minor churn in the display even when the app is for all intents and purposes started. For example, <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=922770">sometimes the scrollbar fading out of view can result in a significantish pixel value change</a>, so I recently upped the threshold of pixels that are different from 2048 to 4096. We also recently encountered a <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=926997">silly problem</a> with a random automation app displaying &#8220;toasts&#8221; which caused results to artificially spike. More tweaking may still be required. However, on the whole I&#8217;m pretty happy with this solution. It gives useful, undeniably objective results whose meaning is easy to understand.</p>

<p><strong>FirefoxOS</strong></p>

<p>So as mentioned previously, we use a camera on FirefoxOS to record output instead of HDMI output. Pretty unsurprisingly, this is much noisier. See this movie of the contacts app starting and note all the random lighting changes, for example:</p>

<div style="width: 409px; " class="wp-video"><!--[if lt IE 9]><![endif]-->
 <video class="wp-video-shortcode" id="video-972-1" width="409" height="580" preload="metadata" controls="controls">
  <source type="video/webm" src="/files/2013/10/contacts-b2g-aug30-load-taphomescreen1.webm?_=1" /> <a href="/files/2013/10/contacts-b2g-aug30-load-taphomescreen1.webm">/files/2013/10/contacts-b2g-aug30-load-taphomescreen1.webm</a></video></div>

<p>My experience has been that pixel differences can be so great between visually identical frames on an eideticker capture on these devices that it&#8217;s pretty much impossible to settle on when startup is done using the frame difference method. It&#8217;s of course possible to detect very large scale changes, but the small scale ones (like the contacts actually appearing in the example above) are very hard to distinguish from random differences in the amount of light absorbed by the camera sensor. Tricks like using median filtering (a.k.a. &#8220;blurring&#8221;) help a bit, but not much. Take a look at this graph, for example:</p>

<p><a href="/files/2013/10/plotly-contacts-load-pixeldiff.png"><img src="/files/2013/10/plotly-contacts-load-pixeldiff.png" alt="plotly-contacts-load-pixeldiff" width="531" height="679" class="alignnone size-full wp-image-980" srcset="/files/2013/10/plotly-contacts-load-pixeldiff-234x300.png 234w, /files/2013/10/plotly-contacts-load-pixeldiff.png 531w" sizes="(max-width: 531px) 100vw, 531px" /></a>
 <br /><a href="https://plot.ly/~WilliamLachance/3">[Link to original]</a></p>

<p>You&#8217;ll note that the pixel differences during &#8220;static&#8221; parts of the capture are highly variable. This is because the pixel difference depends heavily on how &#8220;bright&#8221; each frame is: parts of the capture which are black (e.g. a contacts icon with a black background) have a much lower difference between them than parts that are bright (e.g. the contacts screen fully loaded).</p>

<p>After a day or so of experimenting and research, I settled on an approach which seems to work pretty reliably. Instead of comparing the frames directly, I measure the <a href="http://en.wikipedia.org/wiki/Entropy">entropy</a> of the <a href="http://en.wikipedia.org/wiki/Image_histogram">histogram</a> of colours used in each frame (essentially just an indication of brightness in this case, see <a href="http://brainacle.com/calculating-image-entropy-with-python-how-and-why.html">this article</a> for more on calculating it), then compare that of each frame with the average of the same measure over 5 previous frames (to account for the fact that two frames may be arbitrarily different, but that is unlikely that a sequence of frames will be). This seems to work much better than frame difference in this environment: although there are plenty of minute differences in light absorption in a capture from this camera, the overall color composition stays mostly the same. See this graph:</p>

<p><a href="/files/2013/10/plotly-contacts-load-entropy.png"><img src="/files/2013/10/plotly-contacts-load-entropy.png" alt="plotly-contacts-load-entropy" width="546" height="674" class="alignnone size-full wp-image-979" srcset="/files/2013/10/plotly-contacts-load-entropy-243x300.png 243w, /files/2013/10/plotly-contacts-load-entropy.png 546w" sizes="(max-width: 546px) 100vw, 546px" /></a>
 <br /><a href="https://plot.ly/~WilliamLachance/5">[Link to original]</a></p>

<p>If you look closely, you can see some minor variance in the entropy differences depending on the state of the screen, but it&#8217;s not nearly as pronounced as before. In practice, I&#8217;ve been able to get extremely consistent numbers with a reasonable &#8220;threshold&#8221; of &#8220;0.05&#8221;.</p>

<p>In Eideticker I&#8217;ve tried to steer away from using really complicated math or algorithms to measure things, unless all the alternatives fail. In that sense, I really liked the simplicity of &#8220;pixel differences&#8221; and am not thrilled about having to resort to this: hopefully the concepts in this case (histograms and entropy) are simple enough that most people will be able to understand my methodology, if they care to. Likely I will need to come up with something else for measuring responsiveness and animation smoothness (frames per second), as likely we can&#8217;t count on light composition changing the same way for those cases. My initial thought was to use <a href="http://en.wikipedia.org/wiki/Edge_detection">edge detection</a> (which, while somewhat complex to calculate, is at least easy to understand conceptually) but am open to other ideas.</p> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2013/10/first-eideticker-responsiveness-tests/">First Eideticker Responsiveness Tests</a></h2>
    <p class="index-date">Oct 7th, 2013</p>
    <p><span class="tags"><a href="/tags/Eideticker.html">Eideticker</a>  <a href="/tags/Mozilla.html">Mozilla</a>  <a href="/tags/Responsiveness.html">Responsiveness</a></span></p>
  </header>

<p><em>[ For more information on the Eideticker software I&#8217;m referring to, see <a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/">this entry</a> ]</em></p>

<p>Time for another update on Eideticker. In the last quarter, I&#8217;ve been working on two main items:</p>

<ol>
 <li>Responsiveness tests (Android / FirefoxOS)</li>
 <li>Eideticker for FirefoxOS</li></ol>

<p>The focus of this post is the responsiveness work. I&#8217;ll talk about Eideticker for FirefoxOS soon.</p>

<p>So what do I mean by responsiveness? At a high-level, I mean how quickly one sees a response after performing an action on the device. For example, if I perform a swipe gesture to scroll the content down while browsing CNN.com, how long does it take after
 <br />I start the gesture for the content to <em>visibly</em> scroll down? If you break it down, there&#8217;s a multi-step process that happens behind the scenes after a user action like this:</p>

<p><a href="/files/2013/10/input-events.png"><img src="/files/2013/10/input-events.png" alt="input-events" width="880" height="752" class="alignnone size-full wp-image-957" srcset="/files/2013/10/input-events-300x256.png 300w, /files/2013/10/input-events.png 880w" sizes="(max-width: 880px) 100vw, 880px" /></a></p>

<p>If anywhere in the steps above, there is a significant delay, the user experience is likely to be bad. Usability research
 <br />suggests that any lag that is consistently above 100 milliseconds will lead the user to <a href="http://stackoverflow.com/questions/536300/what-is-the-shortest-perceivable-application-response-delay">perceive things as being laggy</a>. To keep our users happy, we need to do our bit to make sure that we respond quickly at all levels that we control (just the application layer on Android, but pretty much everything on FirefoxOS). Even if we can&#8217;t complete the work required on our end to completely respond to the user&#8217;s desire, we should at least display something to acknowledge that things have changed.</p>

<p>But you can&#8217;t improve what you can&#8217;t measure. Fortunately, we have the means to do calculate of the time delta between <em>most</em> of the steps above. I learned from <a href="http://taras.glek.net/">Taras Glek</a> this weekend that it should be <a href="http://hackaday.com/2012/05/04/reaching-out-to-a-touch-screen-with-a-microcontroller/">possible to simulate</a> the actual capacitative touch event on a modern touch screen. We can recognize when the hardware event is available to be consumed by userspace by monitoring the `/dev/input` subsystem. And once the event reaches the application (the Android or FirefoxOS application) there&#8217;s no reason we can&#8217;t add instrumentation in all sorts of places to track the processing of both the event and the rendering of the response.</p>

<p>My working hypothesis is that it&#8217;s application-level latency (i.e. the time between the application receiving the event and being able to act on it) that dominates, so that&#8217;s what I decided to measure. This is purely based on intuition and by no means proven, so we should test this (it would certainly be an interesting exercise!). However, even if it turns out that there are significant problems here, we still care about the other bits of the stack &#8212; there&#8217;s lots of potentially-latency-introducing churn there and the risk of regression in our own code is probably higher than it is elsewhere since it changes so much.</p>

<p>Last year, I wrote up a tool called <a href="http://wrla.ch/blog/2012/07/the-evolution-of-simulating-events-in-eideticker-from-monkeys-to-orangutns/?utm_source=rss&amp;#038;utm_medium=rss&amp;#038;utm_campaign=the-evolution-of-simulating-events-in-eideticker-from-monkeys-to-orangutns">Orangutan</a> that can directly inject input events into an input device on Android or FirefoxOS. It seemed like a fairly straightforward extension of the tool to output timestamps when these events were registered. It was. Then, by <a href="http://wrla.ch/blog/2013/07/simple-command-line-ntp-client-for-android-and-firefoxos/">synchronizing the time</a> between the device and the machine doing the capturing, we can then correlate the input timestamps to events. To help visualize what&#8217;s going on, I generated this view:</p>

<p><a href="/files/2013/10/taskjs-framediff-view.png"><img src="/files/2013/10/taskjs-framediff-view.png" alt="taskjs-framediff-view" width="583" height="418" class="alignnone size-full wp-image-962" srcset="/files/2013/10/taskjs-framediff-view-300x215.png 300w, /files/2013/10/taskjs-framediff-view.png 583w" sizes="(max-width: 583px) 100vw, 583px" /></a></p>

<p><a href="http://eideticker.wrla.ch/framediff-view.html?title=Frame%20Difference%20Scrolling%20on%20taskjs.org%20%282013-10-06%29&amp;#038;video=videos/video-1381129971.63.webm&amp;#038;framediff=framediffs/framediff-1381129990.79.json&amp;#038;actionlog=actionlogs/action-log-1381129990.79.json">[Link to original]</a></p>

<p>The X axis in that graph represents time. The Y-axis represents the difference between the frame at that time with the previous one in number of pixels. The &#8220;red&#8221; represents periods in capture when events are ongoing (we use different colours only to
 <br />distinguish distinct events). <sup><a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/">1</a></sup></p>

<p>For a first pass at measuring responsiveness, I decided to measure the time between the first event being initiated and there being a significant frame difference (i.e. an observable response to the action). You can see some preliminary results on the eideticker dashboard:</p>

<p><a href="/files/2013/10/taskjs-responsiveness.png"><img src="/files/2013/10/taskjs-responsiveness.png" alt="taskjs-responsiveness" width="637" height="540" class="alignnone size-full wp-image-956" srcset="/files/2013/10/taskjs-responsiveness-300x254.png 300w, /files/2013/10/taskjs-responsiveness.png 637w" sizes="(max-width: 637px) 100vw, 637px" /></a></p>

<p><a href="http://eideticker.mozilla.org/#/samsung-gn/taskjs/timetoresponse">[Link to original]</a></p>

<p>The results seem pretty highly variable at first because I was synchronizing time between the device and an external ntp server, rather than the host machine. I believe this is now fixed, hopefully giving us results that will indicate when regressions occur. As time goes by, we may want to craft some special eideticker tests for responsiveness specifically (e.g. a site where there is heavy javascript background processing).</p>

<p><sup><a href="http://wrla.ch/blog/2012/06/mobile-firefox-measuring-how-a-browser-feels/">1</a></sup> <em>Incidentally, these &#8220;frame difference&#8221; graphs are also quite useful for understanding where and how application startup has regressed in Fennec &#8212; try opening these two startup views side-by-side (before/after a large regression) and spot the difference: <a href="http://eideticker.wrla.ch/framediff-view.html?title=Frame%20Difference%20Startup%20to%20about:home%20%28dirty%20profile%29%20%282013-08-20%29&amp;#038;video=videos/video-1377070981.95.webm&amp;#038;framediff=framediffs/framediff-1377070991.95.json">[1]</a> and <a href="http://eideticker.wrla.ch/framediff-view.html?title=Frame%20Difference%20Startup%20to%20about:home%20%28dirty%20profile%29%20%282013-08-23%29&amp;#038;video=videos/video-1377330042.28.webm&amp;#038;framediff=framediffs/framediff-1377330051.67.json">[2]</a>)</em></p> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2013/09/early-morning-questions/">Early morning questions</a></h2>
    <p class="index-date">Sep 25th, 2013</p>
    <p><span class="tags"><a href="/tags/Philosophy.html">Philosophy</a></span></p>
  </header>

<p>Last night while I was lying in bed the mystery of my being here, present, again occurred to me. Pondered that a bit upon waking up. Let me formulate two mysteries that, as far as I know, no one has given really satisfactory answers to:</p>

<ol>
 <li>Why does anything exist at all? And given that things do exist, why should they take the form that they do (planets, suns, nebulae, even life)?</li>
 <li>What accounts for the &#8220;subjectivity&#8221; of experience? That is, why is life not only here, but (in humanity&#8217;s case at least, probably in the case of other higher-order life, and possibly all life) there is a *conscious* experience that goes on with our perceptions of the world? It does not seem necessary for (1), does it?</li></ol>

<p>Perhaps the answer here is just that the way our minds (and hence anything we could form into thought or language) is based on descriptions of the world according to our perception. But (1) and (2) are in a sense, beyond this. I think in the case of (1) it is obvious why. In the case of (2) this might just be a limitation of our language/thought &#8212; certainly we can express that someone/something is conscious in a 3rd party sort of way (i.e. &#8220;she perceived red&#8221;), though this does not (as far as I can tell) express the <em>realness</em> of the experience. It&#8217;s a description, not the experience. To really understand experience from a 3rd person perspective (and hence why it exists?), you would need to go outside experience &#8212; but description <em>is part of</em> experience! The concept of being outside of it makes no sense.</p>

<p>[ Maybe I am just restating <a href="http://en.wikipedia.org/wiki/Immanuel_Kant">Kant</a> here ]</p> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2013/09/how-to-make-great-coffee-that-doesn-8217-t-generate-966-million-pounds-of-waste-a-year/">How to make great coffee that doesn&#8217;t generate 966 million pounds of waste a year</a></h2>
    <p class="index-date">Sep 17th, 2013</p>
    <p><span class="tags"><a href="/tags/Coffee.html">Coffee</a>  <a href="/tags/Environment.html">Environment</a></span></p>
  </header>

<p>I was kind of appalled today to see this:</p>

<p><a href="/files/2013/09/Story-of-Stuff-Picture.png"><img src="/files/2013/09/Story-of-Stuff-Picture.png" alt="Story of Stuff Picture" width="720" height="720" class="alignnone size-full wp-image-936" srcset="/files/2013/09/Story-of-Stuff-Picture-150x150.png 150w, /files/2013/09/Story-of-Stuff-Picture-300x300.png 300w, /files/2013/09/Story-of-Stuff-Picture.png 720w" sizes="(max-width: 720px) 100vw, 720px" /></a></p>

<p>I initially thought this had to be a tall tale told by hippies, but doing a back of the envelope calculation, I realized that such a figure is entirely possible. Assume each packet weighs 0.05 pounds. Typing that into python I get:
 <br /><code>&lt;br /&gt;
&gt;&gt;&gt; 966*(10**6)/0.05&lt;br /&gt;
19320000000.0</code></p>

<p>19 billion packets. Seems awfully big. But divide that by, say, 10 million people:</p>

<p><code>&gt;&gt;&gt; x = 966*(10**6)/0.05&lt;br /&gt;
&gt;&gt;&gt; x/10**7&lt;br /&gt;
1932.0</code></p>

<p>1932 cups. Hmm, still seems big. That&#8217;s more than 5 cups a day. But if we say 30 million people are drinking this stuff, we rapidly get to the zone of plausibility.</p>

<p>People, it doesn&#8217;t have to be this way. You can have <em>way better</em> coffee that produces <em>zero</em> waste for only marginally more effort. Allow me to present the Will method of coffee production. First off, you use this thing:</p>

<p><a href="/files/2013/09/bialetti_coffee_maker.jpg"><img src="/files/2013/09/bialetti_coffee_maker-767x1024.jpg" alt="bialetti_coffee_maker" width="640" height="854" class="alignnone size-large wp-image-934" srcset="/files/2013/09/bialetti_coffee_maker-224x300.jpg 224w, /files/2013/09/bialetti_coffee_maker-767x1024.jpg 767w, /files/2013/09/bialetti_coffee_maker.jpg 800w" sizes="(max-width: 640px) 100vw, 640px" /></a></p>

<p>I have tried alternatives: french presses, filter coffee, &#8220;cowboy&#8221; percolators, even &#8220;professional&#8221; espresso makers. I maintain that the Bialetti filter produces the best cup of coffee: one full cup of espresso goodness. Not too strong, not too weak. Just perfect. Add some milk and you have an amazing caf&eacute; au lait. Of course, part of getting the best cup is using the right beans. If you&#8217;re brewing at home, you can afford to go a little fancy. Here&#8217;s what I&#8217;m currently using:</p>

<p><a href="/files/2013/09/portlandia_coffee.jpg"><img src="/files/2013/09/portlandia_coffee-767x1024.jpg" alt="portlandia_coffee" width="640" height="854" class="alignnone size-large wp-image-937" srcset="/files/2013/09/portlandia_coffee-224x300.jpg 224w, /files/2013/09/portlandia_coffee-767x1024.jpg 767w, /files/2013/09/portlandia_coffee.jpg 800w" sizes="(max-width: 640px) 100vw, 640px" /></a></p>

<p>Yep, that&#8217;s right. A slice of Portlandia. Got this bag of espresso from Cafe Myriad, a rather upscale coffee joint. I think it was 15 dollars. A small bag like this is good for 30 cups or so. A keurig k-pack is <a href="https://www.keurig.ca/coffee/africana-organic-fair-trade-coffee-k-cup-van-houtte">$17.45 for 24</a>. I&#8217;d say I&#8217;m still ahead. If you&#8217;re on a tighter budget you can get fair trade beans for cheaper ($10 a pound?) from Santropol in Montr&eacute;al. Or whatever. Even generic stuff is probably fine (though I encourage fair trade if you can possibly afford it).</p>

<p>And what do I do with the waste? The only waste product of the Bialetti filter is coffee grinds. If I happened to live in a borough of Montr&eacute;al with composting, I could dump it there. Unfortunately I don&#8217;t (if you live in NDG, please vote for <a href="http://projetmontreal.org">these people</a> in the upcoming municipal election; municipal composting is part of their platform, amongst other awesomeness) so I have a vermicompost. My morning ritual is dump yesterday&#8217;s coffee grinds into this bin:</p>

<p><a href="/files/2013/09/vermicompost_pic.jpg"><img src="/files/2013/09/vermicompost_pic-1024x934.jpg" alt="vermicompost_pic" width="640" height="583" class="alignnone size-large wp-image-935" srcset="/files/2013/09/vermicompost_pic-300x273.jpg 300w, /files/2013/09/vermicompost_pic-1024x934.jpg 1024w, /files/2013/09/vermicompost_pic.jpg 1560w" sizes="(max-width: 640px) 100vw, 640px" /></a></p>

<p>&#8230; and then my numerous worms do the work of turning it into beautiful soil which I use in my <a href="https://plus.google.com/photos/112599231040201259540/albums/5891369648488530945">balcony garden</a> to grow tomotatos, kale, swiss chard, basil, and oregano.</p>

<p>What I want to emphasize most of all is that my ritual <em>takes very little time</em>. Scraping out and cleaning my Bialetti in the worm compost bin takes around minute. Refilling it with water and coffee takes maybe 30 seconds. Yes, once a year I have to take the worm trailings out of my vermicompost bin. That takes longer (maybe 30 minutes to an hour) but it&#8217;s a once a year thing and you avoid having to go to the store to buy fertilizer. Less waste. Way better coffee. Only a marginally more time spent. To me, this is a no-brainer.</p> 
  <hr/>
</article>
<article>
  <header>
    <h2><a href="/blog/2013/08/nixi-update/">NIXI Update</a></h2>
    <p class="index-date">Aug 25th, 2013</p>
    <p><span class="tags"><a href="/tags/BIXI.html">BIXI</a>  <a href="/tags/Data-Visualization.html">Data Visualization</a>  <a href="/tags/Nixi.html">Nixi</a></span></p>
  </header>

<p>I&#8217;ve been working on a new, mobile friendly version of <a href="http://nixi.ca/">Nixi</a> on-and-off for the past year and a bit. I&#8217;m not sure when it&#8217;s ever going to be finished, so I thought I might as well post the work-in-progress, which has these noteworthy improvements:</p>

<ul>
 <li>Even faster than before (using the <a href="http://getbootstrap.com">Bootstrap</a> library behind the scenes, no longer using slow canvas library to update map)</li>
 <li>Sexier graphics (thanks to the aforementioned Bootstrap library)</li>
 <li>Now uses client side URLs to keep track of state as you navigate through the site. This allows you to bookmark a favorite spot (e.g. your home) and then go back to it later. For example, <a href="http://nixi.ca/#/cities/montreal/places/5605%20avenue%20de%20Gaspe">this link</a> will give you a list of BIXI docks near <a href="http://www.station-c.com/">Station C</a>, the coworking space I belong to.</li></ul>

<p>If you use <a href="http://bixi.com">BIXI</a> at all, check it out and let me know what you think!</p>

<p><a href="/files/2013/08/nixi-screenshot.png"> <img src="/files/2013/08/nixi-screenshot-1024x672.png" alt="nixi screenshot" width="640" height="420" class="alignnone size-large wp-image-927" srcset="/files/2013/08/nixi-screenshot-300x196.png 300w, /files/2013/08/nixi-screenshot-1024x672.png 1024w, /files/2013/08/nixi-screenshot.png 1266w" sizes="(max-width: 640px) 100vw, 640px" /></a></p> 
  <hr/>
</article>
<footer>
 <ul class="pagination">
  <li class="page-item"><a class="page-link" href="/index-8.html">
    <quote>&larr;</quote></a></li>
  <li class="page-item"><a class="page-link" href="/index.html">1</a></li>
  <li class="page-item"><a class="page-link" href="/index-2.html">2</a></li>
  <li class="page-item"><a class="page-link" href="/index-3.html">3</a></li>
  <li class="page-item"><a class="page-link" href="/index-4.html">4</a></li>
  <li class="page-item"><a class="page-link" href="/index-5.html">5</a></li>
  <li class="page-item"><a class="page-link" href="/index-6.html">6</a></li>
  <li class="page-item"><a class="page-link" href="/index-7.html">7</a></li>
  <li class="page-item"><a class="page-link" href="/index-8.html">8</a></li>
  <li class="page-item active"><a class="page-link" href="/index-9.html">9</a></li>
  <li class="page-item"><a class="page-link" href="/index-10.html">10</a></li>
  <li class="page-item"><a class="page-link" href="/index-11.html">11</a></li>
  <li class="page-item"><a class="page-link" href="/index-12.html">12</a></li>
  <li class="page-item"><a class="page-link" href="/index-13.html">13</a></li>
  <li class="page-item"><a class="page-link" href="/index-14.html">14</a></li>
  <li class="page-item"><a class="page-link" href="/index-15.html">15</a></li>
  <li class="page-item"><a class="page-link" href="/index-16.html">16</a></li>
  <li class="page-item"><a class="page-link" href="/index-10.html">
    <quote>&rarr;</quote></a></li></ul></footer>
    </div>
    <footer class="container max-w-screen-md px-8 py-4 mx-auto less-important">
      <p>Comments / thoughts? Feel free to send an email to wlach on protonmail.com or
        (for Mozilla-related stuff) reach me at <code>wlach</code> on <a href="https://wiki.mozilla.org/Matrix">Mozilla's instance of Matrix</a>.</p>
      <p>
        Site generated by
        <a href="https://github.com/greghendershott/frog">Frog</a>.
        Post content is licensed under a
        <a rel="license" href="http://creativecommons.org/licenses/by/4.0/"
          >Creative Commons Attribution 4.0 Unported License</a
        >.
      </p>
    </footer>
  </body>
</html>